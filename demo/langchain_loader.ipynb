{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先导入所需第三方库\n",
    "from langchain_community.document_loaders import (\n",
    "    UnstructuredFileLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    PyPDFLoader,\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='https://github.com/InternLM/Tutorial/blob/main/langchain/readme.md\\n\\n基于 InternLM 和 LangChain 搭建你的知识库\\n\\n基于 InternLM 和 LangChain 搭建你的知识库\\n\\n1 环境配置\\n1.1 InternLM 模型部署\\n1.2 模型下载\\n1.3 LangChain 相关环境配置\\n1.4 下载 NLTK 相关资源\\n1.5 下载本项目代码\\n\\n2 知识库搭建\\n2.1 数据收集\\n2.2 加载数据\\n2.3 构建向量数据库\\n2.4 整体脚本\\n\\n3 InternLM 接入 LangChain\\n\\n4 构建检索问答链\\n4.1 加载向量数据库\\n4.2 实例化自定义 LLM 与 Prompt Template\\n4.3 构建检索问答链\\n\\n5 部署 Web Demo\\n\\n6 作业\\n\\n1 环境配置\\n\\n1.1 InternLM 模型部署\\n\\n在 InternStudio 平台中选择 A100(1/4) 的配置，如下图所示镜像选择 Cuda11.7-conda，如下图所示：\\n\\n接下来打开刚刚租用服务器的 进入开发机，并且打开其中的终端开始环境配置、模型下载和运行 demo。\\n\\n进入开发机后，在页面的左上角可以切换 JupyterLab、终端 和  VScode，并在终端输入 bash 命令，进入 conda 环境。如下图所示：\\n\\n进入 conda 环境之后，使用以下命令从本地一个已有的 pytorch 2.0.1 的环境\\n\\nshell\\nbash\\n/root/share/install_conda_env_internlm_base.sh InternLM\\n\\n然后使用以下命令激活环境\\n\\nshell\\nconda activate InternLM\\n\\n并在环境中安装运行 demo 所需要的依赖。\\n\\n```shell\\n\\n升级pip\\n\\npython -m pip install --upgrade pip\\n\\npip install modelscope\\npip install transformers\\npip install streamlit\\npip install sentencepiece\\npip install accelerate\\n```\\n\\n1.2 模型下载\\n\\n在本地的 /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b 目录下已存储有所需的模型文件参数，可以使用软连接连接模型目录：\\n\\nbash\\nmkdir -p /root/langchain\\nln -s /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b /root/langchain/internlm2-chat-1_8b\\n\\n如果本地拷贝模型参数出现问题，我们也可以使用 modelscope 中的 snapshot_download 函数下载模型，第一个参数为模型名称，参数 cache_dir 为模型的下载路径。\\n\\n在 /root 路径下新建目录 langchain，在目录下新建 download.py 文件并在其中输入以下内容，粘贴代码后记得保存文件，如下图所示。并运行 python /root/langchain/download.py 执行下载，模型大小为 14 GB，下载模型大概需要 10~20 分钟\\n\\npython\\nimport torch\\nfrom modelscope import snapshot_download, AutoModel, AutoTokenizer\\nimport os\\nmodel_dir = snapshot_download(\\'Shanghai_AI_Laboratory/internlm2-chat-1_8b\\', cache_dir=\\'/root/langchain\\', revision=\\'v1.0.3\\')\\n\\n注意：使用 pwd 命令可以查看当前的路径，JupyterLab 左侧目录栏显示为 /root/ 下的路径。\\n\\n1.3 LangChain 相关环境配置\\n\\n在已完成 InternLM 的部署基础上，还需要安装以下依赖包：\\n\\nbash\\npip install langchain\\npip install langchain-community\\npip install gradio\\npip install chromadb\\npip install sentence-transformers\\npip install unstructured\\npip install markdown\\n\\n同时，我们需要使用到开源词向量模型 Sentence Transformer:（我们也可以选用别的开源词向量模型来进行 Embedding，目前选用这个模型是相对轻量、支持中文且效果较好的，同学们可以自由尝试别的开源词向量模型）\\n\\n首先需要使用 huggingface 官方提供的 huggingface-cli 命令行工具。安装依赖:\\n\\nshell\\npip install -U huggingface_hub\\n\\n然后在和 /root/langchain 目录下新建python文件 download_hf.py，填入以下代码：\\n\\nresume-download：断点续下\\n\\nlocal-dir：本地存储路径。（linux环境下需要填写绝对路径）\\n\\n```python\\nimport os\\n\\n下载模型\\n\\nos.system(\\'huggingface-cli download --resume-download sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 --local-dir /root/langchain/sentence-transformer\\')\\n```\\n\\n但是，使用 huggingface 下载可能速度较慢，我们可以使用 huggingface 镜像下载。与使用hugginge face下载相同，只需要填入镜像地址即可。\\n\\n将 download_hf.py 中的代码修改为以下代码：\\n\\n```python\\nimport os\\n\\n设置环境变量\\n\\nos.environ[\\'HF_ENDPOINT\\'] = \\'https://hf-mirror.com\\'\\n\\n下载模型\\n\\nos.system(\\'huggingface-cli download --resume-download sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 --local-dir /root/langchain/sentence-transformer\\')\\n```\\n\\n然后，在 /root/data 目录下执行该脚本即可自动开始下载：\\n\\nbash\\npython download_hf.py\\n\\n更多关于镜像使用可以移步至 HF Mirror 查看。\\n\\n1.4 下载 NLTK 相关资源\\n\\n我们在使用开源词向量模型构建开源词向量的时候，需要用到第三方库 nltk 的一些资源。正常情况下，其会自动从互联网上下载，但可能由于网络原因会导致下载中断，此处我们可以从国内仓库镜像地址下载相关资源，保存到服务器上。\\n\\n我们用以下命令下载 nltk 资源并解压到服务器上：\\n\\nbash\\ncd ~\\ngit clone https://gitee.com/yzy0612/nltk_data.git  --branch gh-pages\\ncd nltk_data\\nmv packages/*  ./\\ncd tokenizers\\nunzip punkt.zip\\ncd ../taggers\\nunzip averaged_perceptron_tagger.zip\\n\\n之后使用时服务器即会自动使用已有资源，无需再次下载。\\n\\n1.5 下载本项目代码\\n\\n我们在仓库中同步提供了所有脚本，可以查看该教程文件的同级目录的 demo 文件夹。\\n\\n建议通过以下目录将仓库 clone 到本地，可以直接在本地运行相关代码：\\n\\nbahs\\ncd /root/data\\ngit clone https://github.com/InternLM/tutorial\\n\\n通过上述命令，可以将本仓库 clone 到本地 root/tutorial 目录下，在之后的过程中可以对照仓库中的脚本来完成自己的代码，也可以直接使用仓库中的脚本。\\n\\n2 知识库搭建\\n\\n2.1 数据收集\\n\\n我们选择由上海人工智能实验室开源的一系列大模型工具开源仓库作为语料库来源，包括：\\n\\nOpenCompass：面向大模型评测的一站式平台\\n\\nIMDeploy：涵盖了 LLM 任务的全套轻量化、部署和服务解决方案的高效推理工具箱\\n\\nXTuner：轻量级微调大语言模型的工具库\\n\\nInternLM-XComposer：浦语·灵笔，基于书生·浦语大语言模型研发的视觉-语言大模型\\n\\nLagent：一个轻量级、开源的基于大语言模型的智能体（agent）框架\\n\\nInternLM：一个开源的轻量级训练框架，旨在支持大模型训练而无需大量的依赖\\n\\n首先我们需要将上述远程开源仓库 Clone 到本地，可以使用以下命令：\\n\\n```bash\\n\\n进入到数据库盘\\n\\ncd /root/langchain/data\\n\\nclone 上述开源仓库\\n\\ngit clone https://gitee.com/open-compass/opencompass.git\\ngit clone https://gitee.com/InternLM/lmdeploy.git\\ngit clone https://gitee.com/InternLM/xtuner.git\\ngit clone https://gitee.com/InternLM/InternLM-XComposer.git\\ngit clone https://gitee.com/InternLM/lagent.git\\ngit clone https://gitee.com/InternLM/InternLM.git\\n```\\n\\n接着，为语料处理方便，我们将选用上述仓库中所有的 markdown、txt 文件作为示例语料库。注意，也可以选用其中的代码文件加入到知识库中，但需要针对代码文件格式进行额外处理（因为代码文件对逻辑联系要求较高，且规范性较强，在分割时最好基于代码模块进行分割再加入向量数据库）。\\n\\n我们首先将上述仓库中所有满足条件的文件路径找出来，我们定义一个函数，该函数将递归指定文件夹路径，返回其中所有满足条件（即后缀名为 .md 或者 .txt 的文件）的文件路径：\\n\\npython\\nimport os \\ndef get_files(dir_path):\\n    # args：dir_path，目标文件夹路径\\n    file_list = []\\n    for filepath, dirnames, filenames in os.walk(dir_path):\\n        # os.walk 函数将递归遍历指定文件夹\\n        for filename in filenames:\\n            # 通过后缀名判断文件类型是否满足要求\\n            if filename.endswith(\".md\"):\\n                # 如果满足要求，将其绝对路径加入到结果列表\\n                file_list.append(os.path.join(filepath, filename))\\n            elif filename.endswith(\".txt\"):\\n                file_list.append(os.path.join(filepath, filename))\\n    return file_list\\n\\n2.2 加载数据\\n\\n得到所有目标文件路径之后，我们可以使用 LangChain 提供的 FileLoader 对象来加载目标文件，得到由目标文件解析出的纯文本内容。由于不同类型的文件需要对应不同的 FileLoader，我们判断目标文件类型，并针对性调用对应类型的 FileLoader，同时，调用 FileLoader 对象的 load 方法来得到加载之后的纯文本对象：\\n\\n```python\\nfrom tqdm import tqdm\\n\\nfrom langchain.document_loaders import UnstructuredFileLoader\\n\\nfrom langchain_community.document_loaders import UnstructuredFileLoader\\n\\nfrom langchain.document_loaders import UnstructuredMarkdownLoader\\n\\nfrom langchain_community.document_loaders import UnstructuredMarkdownLoader\\n\\ndef get_text(dir_path):\\n    # args：dir_path，目标文件夹路径\\n    # 首先调用上文定义的函数得到目标文件路径列表\\n    file_lst = get_files(dir_path)\\n    # docs 存放加载之后的纯文本对象\\n    docs = []\\n    # 遍历所有目标文件\\n    for one_file in tqdm(file_lst):\\n        file_type = one_file.split(\\'.\\')[-1]\\n        if file_type == \\'md\\':\\n            loader = UnstructuredMarkdownLoader(one_file)\\n        elif file_type == \\'txt\\':\\n            loader = UnstructuredFileLoader(one_file)\\n        else:\\n            # 如果是不符合条件的文件，直接跳过\\n            continue\\n        docs.extend(loader.load())\\n    return docs\\n```\\n\\n使用上文函数，我们得到的 docs 为一个纯文本对象对应的列表。\\n\\n2.3 构建向量数据库\\n\\n得到该列表之后，我们就可以将它引入到 LangChain 框架中构建向量数据库。由纯文本对象构建向量数据库，我们需要先对文本进行分块，接着对文本块进行向量化。\\n\\nLangChain 提供了多种文本分块工具，此处我们使用字符串递归分割器，并选择分块大小为 500，块重叠长度为 150（由于篇幅限制，此处没有展示切割效果，学习者可以自行尝试一下，想要深入学习 LangChain 文本分块可以参考教程 《LangChain - Chat With Your Data》：\\n\\n```python\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=500, chunk_overlap=150)\\nsplit_docs = text_splitter.split_documents(docs)\\n```\\n\\n接着我们选用开源词向量模型 Sentence Transformer 来进行文本向量化。LangChain 提供了直接引入 HuggingFace 开源社区中的模型进行向量化的接口：\\n\\n```python\\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"./sentence-transformer\")\\n```\\n\\n同时，考虑到 Chroma 是目前最常用的入门数据库，我们选择 Chroma 作为向量数据库，基于上文分块后的文档以及加载的开源向量化模型，将语料加载到指定路径下的向量数据库：\\n\\n```python\\n\\nfrom langchain.vectorstores import Chroma\\n\\nfrom langchain_community.vectorstores import Chroma\\n\\n定义持久化路径\\n\\npersist_directory = \\'./vector_db/chroma\\'\\n\\n加载数据库\\n\\nvectordb = Chroma.from_documents(\\n    documents=split_docs,\\n    embedding=embeddings,\\n    persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\\n)\\n\\n将加载的向量数据库持久化到磁盘上\\n\\nvectordb.persist()\\n```\\n\\n2.4 整体脚本\\n\\n将上述代码整合在一起为知识库搭建的脚本：\\n\\n```python\\n\\n首先导入所需第三方库\\n\\nfrom langchain.document_loaders import UnstructuredFileLoader\\n\\nfrom langchain_community.document_loaders import UnstructuredFileLoader\\n\\nfrom langchain.document_loaders import UnstructuredMarkdownLoader\\n\\nfrom langchain_community.document_loaders import UnstructuredMarkdownLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\nfrom langchain.vectorstores import Chroma\\n\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\\nfrom tqdm import tqdm\\nimport os\\n\\n获取文件路径函数\\n\\ndef get_files(dir_path):\\n    # args：dir_path，目标文件夹路径\\n    file_list = []\\n    for filepath, dirnames, filenames in os.walk(dir_path):\\n        # os.walk 函数将递归遍历指定文件夹\\n        for filename in filenames:\\n            # 通过后缀名判断文件类型是否满足要求\\n            if filename.endswith(\".md\"):\\n                # 如果满足要求，将其绝对路径加入到结果列表\\n                file_list.append(os.path.join(filepath, filename))\\n            elif filename.endswith(\".txt\"):\\n                file_list.append(os.path.join(filepath, filename))\\n    return file_list\\n\\n加载文件函数\\n\\ndef get_text(dir_path):\\n    # args：dir_path，目标文件夹路径\\n    # 首先调用上文定义的函数得到目标文件路径列表\\n    file_lst = get_files(dir_path)\\n    # docs 存放加载之后的纯文本对象\\n    docs = []\\n    # 遍历所有目标文件\\n    for one_file in tqdm(file_lst):\\n        file_type = one_file.split(\\'.\\')[-1]\\n        if file_type == \\'md\\':\\n            loader = UnstructuredMarkdownLoader(one_file)\\n        elif file_type == \\'txt\\':\\n            loader = UnstructuredFileLoader(one_file)\\n        else:\\n            # 如果是不符合条件的文件，直接跳过\\n            continue\\n        docs.extend(loader.load())\\n    return docs\\n\\n目标文件夹\\n\\ntar_dir = [\\n    \"./data/InternLM\",\\n    \"./data/InternLM-XComposer\",\\n    \"./data/lagent\",\\n    \"./data/lmdeploy\",\\n    \"./data/opencompass\",\\n    \"./data/xtuner\"\\n]\\n\\n加载目标文件\\n\\ndocs = []\\nfor dir_path in tar_dir:\\n    docs.extend(get_text(dir_path))\\n\\n对文本进行分块\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=500, chunk_overlap=150)\\nsplit_docs = text_splitter.split_documents(docs)\\n\\n加载开源词向量模型\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"./sentence-transformer\")\\n\\n构建向量数据库\\n\\n定义持久化路径\\n\\npersist_directory = \"./vector_db/chroma\"\\n\\n加载数据库\\n\\nvectordb = Chroma.from_documents(\\n    documents=split_docs,\\n    embedding=embeddings,\\n    persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\\n)\\n\\n将加载的向量数据库持久化到磁盘上\\n\\nvectordb.persist()\\n\\n```\\n\\n可以在 /root/ 下新建一个 langchain目录，将该脚本和后续脚本均放在该目录下运行。运行上述脚本，即可在本地构建已持久化的向量数据库，后续直接导入该数据库即可，无需重复构建。\\n\\n3 InternLM 接入 LangChain\\n\\n为便捷构建 LLM 应用，我们需要基于本地部署的 InternLM，继承 LangChain 的 LLM 类自定义一个 InternLM LLM 子类，从而实现将 InternLM 接入到 LangChain 框架中。完成 LangChain 的自定义 LLM 子类之后，可以以完全一致的方式调用 LangChain 的接口，而无需考虑底层模型调用的不一致。\\n\\n基于本地部署的 InternLM 自定义 LLM 类并不复杂，我们只需从 LangChain.llms.base.LLM 类继承一个子类，并重写构造函数与 _call 函数即可：\\n\\n```python\\nfrom langchain.llms.base import LLM\\nfrom typing import Any, List, Optional\\nfrom langchain.callbacks.manager import CallbackManagerForLLMRun\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch\\n\\nclass InternLM_LLM(LLM):\\n    # 基于本地 InternLM 自定义 LLM 类\\n    tokenizer : AutoTokenizer = None\\n    model: AutoModelForCausalLM = None\\n\\n```\\n\\n在上述类定义中，我们分别重写了构造函数和 _call 函数：对于构造函数，我们在对象实例化的一开始加载本地部署的 InternLM 模型，从而避免每一次调用都需要重新加载模型带来的时间过长；_call 函数是 LLM 类的核心函数，LangChain 会调用该函数来调用 LLM，在该函数中，我们调用已实例化模型的 chat 方法，从而实现对模型的调用并返回调用结果。\\n\\n在整体项目中，我们将上述代码封装为 LLM.py，后续将直接从该文件中引入自定义的 LLM 类。\\n\\n4 构建检索问答链\\n\\nLangChain 通过提供检索问答链对象来实现对于 RAG 全流程的封装。所谓检索问答链，即通过一个对象完成检索增强问答（即RAG）的全流程，针对 RAG 的更多概念，我们会在视频内容中讲解，也欢迎读者查阅该教程来进一步了解：《LLM Universe》。我们可以调用一个 LangChain 提供的 RetrievalQA 对象，通过初始化时填入已构建的数据库和自定义 LLM 作为参数，来简便地完成检索增强问答的全流程，LangChain 会自动完成基于用户提问进行检索、获取相关文档、拼接为合适的 Prompt 并交给 LLM 问答的全部流程。\\n\\n4.1 加载向量数据库\\n\\n首先我们需要将上文构建的向量数据库导入进来，我们可以直接通过 Chroma 以及上文定义的词向量模型来加载已构建的数据库：\\n\\n```python\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\\nimport os\\n\\n定义 Embeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"./sentence-transformer\")\\n\\n向量数据库持久化路径\\n\\npersist_directory = \"./vector_db/chroma\"\\n\\n加载数据库\\n\\nvectordb = Chroma(\\n    persist_directory=persist_directory, \\n    embedding_function=embeddings\\n)\\n```\\n\\n上述代码得到的 vectordb 对象即为我们已构建的向量数据库对象，该对象可以针对用户的 query 进行语义向量检索，得到与用户提问相关的知识片段。\\n\\n4.2 实例化自定义 LLM 与 Prompt Template\\n\\n接着，我们实例化一个基于 InternLM 自定义的 LLM 对象：\\n\\npython\\nfrom LLM import InternLM_LLM\\nllm = InternLM_LLM(model_path = \"./internlm2-chat-1_8b\")\\nllm.predict(\"你是谁\")\\n\\n构建检索问答链，还需要构建一个 Prompt Template，该 Template 其实基于一个带变量的字符串，在检索之后，LangChain 会将检索到的相关文档片段填入到 Template 的变量中，从而实现带知识的 Prompt 构建。我们可以基于 LangChain 的 Template 基类来实例化这样一个 Template 对象：\\n\\n```python\\nfrom langchain.prompts import PromptTemplate\\n\\n我们所构造的 Prompt 模板\\n\\ntemplate = \"\"\"使用以下上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。\\n问题: {question}\\n可参考的上下文：\\n···\\n{context}\\n···\\n如果给定的上下文无法让你做出回答，请回答你不知道。\\n有用的回答:\"\"\"\\n\\n调用 LangChain 的方法来实例化一个 Template 对象，该对象包含了 context 和 question 两个变量，在实际调用时，这两个变量会被检索到的文档片段和用户提问填充\\n\\nQA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\\n```\\n\\n4.3 构建检索问答链\\n\\n最后，可以调用 LangChain 提供的检索问答链构造函数，基于我们的自定义 LLM、Prompt Template 和向量知识库来构建一个基于 InternLM 的检索问答链：\\n\\n```python\\nfrom langchain.chains import RetrievalQA\\n\\nqa_chain = RetrievalQA.from_chain_type(\\n    llm,\\n    retriever=vectordb.as_retriever(),\\n    return_source_documents=True,\\n    chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT}\\n)\\n```\\n\\n得到的 qa_chain 对象即可以实现我们的核心功能，即基于 InternLM 模型的专业知识库助手。我们可以对比该检索问答链和纯 LLM 的问答效果：\\n\\n```python\\n\\n检索问答链回答效果\\n\\nquestion = \"什么是InternLM\"\\nresult = qa_chain({\"query\": question})\\nprint(\"检索问答链回答 question 的结果：\")\\nprint(result[\"result\"])\\n\\n仅 LLM 回答效果\\n\\nresult_2 = llm(question)\\nprint(\"大模型回答 question 的结果：\")\\nprint(result_2)\\n```\\n\\n5 部署 Web Demo\\n\\n在完成上述核心功能后，我们可以基于 Gradio 框架将其部署到 Web 网页，从而搭建一个小型 Demo，便于测试与使用。\\n\\n我们首先将上文的代码内容封装为一个返回构建的检索问答链对象的函数，并在启动 Gradio 的第一时间调用该函数得到检索问答链对象，后续直接使用该对象进行问答对话，从而避免重复加载模型：\\n\\n```python\\n\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\\nimport os\\nfrom LLM import InternLM_LLM\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import RetrievalQA\\n\\ndef load_chain():\\n    # 加载问答链\\n    # 定义 Embeddings\\n    embeddings = HuggingFaceEmbeddings(model_name=\"./sentence-transformer\")\\n\\n```\\n\\n接着我们定义一个类，该类负责加载并存储检索问答链，并响应 Web 界面里调用检索问答链进行回答的动作：\\n\\n```python\\nclass ModelCenter():\\n    \"\"\"\\n    存储问答 Chain 的对象\\n    \"\"\"\\n    def init(self):\\n        self.chain = load_chain()\\n\\n```\\n\\n然后我们只需按照 Gradio 的框架使用方法，实例化一个 Web 界面并将点击动作绑定到上述类的回答方法即可：\\n\\n```python\\n\\n导入必要的库\\n\\nimport gradio as gr\\n\\nfrom langchain.vectorstores import Chroma\\n\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\\nimport os\\nfrom LLM import InternLM_LLM\\nfrom langchain.prompts import PromptTemplate\\n\\ndef load_chain():\\n    # 加载问答链\\n    # 定义 Embeddings\\n    embeddings = HuggingFaceEmbeddings(model_name=\"./sentence-transformer\")\\n\\nclass ModelCenter():\\n    \"\"\"\\n    存储问答 Chain 的对象\\n    \"\"\"\\n    def init(self):\\n        self.chain = load_chain()\\n\\nmodel_center = ModelCenter()\\n\\nblock = gr.Blocks()\\nwith block as demo:\\n    with gr.Row(equal_height=True):\\n        with gr.Column(scale=15):\\n            gr.Markdown(\"\"\"\\n\\nInternLM\\n\\nthreads to consume the request\\n\\ngr.close_all()\\n\\n启动新的 Gradio 应用，设置分享功能为 True，并使用环境变量 PORT1 指定服务器端口。\\n\\ndemo.launch(share=True, server_port=int(os.environ[\\'PORT1\\']))\\n\\n直接启动\\n\\ndemo.launch()\\n\\n```\\n\\n通过将上述代码封装为 run_gradio.py 脚本，直接通过 python 命令运行，即可在本地启动知识库助手的 Web Demo，默认会在 7860 端口运行，接下来将服务器端口映射到本地端口即可访问:\\n\\n此处我们简要介绍如何将服务器端口映射到本地端口：\\n\\n首先我们需要配置一下本地的 SSH Key ，我们这里以Windows为例。\\n\\n在本地机器上打开Power Shell终端。在终端中，运行以下命令来生成SSH密钥对：（如下图所示）\\nshell\\nssh-keygen -t rsa\\n\\n您将被提示选择密钥文件的保存位置，默认情况下是在 ~/.ssh/ 目录中。按Enter键接受默认值或输入自定义路径。\\n\\n公钥默认存储在 ~/.ssh/id_rsa.pub，可以通过系统自带的 cat 工具查看文件内容：（如下图所示）\\n\\n~ 是用户主目录的简写，.ssh 是SSH配置文件的默认存储目录，id_rsa.pub 是SSH公钥文件的默认名称。所以，cat ~\\\\.ssh\\\\id_rsa.pub 的意思是查看用户主目录下的 .ssh 目录中的 id_rsa.pub 文件的内容。\\n\\nshell\\ncat ~\\\\.ssh\\\\id_rsa.pub\\n\\n将公钥复制到剪贴板中，然后回到 InternStudio 控制台，点击配置SSH Key。如下图所示：\\n\\n将刚刚复制的公钥添加进入即可。\\n\\n在本地终端输入以下指令.7860是在服务器中打开的端口，而33090是根据开发机的端口进行更改。如下图所示：\\n\\nshell\\nssh -CNg -L 7860:127.0.0.1:7860 root@ssh.intern-ai.org.cn -p 33090\\n\\n我们在仓库中也同步提供了上述所有脚本，可以查看该教程文件的同级目录的 demo 文件夹。\\n\\n6 作业\\n\\n提交方式：在各个班级对应的 GitHub Discussion 帖子中进行提交。\\n\\n基础作业：\\n\\n复现课程知识库助手搭建过程 (截图)\\n\\n进阶作业：\\n\\n选择一个垂直领域，收集该领域的专业资料构建专业知识库，并搭建专业问答助手，并在 OpenXLab 上成功部署（截图，并提供应用地址）\\n\\n整体实训营项目：\\n\\n时间周期：即日起致课程结束\\n\\n即日开始可以在班级群中随机组队完成一个大作业项目，一些可提供的选题如下：\\n\\n人情世故大模型：一个帮助用户撰写新年祝福文案的人情事故大模型\\n\\n中小学数学大模型：一个拥有一定数学解题能力的大模型\\n\\n心理大模型：一个治愈的心理大模型\\n\\n工具调用类项目：结合 Lagent 构建数据集训练 InternLM 模型，支持对 MMYOLO 等工具的调用\\n\\n其他基于书生·浦语工具链的小项目都在范围内，欢迎大家充分发挥想象力。', metadata={'source': 'readme.md'})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = UnstructuredMarkdownLoader(\"readme.md\")\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# 首先导入所需第三方库\\n\\nfrom langchain_community.document_loaders import (\\n\\nUnstructuredFileLoader,\\n\\nUnstructuredMarkdownLoader,\\n\\nPyPDFLoader,\\n\\n)\\n\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\nfrom langchain_community.vectorstores import Chroma\\n\\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\\n\\nfrom tqdm import tqdm\\n\\nimport os\\n\\n# 获取文件路径函数 def get_files(dir_path): # args：dir_path，目标文件夹路径 file_list = [] for filepath, dirnames, filenames in os.walk(dir_path): # os.walk 函数将递归遍历指定文件夹 for filename in filenames: # 通过后缀名判断文件类型是否满足要求 if filename.endswith(\".md\"): # 如果满足要求，将其绝对路径加入到结果列表 file_list.append(os.path.join(filepath, filename)) elif filename.endswith(\".txt\"): file_list.append(os.path.join(filepath, filename)) return file_list\\n\\n# 加载文件函数\\n\\ndef get_text(dir_path):\\n\\n# args：dir_path，目标文件夹路径\\n\\n# 首先调用上文定义的函数得到目标文件路径列表\\n\\nfile_lst = get_files(dir_path)\\n\\n# docs 存放加载之后的纯文本对象\\n\\ndocs = []\\n\\n# 遍历所有目标文件\\n\\nfor one_file in tqdm(file_lst):\\n\\nfile_type = one_file.split(\\'. \\')[\\n\\n1]\\n\\nif file_type == \\'md\\':\\n\\nloader = UnstructuredMarkdownLoader(one_file)\\n\\nelif file_type == \\'txt\\':\\n\\nloader = UnstructuredFileLoader(one_file)\\n\\nelse:\\n\\n# 如果是不符合条件的文件，直接跳过\\n\\ncontinue\\n\\ndocs.extend(loader.load())\\n\\nreturn docs\\n\\n# 目标文件夹 tar_dirs = \"./data\" dirs = os.listdir(tar_dirs) dirs = [os.path.join(tar_dirs, dir) for dir in dirs] dirs = [dir for dir in dirs if os.path.isdir(dir)]\\n\\n# 加载目标文件\\n\\ndocs = []\\n\\nfor dir_path in dirs:\\n\\ndocs.extend(get_text(dir_path))\\n\\n# 对文本进行分块\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n\\nchunk_size=500, chunk_overlap=150)\\n\\nsplit_docs = text_splitter.split_documents(docs)\\n\\n# 加载开源词向量模型\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"./sentence\\n\\ntransformer\")\\n\\n# 构建向量数据库\\n\\n# 定义持久化路径\\n\\npersist_directory = \"./vector_db/chroma\"\\n\\n# 加载数据库\\n\\nvectordb = Chroma.from_documents(\\n\\ndocuments=split_docs,\\n\\nembedding=embeddings,\\n\\npersist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\\n\\n)\\n\\n# 将加载的向量数据库持久化到磁盘上\\n\\nvectordb.persist()', metadata={'source': 'create_db.py'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = UnstructuredFileLoader(\"create_db.py\")\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nPatrick Esser*Sumith Kulal Andreas Blattmann Rahim Entezari Jonas M ¨uller Harry Saini Yam Levi\\nDominik Lorenz Axel Sauer Frederic Boesel Dustin Podell Tim Dockhorn Zion English\\nKyle Lacey Alex Goodwin Yannik Marek Robin Rombach*\\nStability AI\\nFigure 1. High-resolution samples from our 8B rectified flow model, showcasing its capabilities in typography, precise prompt following\\nand spatial reasoning, attention to fine details, and high image quality across a wide variety of styles.\\nAbstract\\nDiffusion models create data from noise by invert-\\ning the forward paths of data towards noise and\\nhave emerged as a powerful generative modeling\\ntechnique for high-dimensional, perceptual data\\nsuch as images and videos. Rectified flow is a re-\\ncent generative model formulation that connects\\ndata and noise in a straight line. Despite its better\\ntheoretical properties and conceptual simplicity, it\\nis not yet decisively established as standard prac-\\ntice. In this work, we improve existing noise sam-\\npling techniques for training rectified flow mod-\\nels by biasing them towards perceptually relevant\\nscales. Through a large-scale study, we demon-\\n*Equal contribution . <first.last >@stability.ai.strate the superior performance of this approach\\ncompared to established diffusion formulations\\nfor high-resolution text-to-image synthesis. Ad-\\nditionally, we present a novel transformer-based\\narchitecture for text-to-image generation that uses\\nseparate weights for the two modalities and en-\\nables a bidirectional flow of information between\\nimage and text tokens, improving text comprehen-\\nsion, typography, and human preference ratings.\\nWe demonstrate that this architecture follows pre-\\ndictable scaling trends and correlates lower vali-\\ndation loss to improved text-to-image synthesis as\\nmeasured by various metrics and human evalua-\\ntions. Our largest models outperform state-of-the-\\nart models, and we will make our experimental\\ndata, code, and model weights publicly available.\\n1arXiv:2403.03206v1  [cs.CV]  5 Mar 2024', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 0}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\n1. Introduction\\nDiffusion models create data from noise (Song et al., 2020).\\nThey are trained to invert forward paths of data towards\\nrandom noise and, thus, in conjunction with approximation\\nand generalization properties of neural networks, can be\\nused to generate new data points that are not present in\\nthe training data but follow the distribution of the training\\ndata (Sohl-Dickstein et al., 2015; Song & Ermon, 2020).\\nThis generative modeling technique has proven to be very\\neffective for modeling high-dimensional, perceptual data\\nsuch as images (Ho et al., 2020). In recent years, diffusion\\nmodels have become the de-facto approach for generating\\nhigh-resolution images and videos from natural language\\ninputs with impressive generalization capabilities (Saharia\\net al., 2022b; Ramesh et al., 2022; Rombach et al., 2022;\\nPodell et al., 2023; Dai et al., 2023; Esser et al., 2023;\\nBlattmann et al., 2023b; Betker et al., 2023; Blattmann et al.,\\n2023a; Singer et al., 2022). Due to their iterative nature\\nand the associated computational costs, as well as the long\\nsampling times during inference, research on formulations\\nfor more efficient training and/or faster sampling of these\\nmodels has increased (Karras et al., 2023; Liu et al., 2022).\\nWhile specifying a forward path from data to noise leads to\\nefficient training, it also raises the question of which path\\nto choose. This choice can have important implications\\nfor sampling. For example, a forward process that fails to\\nremove all noise from the data can lead to a discrepancy\\nin training and test distribution and result in artifacts such\\nas gray image samples (Lin et al., 2024). Importantly, the\\nchoice of the forward process also influences the learned\\nbackward process and, thus, the sampling efficiency. While\\ncurved paths require many integration steps to simulate the\\nprocess, a straight path could be simulated with a single\\nstep and is less prone to error accumulation. Since each step\\ncorresponds to an evaluation of the neural network, this has\\na direct impact on the sampling speed.\\nA particular choice for the forward path is a so-called Rec-\\ntified Flow (Liu et al., 2022; Albergo & Vanden-Eijnden,\\n2022; Lipman et al., 2023), which connects data and noise\\non a straight line. Although this model class has better\\ntheoretical properties, it has not yet become decisively es-\\ntablished in practice. So far, some advantages have been\\nempirically demonstrated in small and medium-sized ex-\\nperiments (Ma et al., 2024), but these are mostly limited to\\nclass-conditional models. In this work, we change this by in-\\ntroducing a re-weighting of the noise scales in rectified flow\\nmodels, similar to noise-predictive diffusion models (Ho\\net al., 2020). Through a large-scale study, we compare\\nour new formulation to existing diffusion formulations and\\ndemonstrate its benefits.\\nWe show that the widely used approach for text-to-image\\nsynthesis, where a fixed text representation is fed directlyinto the model (e.g., via cross-attention (Vaswani et al.,\\n2017; Rombach et al., 2022)), is not ideal, and present\\na new architecture that incorporates learnable streams for\\nboth image and text tokens, which enables a two-way flow\\nof information between them. We combine this with our\\nimproved rectified flow formulation and investigate its scala-\\nbility. We demonstrate a predictable scaling trend in the val-\\nidation loss and show that a lower validation loss correlates\\nstrongly with improved automatic and human evaluations.\\nOur largest models outperform state-of-the art open models\\nsuch as SDXL (Podell et al., 2023), SDXL-Turbo (Sauer\\net al., 2023), Pixart- α(Chen et al., 2023), and closed-source\\nmodels such as DALL-E 3 (Betker et al., 2023) both in\\nquantitative evaluation (Ghosh et al., 2023) of prompt un-\\nderstanding and human preference ratings.\\nThe core contributions of our work are: (i) We conduct a\\nlarge-scale, systematic study on different diffusion model\\nand rectified flow formulations to identify the best setting.\\nFor this purpose, we introduce new noise samplers for recti-\\nfied flow models that improve performance over previously\\nknown samplers. (ii) We devise a novel, scalable architec-\\nture for text-to-image synthesis that allows bi-directional\\nmixing between text and image token streams within the\\nnetwork. We show its benefits compared to established back-\\nbones such as UViT (Hoogeboom et al., 2023) and DiT (Pee-\\nbles & Xie, 2023). Finally, we (iii) perform a scaling study\\nof our model and demonstrate that it follows predictable\\nscaling trends. We show that a lower validation loss cor-\\nrelates strongly with improved text-to-image performance\\nassessed via metrics such as T2I-CompBench (Huang et al.,\\n2023), GenEval (Ghosh et al., 2023) and human ratings. We\\nmake results, code, and model weights publicly available.\\n2. Simulation-Free Training of Flows\\nWe consider generative models that define a mapping be-\\ntween samples x1from a noise distribution p1to samples\\nx0from a data distribution p0in terms of an ordinary differ-\\nential equation (ODE),\\ndyt=vΘ(yt, t)dt , (1)\\nwhere the velocity vis parameterized by the weights Θof\\na neural network. Prior work by Chen et al. (2018) sug-\\ngested to directly solve Equation (1) via differentiable ODE\\nsolvers. However, this process is computationally expensive,\\nespecially for large network architectures that parameterize\\nvΘ(yt, t). A more efficient alternative is to directly regress\\na vector field utthat generates a probability path between\\np0andp1. To construct such a ut, we define a forward\\nprocess, corresponding to a probability path ptbetween p0\\nandp1=N(0,1), as\\nzt=atx0+btϵwhere ϵ∼ N(0, I). (2)\\n2', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 1}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nFora0= 1, b0= 0, a1= 0andb1= 1, the marginals,\\npt(zt) =Eϵ∼N(0,I)pt(zt|ϵ), (3)\\nare consistent with the data and noise distribution.\\nTo express the relationship between zt, x0andϵ, we intro-\\nduceψtandutas\\nψt(·|ϵ) :x07→atx0+btϵ (4)\\nut(z|ϵ):=ψ′\\nt(ψ−1\\nt(z|ϵ)|ϵ) (5)\\nSince ztcan be written as solution to the ODE z′\\nt=ut(zt|ϵ),\\nwith initial value z0=x0,ut(·|ϵ)generates pt(·|ϵ). Re-\\nmarkably, one can construct a marginal vector field utwhich\\ngenerates the marginal probability paths pt(Lipman et al.,\\n2023) (see B.1), using the conditional vector fields ut(·|ϵ):\\nut(z) =Eϵ∼N(0,I)ut(z|ϵ)pt(z|ϵ)\\npt(z)(6)\\nWhile regressing utwith the Flow Matching objective\\nLFM=Et,pt(z)||vΘ(z, t)−ut(z)||2\\n2. (7)\\ndirectly is intractable due to the marginalization in Equa-\\ntion 6, Conditional Flow Matching (see B.1),\\nLCFM =Et,pt(z|ϵ),p(ϵ)||vΘ(z, t)−ut(z|ϵ)||2\\n2,(8)\\nwith the conditional vector fields ut(z|ϵ)provides an equiv-\\nalent yet tractable objective.\\nTo convert the loss into an explicit form we insert\\nψ′\\nt(x0|ϵ) =a′\\ntx0+b′\\ntϵandψ−1\\nt(z|ϵ) =z−btϵ\\natinto (5)\\nz′\\nt=ut(zt|ϵ) =a′\\nt\\natzt−ϵbt(a′\\nt\\nat−b′\\nt\\nbt). (9)\\nNow, consider the signal-to-noise ratio λt:= loga2\\nt\\nb2\\nt. With\\nλ′\\nt= 2(a′\\nt\\nat−b′\\nt\\nbt), we can rewrite Equation (9) as\\nut(zt|ϵ) =a′\\nt\\natzt−bt\\n2λ′\\ntϵ (10)\\nNext, we use Equation (10) to reparameterize Equation (8)\\nas a noise-prediction objective:\\nLCFM =Et,pt(z|ϵ),p(ϵ)||vΘ(z, t)−a′\\nt\\natz+bt\\n2λ′\\ntϵ||2\\n2(11)\\n=Et,pt(z|ϵ),p(ϵ)\\x12\\n−bt\\n2λ′\\nt\\x132\\n||ϵΘ(z, t)−ϵ||2\\n2(12)\\nwhere we defined ϵΘ:=−2\\nλ′\\ntbt(vΘ−a′\\nt\\natz).\\nNote that the optimum of the above objective does not\\nchange when introducing a time-dependent weighting. Thus,one can derive various weighted loss functions that provide\\na signal towards the desired solution but might affect the\\noptimization trajectory. For a unified analysis of different\\napproaches, including classic diffusion formulations, we\\ncan write the objective in the following form (following\\nKingma & Gao (2023)):\\nLw(x0) =−1\\n2Et∼U(t),ϵ∼N(0,I)\\x02\\nwtλ′\\nt∥ϵΘ(zt, t)−ϵ∥2\\x03\\n,\\nwhere wt=−1\\n2λ′\\ntb2\\ntcorresponds to LCFM .\\n3. Flow Trajectories\\nIn this work, we consider different variants of the above\\nformalism that we briefly describe in the following.\\nRectified Flow Rectified Flows (RFs) (Liu et al., 2022;\\nAlbergo & Vanden-Eijnden, 2022; Lipman et al., 2023)\\ndefine the forward process as straight paths between the\\ndata distribution and a standard normal distribution, i.e.\\nzt= (1−t)x0+tϵ , (13)\\nand uses LCFM which then corresponds to wRF\\nt=t\\n1−t.\\nThe network output directly parameterizes the velocity vΘ.\\nEDM EDM (Karras et al., 2022) uses a forward process\\nof the form\\nzt=x0+btϵ (14)\\nwhere (Kingma & Gao, 2023) bt= exp F−1\\nN(t|Pm, P2\\ns)\\nwithF−1\\nNbeing the quantile function of the normal distribu-\\ntion with mean Pmand variance P2\\ns. Note that this choice\\nresults in\\nλt∼ N(−2Pm,(2Ps)2)fort∼ U(0,1) (15)\\nThe network is parameterized through an F-prediction\\n(Kingma & Gao, 2023; Karras et al., 2022) and the loss\\ncan be written as LwEDM\\ntwith\\nwEDM\\nt=N(λt| −2Pm,(2Ps)2)(e−λt+ 0.52) (16)\\nCosine (Nichol & Dhariwal, 2021) proposed a forward\\nprocess of the form\\nzt= cos\\x00π\\n2t\\x01\\nx0+ sin\\x00π\\n2t\\x01\\nϵ . (17)\\nIn combination with an ϵ-parameterization and loss, this\\ncorresponds to a weighting wt= sech( λt/2). When com-\\nbined with a v-prediction loss (Kingma & Gao, 2023), the\\nweighting is given by wt=e−λt/2.\\n3', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 2}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\n(LDM-)Linear LDM (Rombach et al., 2022) uses a mod-\\nification of the DDPM schedule (Ho et al., 2020). Both are\\nvariance preserving schedules, i.e. bt=p\\n1−a2\\nt, and de-\\nfineatfor discrete timesteps t= 0, . . . , T −1in terms\\nof diffusion coefficients βtasat= (Qt\\ns=0(1−βs))1\\n2.\\nFor given boundary values β0andβT−1, DDPM uses\\nβt=β0+t\\nT−1(βT−1−β0)and LDM uses βt=\\x10p\\nβ0+t\\nT−1(p\\nβT−1−p\\nβ0)\\x112\\n.\\n3.1. Tailored SNR Samplers for RF models\\nThe RF loss trains the velocity vΘuniformly on all timesteps\\nin[0,1]. Intuitively, however, the resulting velocity predic-\\ntion target ϵ−x0is more difficult for tin the middle of\\n[0,1], since for t= 0, the optimal prediction is the mean\\nofp1, and for t= 1the optimal prediction is the mean of\\np0. In general, changing the distribution over tfrom the\\ncommonly used uniform distribution U(t)to a distribution\\nwith density π(t)is equivalent to a weighted loss Lwπ\\ntwith\\nwπ\\nt=t\\n1−tπ(t) (18)\\nThus, we aim to give more weight to intermediate timesteps\\nby sampling them more frequently. Next, we describe the\\ntimestep densities π(t)that we use to train our models.\\nLogit-Normal Sampling One option for a distribution\\nthat puts more weight on intermediate steps is the logit-\\nnormal distribution (Atchison & Shen, 1980). Its density,\\nπln(t;m, s) =1\\ns√\\n2π1\\nt(1−t)exp\\x10\\n−(logit(t)−m)2\\n2s2\\x11\\n,\\n(19)\\nwhere logit(t) = logt\\n1−t, has a location parameter, m, and\\na scale parameter, s. The location parameter enables us to\\nbias the training timesteps towards either data p0(negative\\nm) or noise p1(positive m). As shown in Figure 11, the\\nscale parameters controls how wide the distribution is.\\nIn practice, we sample the random variable ufrom a nor-\\nmal distribution u∼ N (u;m, s)and map it through the\\nstandard logistic function.\\nMode Sampling with Heavy Tails The logit-normal den-\\nsity always vanishes at the endpoints 0and1. To study\\nwhether this has adverse effects on the performance, we\\nalso use a timestep sampling distribution with strictly posi-\\ntive density on [0,1]. For a scale parameter s, we define\\nfmode(u;s) = 1−u−s·\\x10\\ncos2\\x00π\\n2u\\x01\\n−1 +u\\x11\\n.(20)\\nFor−1≤s≤2\\nπ−2, this function is monotonic, and we\\ncan use it to sample from the implied density πmode(t;s) =\\x0c\\x0cd\\ndtf−1\\nmode(t)\\x0c\\x0c. As seen in Figure 11, the scale parametercontrols the degree to which either the midpoint (positive\\ns) or the endpoints (negative s) are favored during sam-\\npling. This formulation also includes a uniform weighting\\nπmode(t;s= 0) = U(t)fors= 0, which has been used\\nwidely in previous works on Rectified Flows (Liu et al.,\\n2022; Ma et al., 2024).\\nCosMap Finally, we also consider the cosine schedule\\n(Nichol & Dhariwal, 2021) from Section 3 in the RF setting.\\nIn particular, we are looking for a mapping f:u7→f(u) =\\nt, u∈[0,1], such that the log-snr matches that of the cosine\\nschedule: 2 logcos(π\\n2u)\\nsin(π\\n2u)= 2 log1−f(u)\\nf(u). Solving for f, we\\nobtain for u∼ U(u)\\nt=f(u) = 1−1\\ntan(π\\n2u) + 1, (21)\\nfrom which we obtain the density\\nπCosMap (t) =\\x0c\\x0c\\x0c\\x0cd\\ndtf−1(t)\\x0c\\x0c\\x0c\\x0c=2\\nπ−2πt+ 2πt2. (22)\\n4. Text-to-Image Architecture\\nFor text-conditional sampling of images, our model has to\\ntake both modalities, text and images, into account. We\\nuse pretrained models to derive suitable representations and\\nthen describe the architecture of our diffusion backbone. An\\noverview of this is presented in Figure 2.\\nOur general setup follows LDM (Rombach et al., 2022)\\nfor training text-to-image models in the latent space of a\\npretrained autoencoder. Similar to the encoding of images to\\nlatent representations, we also follow previous approaches\\n(Saharia et al., 2022b; Balaji et al., 2022) and encode the text\\nconditioning cusing pretrained, frozen text models. Details\\ncan be found in Appendix B.2.\\nMultimodal Diffusion Backbone Our architecture builds\\nupon the DiT (Peebles & Xie, 2023) architecture. DiT only\\nconsiders class conditional image generation and uses a\\nmodulation mechanism to condition the network on both\\nthe timestep of the diffusion process and the class label.\\nSimilarly, we use embeddings of the timestep tandcvec\\nas inputs to the modulation mechanism. However, as the\\npooled text representation retains only coarse-grained infor-\\nmation about the text input (Podell et al., 2023), the network\\nalso requires information from the sequence representation\\ncctxt.\\nWe construct a sequence consisting of embeddings of the\\ntext and image inputs. Specifically, we add positional en-\\ncodings and flatten 2×2patches of the latent pixel rep-\\nresentation x∈Rh×w×cto a patch encoding sequence of\\nlength1\\n2·h·1\\n2·w. After embedding this patch encoding\\nand the text encoding cctxtto a common dimensionality, we\\n4', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 3}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nCaption\\nCLIP-L/14 CLIP-G/14 T5 XXLPooled\\nLinear\\ncMLP\\nMLP\\nSinusoidal Encoding\\nTimestep+ yNoised Latent\\nPatching\\nLinear\\n+Positional\\nEmbedding\\nx\\nMM-DiT -Block 1\\nMM-DiT -Block 2\\n. . .\\nMM-DiT -Block d\\nModulation\\nLinear\\nUnpatching\\nOutput77 + 77 tokens\\n4096\\nchannel\\n(a) Overview of all components.c x\\nLayernorm\\nMod: αc· •+βc\\nLinear\\nAttention\\nLinear\\n∗\\n+\\nLayernorm\\nMod: δc· •+ϵc\\nMLP\\n∗\\n+αc\\nβc\\nγc\\nδc\\nϵc\\nζcy\\nSiLU\\nLinear\\nLayernorm\\nMod: αx· •+βx\\nLinear\\nLinear\\n∗\\n+\\nLayernorm\\nMod: δx· •+ϵx\\nMLP\\n∗\\n+αx\\nβx\\nγx\\nδx\\nϵx\\nζxSiLU\\nLinear\\nK Q V⊙⊙⊙Opt.\\nRMS-\\nNormOpt.\\nRMS-\\nNormOpt.\\nRMS-\\nNormOpt.\\nRMS-\\nNorm\\n(b) One MM-DiT block\\nFigure 2. Our model architecture. Concatenation is indicated by ⊙and element-wise multiplication by ∗. The RMS-Norm for QandK\\ncan be added to stabilize training runs. Best viewed zoomed in.\\nconcatenate the two sequences. We then follow DiT and\\napply a sequence of modulated attention and MLPs.\\nSince text and image embeddings are conceptually quite\\ndifferent, we use two separate sets of weights for the two\\nmodalities. As shown in Figure 2b, this is equivalent to\\nhaving two independent transformers for each modality, but\\njoining the sequences of the two modalities for the attention\\noperation, such that both representations can work in their\\nown space yet take the other one into account.\\nFor our scaling experiments, we parameterize the size of\\nthe model in terms of the model’s depth d,i.e. the number\\nof attention blocks, by setting the hidden size to 64·d\\n(expanded to 4·64·dchannels in the MLP blocks), and the\\nnumber of attention heads equal to d.\\n5. Experiments\\n5.1. Improving Rectified Flows\\nWe aim to understand which of the approaches for\\nsimulation-free training of normalizing flows as in Equa-\\ntion 1 is the most efficient. To enable comparisons across\\ndifferent approaches, we control for the optimization algo-\\nrithm, the model architecture, the dataset and samplers. Inaddition, the losses of different approaches are incomparable\\nand also do not necessarily correlate with the quality of out-\\nput samples; hence we need evaluation metrics that allow for\\na comparison between approaches. We train models on Ima-\\ngeNet (Russakovsky et al., 2014) and CC12M (Changpinyo\\net al., 2021), and evaluate both the training and the EMA\\nweights of the models during training using validation losses,\\nCLIP scores (Radford et al., 2021; Hessel et al., 2021), and\\nFID (Heusel et al., 2017) under different sampler settings\\n(different guidance scales and sampling steps). We calcu-\\nlate the FID on CLIP features as proposed by (Sauer et al.,\\n2021). All metrics are evaluated on the COCO-2014 valida-\\ntion split (Lin et al., 2014). Full details on the training and\\nsampling hyperparameters are provided in Appendix B.3.\\n5.1.1. R ESULTS\\nWe train each of 61 different formulations on the two\\ndatasets. We include the following variants from Section 3:\\n•Both ϵ- and v-prediction loss with linear\\n(eps/linear ,v/linear ) and cosine ( eps/cos ,\\nv/cos ) schedule.\\n•RF loss with πmode(t;s)(rf/mode(s) ) with 7 val-\\nues for schosen uniformly between −1and1.75, and\\n5', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 4}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nrank averaged over\\nvariant all 5 steps 50 steps\\nrf/lognorm(0.00, 1.00) 1.54 1.25 1.50\\nrf/lognorm(1.00, 0.60) 2.08 3.50 2.00\\nrf/lognorm(0.50, 0.60) 2.71 8.50 1.00\\nrf/mode(1.29) 2.75 3.25 3.00\\nrf/lognorm(0.50, 1.00) 2.83 1.50 2.50\\neps/linear 2.88 4.25 2.75\\nrf/mode(1.75) 3.33 2.75 2.75\\nrf/cosmap 4.13 3.75 4.00\\nedm(0.00, 0.60) 5.63 13.25 3.25\\nrf 5.67 6.50 5.75\\nv/linear 6.83 5.75 7.75\\nedm(0.60, 1.20) 9.00 13.00 9.00\\nv/cos 9.17 12.25 8.75\\nedm/cos 11.04 14.25 11.25\\nedm/rf 13.04 15.25 13.25\\nedm(-1.20, 1.20) 15.58 20.25 15.00\\nTable 1. Global ranking of variants. For this ranking, we apply\\nnon-dominated sorting averaged over EMA and non-EMA weights,\\ntwo datasets and different sampling settings.\\nImageNet CC12M\\nvariant CLIP FID CLIP FID\\nrf 0.247 49.70 0.217 94.90\\nedm(-1.20, 1.20) 0.236 63.12 0.200 116.60\\neps/linear 0.245 48.42 0.222 90.34\\nv/cos 0.244 50.74 0.209 97.87\\nv/linear 0.246 51.68 0.217 100.76\\nrf/lognorm(0.50, 0.60) 0.256 80.41 0.233 120.84\\nrf/mode(1.75) 0.253 44.39 0.218 94.06\\nrf/lognorm(1.00, 0.60) 0.254 114.26 0.234 147.69\\nrf/lognorm(-0.50, 1.00) 0.248 45.64 0.219 89.70\\nrf/lognorm(0.00, 1.00) 0.250 45.78 0.224 89.91\\nTable 2. Metrics for different variants. FID and CLIP scores of\\ndifferent variants with 25 sampling steps. We highlight the best,\\nsecond best , and third best entries.\\nadditionally for s= 1.0ands= 0which corresponds\\nto uniform timestep sampling ( rf/mode ).\\n•RF loss with πln(t;m, s)(rf/lognorm(m, s) )\\nwith 30 values for (m, s)in the grid with muniform\\nbetween −1and1, andsuniform between 0.2and2.2.\\n• RF loss with πCosMap (t)(rf/cosmap ).\\n•EDM ( edm( Pm, Ps)) with 15 values for Pmchosen\\nuniformly between −1.2and1.2andPsuniform be-\\ntween 0.6and1.8. Note that Pm, Ps= (−1.2,1.2)\\ncorresponds to the parameters in (Karras et al., 2022).\\n•EDM with a schedule such that it matches the log-SNR\\nweighting of rf(edm/rf ) and one that matches the\\nlog-SNR weighting of v/cos (edm/cos ).\\nFor each run, we select the step with minimal validation loss\\nwhen evaluated with EMA weights and then collect CLIP\\nscores and FID obtained with 6 different sampler settingsboth with and without EMA weights.\\nFor all 24 combinations of sampler settings, EMA weights,\\nand dataset choice, we rank the different formulations using\\na non-dominated sorting algorithm. For this, we repeatedly\\ncompute the variants that are Pareto optimal according to\\nCLIP and FID scores, assign those variants the current iter-\\nation index, remove those variants, and continue with the\\nremaining ones until all variants get ranked. Finally, we\\naverage those ranks over the 24 different control settings.\\nWe present the results in Tab. 1, where we only show the\\ntwo best-performing variants for those variants that were\\nevaluated with different hyperparameters. We also show\\nranks where we restrict the averaging over sampler settings\\nwith 5 steps and with 50 steps.\\nWe observe that rf/lognorm(0.00, 1.00) consis-\\ntently achieves a good rank. It outperforms a rectified\\nflow formulation with uniform timestep sampling ( rf) and\\nthus confirms our hypothesis that intermediate timesteps are\\nmore important. Among all the variants, only rectified flow\\nformulations with modified timestep sampling perform bet-\\nter than the LDM-Linear (Rombach et al., 2022) formulation\\n(eps/linear ) used previously.\\nWe also observe that some variants perform well in some\\nsettings but worse in others, e.g.rf/lognorm(0.50,\\n0.60) is the best-performing variant with 50 sampling\\nsteps but much worse (average rank 8.5) with 5 sampling\\nsteps. We observe a similar behavior with respect to the\\ntwo metrics in Tab. 2. The first group shows representa-\\ntive variants and their metrics on both datasets with 25\\nsampling steps. The next group shows the variants that\\nachieve the best CLIP and FID scores. With the exception\\nofrf/mode(1.75) , these variants typically perform very\\nwell in one metric but relatively badly in the other. In con-\\ntrast, we once again observe that rf/lognorm(0.00,\\n1.00) achieves good performance across metrics and\\ndatasets, where it obtains the third-best scores two out of\\nfour times and once the second-best performance.\\nFinally, we illustrate the qualitative behavior of different\\nformulations in Figure 3, where we use different colors\\nfor different groups of formulations ( edm,rf,eps and\\nv). Rectified flow formulations generally perform well and,\\ncompared to other formulations, their performance degrades\\nless when reducing the number of sampling steps.\\n5.2. Improving Modality Specific Representations\\nHaving found a formulation in the previous section that\\nallows rectified flow models to not only compete with estab-\\nlished diffusion formulations such as LDM-Linear (Rom-\\nbach et al., 2022) or EDM (Karras et al., 2022), but even\\noutperforms them, we now turn to the application of our\\nformulation to high-resolution text-to-image synthesis. Ac-\\n6', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 5}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\n10 20 30 40 506080100120140\\nedm(-1.20, 1.20)\\neps/linear\\nrf/lognorm(0.00, 1.00)\\nrf\\nv/cos\\nv/linear\\nnumber of sampling stepsFID\\nFigure 3. Rectified flows are sample efficient. Rectified Flows\\nperform better then other formulations when sampling fewer steps.\\nFor 25 and more steps, only rf/lognorm(0.00, 1.00) re-\\nmains competitive to eps/linear .\\nMetric 4 chn 8 chn 16 chn\\nFID (↓) 2.41 1.56 1.06\\nPerceptual Similarity ( ↓) 0.85 0.68 0.45\\nSSIM ( ↑) 0.75 0.79 0.86\\nPSNR ( ↑) 25.12 26.40 28.62\\nTable 3. Improved Autoencoders. Reconstruction performance\\nmetrics for different channel configurations. The downsampling\\nfactor for all models is f= 8.\\ncordingly, the final performance of our algorithm depends\\nnot only on the training formulation, but also on the parame-\\nterization via a neural network and the quality of the image\\nand text representations we use. In the following sections,\\nwe describe how we improve all these components before\\nscaling our final method in Section 5.3.\\n5.2.1. I MPROVED AUTOENCODERS\\nLatent diffusion models achieve high efficiency by operating\\nin the latent space of a pretrained autoencoder (Rombach\\net al., 2022), which maps an input RGB X∈RH×W×3into\\na lower-dimensional space x=E(X)∈Rh×w×d. The\\nreconstruction quality of this autoencoder provides an upper\\nbound on the achievable image quality after latent diffusion\\ntraining. Similar to Dai et al. (2023), we find that increasing\\nthe number of latent channels dsignificantly boosts recon-\\nstruction performance, see Table 3. Intuitively, predicting\\nlatents with higher dis a more difficult task, and thus mod-\\nels with increased capacity should be able to perform better\\nfor larger d, ultimately achieving higher image quality. We\\nconfirm this hypothesis in Figure 10, where we see that the\\nd= 16 autoencoder exhibits better scaling performance in\\nterms of sample FID. For the remainder of this paper, we\\nthus choose d= 16 .\\n5.2.2. I MPROVED CAPTIONS\\nBetker et al. (2023) demonstrated that synthetically gen-\\nerated captions can greatly improve text-to-image models\\ntrained at scale. This is due to the oftentimes simplisticOriginal Captions 50/50 Mix\\nsuccess rate [%] success rate [%]\\nColor Attribution 11.75 24.75\\nColors 71.54 68.09\\nPosition 6.50 18.00\\nCounting 33.44 41.56\\nSingle Object 95.00 93.75\\nTwo Objects 41.41 52.53\\nOverall score 43.27 49.78\\nTable 4. Improved Captions . Using a 50/50 mixing ratio of\\nsynthetic (via CogVLM (Wang et al., 2023)) and original cap-\\ntions improves text-to-image performance. Assessed via the\\nGenEval (Ghosh et al., 2023) benchmark.\\nnature of the human-generated captions that come with\\nlarge-scale image datasets, which overly focus on the image\\nsubject and usually omit details describing the background\\nor composition of the scene, or, if applicable, displayed\\ntext (Betker et al., 2023). We follow their approach and\\nuse an off-the-shelf, state-of-the-art vision-language model,\\nCogVLM (Wang et al., 2023), to create synthetic annotations\\nfor our large-scale image dataset. As synthetic captions may\\ncause a text-to-image model to forget about certain concepts\\nnot present in the VLM’s knowledge corpus, we use a ratio\\nof 50 % original and 50 % synthetic captions.\\nTo assess the effect of training on this caption mix, we train\\ntwod= 15 MM-DiT models for 250k steps, one on only\\noriginal captions and the other on the 50/50 mix. We evalu-\\nate the trained models using the GenEval benchmark (Ghosh\\net al., 2023) in Table 4. The results demonstrate that the\\nmodel trained with the addition of synthetic captions clearly\\noutperforms the model that only utilizes original captions.\\nWe thus use the 50/50 synthetic/original caption mix for the\\nremainder of this work.\\n5.2.3. I MPROVED TEXT-TO-IMAGE BACKBONES\\nIn this section, we compare the performance of existing\\ntransformer-based diffusion backbones with our novel mul-\\ntimodal transformer-based diffusion backbone, MM-DiT , as\\nintroduced in Section 4. MM-DiT is specifically designed to\\nhandle different domains, here text and image tokens, using\\n(two) different sets of trainable model weights. More specif-\\nically, we follow the experimental setup from Section 5.1\\nand compare text-to-image performance on CC12M of DiT,\\nCrossDiT (DiT but with cross-attending to the text tokens\\ninstead of sequence-wise concatenation (Chen et al., 2023))\\nand our MM-DiT . For MM-DiT , we compare models with\\ntwo sets of weights and three sets of weights, where the lat-\\nter handles the CLIP (Radford et al., 2021) and T5 (Raffel\\net al., 2019) tokens ( c.f. Section 4) separately. Note that DiT\\n(w/ concatenation of text and image tokens as in Section 4)\\ncan be interpreted as a special case of MM-DiT with one\\n7', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 6}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\na space elevator,\\ncinematic scifi art\\nA cheeseburger with juicy\\nbeef patties and melted\\ncheese sits on top of a toilet\\nthat looks like a throne and\\nstands in the middle of the\\nroyal chamber.\\na hole in the floor of my\\nbathroom with small\\ngremlins living in it\\na small office made out of car\\nparts\\nThis dreamlike digital art\\ncaptures a vibrant,\\nkaleidoscopic bird in a lush\\nrainforest.\\nhuman life depicted entirely\\nout of fractals\\nan origami pig on fire\\nin the middle of a\\ndark room with a\\npentagram on the\\nfloor\\nan old rusted robot wearing pants and a jacket riding skis in a supermarket.\\n smiling cartoon dog sits at a table, coffee mug on hand, as a room goes up in flames. “This is fine,” the\\ndog assures himself.\\nA whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus. This imaginative creature features the distinctive, bulky body of a hippo, but with a texture and\\nappearance resembling a golden-brown, crispy waffle. The creature might have elements like waffle squares across its skin and a syrup-like sheen. It’s set in a surreal environment that playfully combines a\\nnatural water habitat of a hippo with elements of a breakfast table setting, possibly including oversized utensils or plates in the background. The image should evoke a sense of playful absurdity and culinary\\nfantasy.\\n8', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 7}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nFigure 4. Training dynamics of model architectures. Compara-\\ntive analysis of DiT,CrossDiT ,UViT , and MM-DiT on CC12M,\\nfocusing on validation loss, CLIP score, and FID. Our proposed\\nMM-DiT performs favorably across all metrics.\\nshared set of weights for all modalities. Finally, we consider\\nthe UViT (Hoogeboom et al., 2023) architecture as a hybrid\\nbetween the widely used UNets and transformer variants.\\nWe analyze the convergence behavior of these architectures\\nin Figure 4: Vanilla DiT underperforms UViT. The cross-\\nattention DiT variant CrossDiT achieves better performance\\nthan UViT, although UViT seems to learn much faster ini-\\ntially. Our MM-DiT variant significantly outperforms the\\ncross-attention and vanilla variants. We observe only a small\\ngain when using three parameter sets instead of two (at the\\ncost of increased parameter count and VRAM usage), and\\nthus opt for the former option for the remainder of this work.\\n5.3. Training at Scale\\nBefore scaling up, we filter and preencode our data to ensure\\nsafe and efficient pretraining. Then, all previous consider-\\nations of diffusion formulations, architectures, and data\\nculminate in the last section, where we scale our models up\\nto 8B parameters.\\n5.3.1. D ATA PREPROCESSING\\nPre-Training Mitigations Training data significantly im-\\npacts a generative model’s abilities. Consequently, data\\nfiltering is effective at constraining undesirable capabili-\\nties (Nichol, 2022). Before training at sale, we filter our\\ndata for the following categories: (i) Sexual content: We\\nuse NSFW-detection models to filter for explicit content.\\n(ii) Aesthetics: We remove images for which our rating\\nsystems predict a low score. (iii) Regurgitation: We use a\\ncluster-based deduplication method to remove perceptual\\nand semantic duplicates from the training data; see Ap-\\npendix E.2.\\nPrecomputing Image and Text Embeddings Our model\\nuses the output of multiple pretrained, frozen networks as in-\\nputs (autoencoder latents and text encoder representations).\\nSince these outputs are constant during training, we precom-\\npute them once for the entire dataset. We provide a detailed\\ndiscussion of our approach in Appendix E.1.\\nFigure 5. Effects of QK-normalization. Normalizing the Q- and\\nK-embeddings before calculating the attention matrix prevents the\\nattention-logit growth instability ( left), which causes the attention\\nentropy to collapse ( right ) and has been previously reported in the\\ndiscriminative ViT literature (Dehghani et al., 2023; Wortsman\\net al., 2023). In contrast with these previous works, we observe\\nthis instability in the last transformer blocks of our networks. Max-\\nimum attention logits and attention entropies are shown averaged\\nover the last 5 blocks of a 2B (d=24) model.\\n5.3.2. F INETUNING ON HIGHRESOLUTIONS\\nQK-Normalization In general, we pretrain all of our mod-\\nels on low-resolution images of size 2562pixels. Next, we\\nfinetune our models on higher resolutions with mixed as-\\npect ratios (see next paragraph for details). We find that,\\nwhen moving to high resolutions, mixed precision train-\\ning can become unstable and the loss diverges. This can\\nbe remedied by switching to full precision training — but\\ncomes with a ∼2×performance drop compared to mixed-\\nprecision training. A more efficient alternative is reported\\nin the (discriminative) ViT literature: Dehghani et al. (2023)\\nobserve that the training of large vision transformer models\\ndiverges because the attention entropy grows uncontrollably.\\nTo avoid this, Dehghani et al. (2023) propose to normalize\\nQ and K before the attention operation. We follow this\\napproach and use RMSNorm (Zhang & Sennrich, 2019)\\nwith learnable scale in both streams of our MMDiT archi-\\ntecture for our models, see Figure 2. As demonstrated in\\nFigure 5, the additional normalization prevents the attention\\nlogit growth instability, confirming findings by Dehghani\\net al. (2023) and Wortsman et al. (2023) and enables efficient\\ntraining at bf16-mixed (Chen et al., 2019) precision when\\ncombined with ϵ= 10−15in the AdamW (Loshchilov &\\nHutter, 2017) optimizer. This technique can also be applied\\non pretrained models that have not used qk-normalization\\nduring pretraining: The model quickly adapts to the addi-\\ntional normalization layers and trains more stably. Finally,\\nwe would like to point out that although this method can\\ngenerally help to stabilize the training of large models, it is\\nnot a universal recipe and may need to be adapted depending\\non the exact training setup.\\nPositional Encodings for Varying Aspect Ratios After\\ntraining on a fixed 256×256resolution we aim to (i) in-\\ncrease the resolution and resolution and (ii) enable inference\\nwith flexible aspect ratios. Since we use 2d positional fre-\\n9', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 8}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nFigure 6. Timestep shifting at higher resolutions. Top right: Hu-\\nman quality preference rating when applying the shifting based\\non Equation (23). Bottom row: A5122model trained and sam-\\npled withp\\nm/n = 1.0(top) andp\\nm/n = 3.0(bottom ). See\\nSection 5.3.2.\\nquency embeddings we have to adapt them based on the\\nresolution. In the multi-aspect ratio setting, a direct inter-\\npolation of the embeddings as in (Dosovitskiy et al., 2020)\\nwould not reflect the side lengths correctly. Instead we use\\na combination of extended and interpolated position grids\\nwhich are subsequently frequency embedded.\\nFor a target resolution of S2pixels, we use bucketed sam-\\npling (NovelAI, 2022; Podell et al., 2023) such that that each\\nbatch consists of images of a homogeneous size H×W,\\nwhere H·W≈S2. For the maximum and minimum\\ntraining aspect ratios, this results in the maximum values for\\nwidth, Wmax, and height, Hmax, that will be encountered. Let\\nhmax=Hmax/16, wmax=Wmax/16ands=S/16be the\\ncorresponding sizes in latent space (a factor 8) after patching\\n(a factor 2). Based on these values, we construct a vertical\\nposition grid with the values ((p−hmax−s\\n2)·256\\nS)hmax−1\\np=0 and\\ncorrespondingly for the horizontal positions. We then center-\\ncrop from the resulting positional 2d grid before embedding\\nit.\\nResolution-dependent shifting of timestep schedules In-\\ntuitively, since higher resolutions have more pixels, we need\\nmore noise to destroy their signal. Assume we are working\\nin a resolution with n=H·Wpixels. Now, consider a\\n”constant” image, i.e. one where every pixel has the value\\nc. The forward process produces zt= (1−t)c 1+tϵ,\\nwhere both 1andϵ∈Rn. Thus, ztprovides nobservations\\nof the random variable Y= (1−t)c+tηwithcandη\\ninR, and ηfollows a standard normal distribution. Thus,\\nE(Y) = (1 −t)candσ(Y) =t. We can therefore recover\\ncviac=1\\n1−tE(Y), and the error between cand its sam-\\nple estimate ˆc=1\\n1−tPn\\ni=1zt,ihas a standard deviation of\\nFigure 7. Human Preference Evaluation against currrent\\nclosed and open SOTA generative image models. Our 8B model\\ncompares favorable against current state-of-the-art text-to-image\\nmodels when evaluated on the parti-prompts (Yu et al., 2022)\\nacross the categories visual quality ,prompt following andtypogra-\\nphy generation .\\nσ(t, n) =t\\n1−tq\\n1\\nn(because the standard error of the mean\\nforYhas deviationt√n). So if one already knows that the\\nimage z0was constant across its pixels, σ(t, n)represents\\nthe degree of uncertainty about z0. For example, we imme-\\ndiately see that doubling the width and height leads to half\\nthe uncertainty at any given time 0< t < 1. But, we can\\nnow map a timestep tnat resolution nto a timestep tmat\\nresolution mthat results in the same degree of uncertainty\\nvia the ansatz σ(tn, n) =σ(tm, m). Solving for tmgives\\ntm=pm\\nntn\\n1 + (pm\\nn−1)tn(23)\\nWe visualize this shifting function in Figure 6. Note that the\\nassumption of constant images is not realistic. To find good\\nvalues for the shift value α:=pm\\nnduring inference, we\\napply them to the sampling steps of a model trained at reso-\\nlution 1024×1024 and run a human preference study. The\\nresults in Figure 6 show a strong preference for samples with\\nshifts greater than 1.5but less drastic differences among the\\nhigher shift values. In our subsequent experiments, we thus\\nuse a shift value of α= 3.0both during training and sam-\\npling at resolution 1024×1024 . A qualitative comparison\\nbetween samples after 8k training steps with and without\\nsuch a shift can be found in Figure 6. Finally, note that\\nEquation 23 implies a log-SNR shift of logn\\nmsimilar to\\n(Hoogeboom et al., 2023):\\nλtm= 2 log1−tnpm\\nntn(24)\\n=λtn−2 logα=λtn−logm\\nn. (25)\\n10', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 9}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nAfter the shifted training at resolution 1024×1024 , we align\\nthe model using Direct Preference Optimization (DPO) as\\ndescribed in Appendix C.\\n5.3.3. R ESULTS\\nIn Figure 8, we examine the effect of training our MM-DiT\\nat scale. For images, we conduct a large scaling study and\\ntrain models with different numbers of parameters for 500k\\nsteps on 2562pixels resolution using preencoded data, c.f.\\nAppendix E.1, with a batch size of 4096. We train on 2×2\\npatches (Peebles & Xie, 2023), and report validation losses\\non the CoCo dataset (Lin et al., 2014) every 50k steps. In\\nparticular, to reduce noise in the validation loss signal, we\\nsample loss levels equidistant in t∈(0,1)and compute\\nvalidation loss for each level separately. We then average\\nthe loss across all but the last ( t= 1) levels.\\nSimilarly, we conduct a preliminary scaling study of our\\nMM-DiT on videos. To this end we start from the pretrained\\nimage weights and additionally use a 2x temporal patching.\\nWe follow Blattmann et al. (2023b) and feed data to the\\npretrained model by collapsing the temporal into the batch\\naxis. In each attention layer we rearrange the representation\\nin the visual stream and add a full attention over all spatio-\\ntemporal tokens after the spatial attention operation before\\nthe final feedforward layer. Our video models are trained for\\n140k steps with a batch size of 512 on videos comprising\\n16 frames with 2562pixels. We report validation losses on\\nthe Kinetics dataset (Carreira & Zisserman, 2018) every 5k\\nsteps. Note that our reported FLOPs for video training in\\nFigure 8 are only FLOPs from video training and do not\\ninclude the FLOPs from image pretraining.\\nFor both the image and video domains, we observe a smooth\\ndecrease in the validation loss when increasing model size\\nand training steps. We find the validation loss to be highly\\ncorrelated to comprehensive evaluation metrics (Comp-\\nBench (Huang et al., 2023), GenEval (Ghosh et al., 2023))\\nand to human preference. These results support the valida-\\ntion loss as a simple and general measure of model perfor-\\nmance. Our results do not show saturation neither for image\\nnot for video models.\\nFigure 12 illustrates how training a larger model for longer\\nimpacts sample quality. Tab. 5 shows the results of GenEval\\nin full. When applying the methods presented in Sec-\\ntion 5.3.2 and increasing training image resolution, our\\nbiggest model excels in most categories and outperforms\\nDALLE 3 (Betker et al., 2023), the current state of the art in\\nprompt comprehension, in overall score.\\nOurd= 38 model outperforms current proprietary (Betker\\net al., 2023; ide, 2024) and open (Sauer et al., 2023; pla,\\n2024; Chen et al., 2023; Pernias et al., 2023) SOTA gener-\\native image models in human preference evaluation on theModel OverallObjects\\nCounting Colors PositionColor\\nSingle Two Attribution\\nminDALL-E 0.23 0.73 0.11 0.12 0.37 0.02 0.01\\nSD v1.5 0.43 0.97 0.38 0.35 0.76 0.04 0.06\\nPixArt-alpha 0.48 0.98 0.50 0.44 0.80 0.08 0.07\\nSD v2.1 0.50 0.98 0.51 0.44 0.85 0.07 0.17\\nDALL-E 2 0.52 0.94 0.66 0.49 0.77 0.10 0.19\\nSDXL 0.55 0.98 0.74 0.39 0.85 0.15 0.23\\nSDXL Turbo 0.55 1.00 0.72 0.49 0.80 0.10 0.18\\nIF-XL 0.61 0.97 0.74 0.66 0.81 0.13 0.35\\nDALL-E 3 0.67 0.96 0.87 0.47 0.83 0.43 0.45\\nOurs (depth=18), 51220.58 0.97 0.72 0.52 0.78 0.16 0.34\\nOurs (depth=24), 51220.62 0.98 0.74 0.63 0.67 0.34 0.36\\nOurs (depth=30), 51220.64 0.96 0.80 0.65 0.73 0.33 0.37\\nOurs (depth=38), 51220.68 0.98 0.84 0.66 0.74 0.40 0.43\\nOurs (depth=38), 5122w/DPO 0.71 0.98 0.89 0.73 0.83 0.34 0.47\\nOurs (depth=38), 10242w/DPO 0.74 0.99 0.94 0.72 0.89 0.33 0.60\\nTable 5. GenEval comparisons . Our largest model (depth=38)\\noutperforms all current open models and DALLE-3 (Betker et al.,\\n2023) on GenEval (Ghosh et al., 2023). We highlight the best,\\nsecond best , and third best entries. For DPO, see Appendix C.\\nrelative CLIP score decrease [%]\\n5/50 steps 10/50 steps 20/50 steps path length\\ndepth=15 4.30 0.86 0.21 191.13\\ndepth=30 3.59 0.70 0.24 187.96\\ndepth=38 2.71 0.14 0.08 185.96\\nTable 6. Impact of model size on sampling efficiency. The table\\nshows the relative performance decrease relative to CLIP scores\\nevaluated using 50 sampling steps at a fixed seed. Larger models\\ncan be sampled using fewer steps, which we attribute to increased\\nrobustness and better fitting the straight-path objective of rectified\\nflow models, resulting in shorter path lengths. Path length is\\ncalculated by summing up ∥vθ·dt∥over 50 steps.\\nParti-prompts benchmark (Yu et al., 2022) in the categories\\nvisual aesthetics ,prompt following andtypography gener-\\nation ,c.f. Figure 7. For evaluating human preference in\\nthese categories, raters were shown pairwise outputs from\\ntwo models, and asked to answer the following questions:\\nPrompt following: Which image looks more representative\\nto the text shown above and faithfully follows it?\\nVisual aesthetics: Given the prompt, which image is of\\nhigher-quality andaesthetically more pleasing ?\\nTypography: Which image more accurately shows/displays\\nthe text specified in the above description? More accurate\\nspelling is preferred! Ignore other aspects.\\nLastly, Table 6 highlights an intriguing result: not only do\\nbigger models perform better, they also require fewer steps\\nto reach their peak performance.\\nFlexible Text Encoders While the main motivation for\\nusing multiple text-encoders is boosting the overall model\\nperformance (Balaji et al., 2022), we now show that this\\nchoice additionally increases the flexibility of our MM-DiT -\\nbased rectified flow during inference. As described in Ap-\\npendix B.3 we train our model with three text encoders, with\\nan individual drop-out rate of 46.3%. Hence, at inference\\n11', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 10}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nFigure 8. Quantitative effects of scaling. We analyze the impact of model size on performance, maintaining consistent training\\nhyperparameters throughout. An exception is depth=38, where learning rate adjustments at 3×105steps were necessary to prevent\\ndivergence. (Top) Validation loss smoothly decreases as a function of both model size and training steps for both image (columns 1 and 2)\\nand video models (columns 3 and 4). (Bottom) Validation loss is a strong predictor of overall model performance . There is a marked\\ncorrelation between validation loss and holistic image evaluation metrics, including GenEval (Ghosh et al., 2023), column 1, human\\npreference, column 2, and T2I-CompBench (Huang et al., 2023), column 3. For video models we observe a similar correlation between\\nvalidation loss and human preference, column 4. .\\nAll text-encoders w/o T5 (Raffel et al., 2019)\\n“A burger patty, with the bottom bun and lettuce and tomatoes. ”COFFEE” written on it in mustard”\\n“A monkey holding a sign reading ”Scaling transformer models is awesome!”\\n“A mischievous ferret with a playful grin squeezes itself into a large glass jar, surrounded by\\ncolorful candy. The jar sits on a wooden table in a cozy kitchen, and warm sunlight filters\\nthrough a nearby window”\\nFigure 9. Impact of T5. We observe T5 to be important for\\ncomplex prompts e.g. such involving a high degree of detail or\\nlonger spelled text (rows 2 and 3). For most prompts, however, we\\nfind that removing T5 at inference time still achieves competitive\\nperformance.\\ntime, we can use an arbitrary subset of all three text encoders.\\nThis offers means for trading off model performance for im-\\nproved memory efficiency, which is particularly relevant\\nfor the 4.7B parameters of T5-XXL (Raffel et al., 2019)\\nthat require significant amounts of VRAM. Interestingly, we\\nobserve limited performance drops when using only the two\\nCLIP-based text-encoders for the text prompts and replac-\\ning the T5 embeddings by zeros. We provide a qualitative\\nvisualization in Figure 9. Only for complex prompts involv-ing either highly detailed descriptions of a scene or larger\\namounts of written text do we find significant performance\\ngains when using all three text-encoders. These observa-\\ntions are also verified in the human preference evaluation\\nresults in Figure 7 ( Ours w/o T5 ). Removing T5 has no\\neffect on aesthetic quality ratings ( 50% win rate), and only a\\nsmall impact on prompt adherence ( 46% win rate), whereas\\nits contribution to the capabilities of generating written text\\nare more significant ( 38% win rate).\\n6. Conclusion\\nIn this work, we presented a scaling analysis of rectified\\nflow models for text-to-image synthesis. We proposed a\\nnovel timestep sampling for rectified flow training that im-\\nproves over previous diffusion training formulations for\\nlatent diffusion models and retains the favourable proper-\\nties of rectified flows in the few-step sampling regime. We\\nalso demonstrated the advantages of our transformer-based\\nMM-DiT architecture that takes the multi-modal nature of\\nthe text-to-image task into account. Finally, we performed\\na scaling study of this combination up to a model size of\\n8B parameters and 5×1022training FLOPs. We showed\\nthat validation loss improvements correlate with both exist-\\ning text-to-image benchmarks as well as human preference\\nevaluations. This, in combination with our improvements in\\ngenerative modeling and scalable, multimodal architectures\\nachieves performance that is competitive with state-of-the-\\nart proprietary models. The scaling trend shows no signs of\\nsaturation, which makes us optimistic that we can continue\\nto improve the performance of our models in the future.\\n12', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 11}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nBroader Impact\\nThis paper presents work whose goal is to advance the field\\nof machine learning in general and image synthesis in par-\\nticular. There are many potential societal consequences\\nof our work, none of which we feel must be specifically\\nhighlighted here. For an extensive discussion of the gen-\\neral ramifications of diffusion models, we point interested\\nreaders towards (Po et al., 2023).\\nReferences\\nIdeogram v1.0 announcement, 2024. URL https://ab\\nout.ideogram.ai/1.0 .\\nPlayground v2.5 announcement, 2024. URL https://bl\\nog.playgroundai.com/playground-v2-5/ .\\nAlbergo, M. S. and Vanden-Eijnden, E. Building normaliz-\\ning flows with stochastic interpolants, 2022.\\nAtchison, J. and Shen, S. M. Logistic-normal distributions:\\nSome properties and uses. Biometrika , 67(2):261–272,\\n1980.\\nautofaiss. autofaiss, 2023. URL https://github.c\\nom/criteo/autofaiss .\\nBalaji, Y ., Nah, S., Huang, X., Vahdat, A., Song, J., Zhang,\\nQ., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro,\\nB., Karras, T., and Liu, M.-Y . ediff-i: Text-to-image\\ndiffusion models with an ensemble of expert denoisers,\\n2022.\\nBetker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L.,\\nOuyang, L., Zhuang, J., Lee, J., Guo, Y ., et al. Improving\\nimage generation with better captions. Computer Science.\\nhttps://cdn. openai. com/papers/dall-e-3. pdf , 2(3), 2023.\\nBlattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D.,\\nKilian, M., Lorenz, D., Levi, Y ., English, Z., V oleti, V .,\\nLetts, A., et al. Stable video diffusion: Scaling latent\\nvideo diffusion models to large datasets. arXiv preprint\\narXiv:2311.15127 , 2023a.\\nBlattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim,\\nS. W., Fidler, S., and Kreis, K. Align your latents: High-\\nresolution video synthesis with latent diffusion models,\\n2023b.\\nBrooks, T., Holynski, A., and Efros, A. A. Instructpix2pix:\\nLearning to follow image editing instructions. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pp. 18392–18402, 2023.\\nCarlini, N., Hayes, J., Nasr, M., Jagielski, M., Sehwag,\\nV ., Tramer, F., Balle, B., Ippolito, D., and Wallace, E.\\nExtracting training data from diffusion models. In 32ndUSENIX Security Symposium (USENIX Security 23) , pp.\\n5253–5270, 2023.\\nCarreira, J. and Zisserman, A. Quo vadis, action recogni-\\ntion? a new model and the kinetics dataset, 2018.\\nChangpinyo, S., Sharma, P. K., Ding, N., and Soricut,\\nR. Conceptual 12m: Pushing web-scale image-text pre-\\ntraining to recognize long-tail visual concepts. 2021\\nIEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition (CVPR) , pp. 3557–3567, 2021. URL\\nhttps://api.semanticscholar.org/Corp\\nusID:231951742 .\\nChen, D., Chou, C., Xu, Y ., and Hseu, J. Bfloat16: The\\nsecret to high performance on cloud tpus, 2019. URL\\nhttps://cloud.google.com/blog/produc\\nts/ai-machine-learning/bfloat16-the-s\\necret-to-high-performance-on-cloud-t\\npus?hl=en .\\nChen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y ., Wang,\\nZ., Kwok, J., Luo, P., Lu, H., and Li, Z. Pixart-a: Fast\\ntraining of diffusion transformer for photorealistic text-\\nto-image synthesis, 2023.\\nChen, T. Q., Rubanova, Y ., Bettencourt, J., and Duvenaud,\\nD. K. Neural ordinary differential equations. In Neural\\nInformation Processing Systems , 2018. URL https:\\n//api.semanticscholar.org/CorpusID:49\\n310446 .\\nCherti, M., Beaumont, R., Wightman, R., Wortsman, M.,\\nIlharco, G., Gordon, C., Schuhmann, C., Schmidt, L.,\\nand Jitsev, J. Reproducible scaling laws for contrastive\\nlanguage-image learning. In 2023 IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR) .\\nIEEE, 2023. doi: 10.1109/cvpr52729.2023.00276. URL\\nhttp://dx.doi.org/10.1109/CVPR52729.2\\n023.00276 .\\nDai, X., Hou, J., Ma, C.-Y ., Tsai, S., Wang, J., Wang, R.,\\nZhang, P., Vandenhende, S., Wang, X., Dubey, A., Yu, M.,\\nKadian, A., Radenovic, F., Mahajan, D., Li, K., Zhao, Y .,\\nPetrovic, V ., Singh, M. K., Motwani, S., Wen, Y ., Song,\\nY ., Sumbaly, R., Ramanathan, V ., He, Z., Vajda, P., and\\nParikh, D. Emu: Enhancing image generation models\\nusing photogenic needles in a haystack, 2023.\\nDao, Q., Phung, H., Nguyen, B., and Tran, A. Flow match-\\ning in latent space, 2023.\\nDehghani, M., Djolonga, J., Mustafa, B., Padlewski, P.,\\nHeek, J., Gilmer, J., Steiner, A., Caron, M., Geirhos, R.,\\nAlabdulmohsin, I., Jenatton, R., Beyer, L., Tschannen,\\nM., Arnab, A., Wang, X., Riquelme, C., Minderer, M.,\\nPuigcerver, J., Evci, U., Kumar, M., van Steenkiste, S.,\\n13', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 12}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nElsayed, G. F., Mahendran, A., Yu, F., Oliver, A., Huot,\\nF., Bastings, J., Collier, M. P., Gritsenko, A., Birodkar,\\nV ., Vasconcelos, C., Tay, Y ., Mensink, T., Kolesnikov,\\nA., Paveti ´c, F., Tran, D., Kipf, T., Lu ˇci´c, M., Zhai, X.,\\nKeysers, D., Harmsen, J., and Houlsby, N. Scaling vision\\ntransformers to 22 billion parameters, 2023.\\nDhariwal, P. and Nichol, A. Diffusion models beat gans on\\nimage synthesis, 2021.\\nDockhorn, T., Vahdat, A., and Kreis, K. Score-based gener-\\native modeling with critically-damped langevin diffusion.\\narXiv preprint arXiv:2112.07068 , 2021.\\nDockhorn, T., Vahdat, A., and Kreis, K. Genie: Higher-\\norder denoising diffusion solvers, 2022.\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\\nHeigold, G., Gelly, S., et al. An image is worth 16x16\\nwords: Transformers for image recognition at scale. ICLR ,\\n2020.\\nEsser, P., Chiu, J., Atighehchian, P., Granskog, J., and Ger-\\nmanidis, A. Structure and content-guided video synthesis\\nwith diffusion models, 2023.\\nEuler, L. Institutionum calculi integralis . Number Bd. 1 in\\nInstitutionum calculi integralis. imp. Acad. imp. Sa `ent.,\\n1768. URL https://books.google.de/book\\ns?id=Vg8OAAAAQAAJ .\\nFischer, J. S., Gui, M., Ma, P., Stracke, N., Baumann, S. A.,\\nand Ommer, B. Boosting latent diffusion with flow match-\\ning. arXiv preprint arXiv:2312.07360 , 2023.\\nGhosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An\\nobject-focused framework for evaluating text-to-image\\nalignment. arXiv preprint arXiv:2310.11513 , 2023.\\nGupta, A., Yu, L., Sohn, K., Gu, X., Hahn, M., Fei-Fei, L.,\\nEssa, I., Jiang, L., and Lezama, J. Photorealistic video\\ngeneration with diffusion models, 2023.\\nHessel, J., Holtzman, A., Forbes, M., Le Bras, R., and\\nChoi, Y . Clipscore: A reference-free evaluation metric for\\nimage captioning. In Proceedings of the 2021 Conference\\non Empirical Methods in Natural Language Processing .\\nAssociation for Computational Linguistics, 2021. doi:\\n10.18653/v1/2021.emnlp-main.595. URL http://dx\\n.doi.org/10.18653/v1/2021.emnlp-main.\\n595.\\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\\nHochreiter, S. Gans trained by a two time-scale update\\nrule converge to a local nash equilibrium, 2017.\\nHo, J. and Salimans, T. Classifier-free diffusion guidance,\\n2022.Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\\nbilistic models, 2020.\\nHo, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko,\\nA., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J.,\\nand Salimans, T. Imagen video: High definition video\\ngeneration with diffusion models, 2022.\\nHoogeboom, E., Heek, J., and Salimans, T. Simple diffusion:\\nEnd-to-end diffusion for high resolution images, 2023.\\nHuang, K., Sun, K., Xie, E., Li, Z., and Liu, X. T2i-\\ncompbench: A comprehensive benchmark for open-world\\ncompositional text-to-image generation. arXiv preprint\\narXiv:2307.06350 , 2023.\\nHyv¨arinen, A. Estimation of non-normalized statistical\\nmodels by score matching. J. Mach. Learn. Res. , 6:695–\\n709, 2005. URL https://api.semanticschola\\nr.org/CorpusID:1152227 .\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\\nAmodei, D. Scaling laws for neural language models,\\n2020.\\nKarras, T., Aittala, M., Aila, T., and Laine, S. Elucidating\\nthe design space of diffusion-based generative models.\\nArXiv , abs/2206.00364, 2022. URL https://api.se\\nmanticscholar.org/CorpusID:249240415 .\\nKarras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila,\\nT., and Laine, S. Analyzing and improving the train-\\ning dynamics of diffusion models. arXiv preprint\\narXiv:2312.02696 , 2023.\\nKingma, D. P. and Gao, R. Understanding diffusion ob-\\njectives as the elbo with simple data augmentation. In\\nThirty-seventh Conference on Neural Information Pro-\\ncessing Systems , 2023.\\nLee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D.,\\nCallison-Burch, C., and Carlini, N. Deduplicating train-\\ning data makes language models better. arXiv preprint\\narXiv:2107.06499 , 2021.\\nLee, S., Kim, B., and Ye, J. C. Minimizing trajectory curva-\\nture of ode-based generative models, 2023.\\nLin, S., Liu, B., Li, J., and Yang, X. Common diffusion noise\\nschedules and sample steps are flawed. In Proceedings\\nof the IEEE/CVF Winter Conference on Applications of\\nComputer Vision , pp. 5404–5411, 2024.\\nLin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\\nmanan, D., Doll ´ar, P., and Zitnick, C. L. Microsoft COCO:\\nCommon Objects in Context , pp. 740–755. Springer In-\\nternational Publishing, 2014. ISBN 9783319106021. doi:\\n14', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 13}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\n10.1007/978-3-319-10602-1 48. URL http://dx.d\\noi.org/10.1007/978-3-319-10602-1_48 .\\nLipman, Y ., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and\\nLe, M. Flow matching for generative modeling. In The\\nEleventh International Conference on Learning Repre-\\nsentations , 2023. URL https://openreview.net\\n/forum?id=PqvMRDCJT9t .\\nLiu, X., Gong, C., and Liu, Q. Flow straight and fast:\\nLearning to generate and transfer data with rectified flow,\\n2022.\\nLiu, X., Zhang, X., Ma, J., Peng, J., and Liu, Q. Instaflow:\\nOne step is enough for high-quality diffusion-based text-\\nto-image generation, 2023.\\nLoshchilov, I. and Hutter, F. Fixing weight decay regular-\\nization in adam. ArXiv , abs/1711.05101, 2017. URL\\nhttps://api.semanticscholar.org/Corp\\nusID:3312944 .\\nLu, C., Zhou, Y ., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-\\nsolver++: Fast solver for guided sampling of diffusion\\nprobabilistic models, 2023.\\nMa, N., Goldstein, M., Albergo, M. S., Boffi, N. M., Vanden-\\nEijnden, E., and Xie, S. Sit: Exploring flow and diffusion-\\nbased generative models with scalable interpolant trans-\\nformers, 2024.\\nNichol, A. Dall-e 2 pre-training mitigations. https:\\n//openai.com/research/dall-e-2-pre-t\\nraining-mitigations , 2022.\\nNichol, A. and Dhariwal, P. Improved denoising diffusion\\nprobabilistic models, 2021.\\nNovelAI. Novelai improvements on stable diffusion, 2022.\\nURL https://blog.novelai.net/novelai\\n-improvements-on-stable-diffusion-e10\\nd38db82ac .\\nPeebles, W. and Xie, S. Scalable diffusion models with\\ntransformers. In 2023 IEEE/CVF International Con-\\nference on Computer Vision (ICCV) . IEEE, 2023. doi:\\n10.1109/iccv51070.2023.00387. URL http://dx.d\\noi.org/10.1109/ICCV51070.2023.00387 .\\nPernias, P., Rampas, D., Richter, M. L., Pal, C. J., and\\nAubreville, M. Wuerstchen: An efficient architecture for\\nlarge-scale text-to-image diffusion models, 2023.\\nPizzi, E., Roy, S. D., Ravindra, S. N., Goyal, P., and Douze,\\nM. A self-supervised descriptor for image copy detection.\\nInProceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition , pp. 14532–14542,\\n2022.Po, R., Yifan, W., Golyanik, V ., Aberman, K., Barron, J. T.,\\nBermano, A. H., Chan, E. R., Dekel, T., Holynski, A.,\\nKanazawa, A., et al. State of the art on diffusion models\\nfor visual computing. arXiv preprint arXiv:2310.07204 ,\\n2023.\\nPodell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn,\\nT., M ¨uller, J., Penna, J., and Rombach, R. Sdxl: Im-\\nproving latent diffusion models for high-resolution image\\nsynthesis, 2023.\\nPooladian, A.-A., Ben-Hamu, H., Domingo-Enrich, C.,\\nAmos, B., Lipman, Y ., and Chen, R. T. Q. Multisam-\\nple flow matching: Straightening flows with minibatch\\ncouplings, 2023.\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\\nJ., Krueger, G., and Sutskever, I. Learning transferable\\nvisual models from natural language supervision, 2021.\\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Man-\\nning, C. D., and Finn, C. Direct Preference Optimiza-\\ntion: Your Language Model is Secretly a Reward Model.\\narXiv:2305.18290 , 2023.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\\nthe limits of transfer learning with a unified text-to-text\\ntransformer, 2019.\\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,\\nM. Hierarchical text-conditional image generation with\\nclip latents, 2022.\\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\\nOmmer, B. High-resolution image synthesis with latent\\ndiffusion models. In 2022 IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) . IEEE,\\n2022. doi: 10.1109/cvpr52688.2022.01042. URL\\nhttp://dx.doi.org/10.1109/CVPR52688.2\\n022.01042 .\\nRonneberger, O., Fischer, P., and Brox, T. U-Net: Convolu-\\ntional Networks for Biomedical Image Segmentation , pp.\\n234–241. Springer International Publishing, 2015. ISBN\\n9783319245744. doi: 10.1007/978-3-319-24574-4 28.\\nURL http://dx.doi.org/10.1007/978-3-3\\n19-24574-4_28 .\\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\\nMa, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\\nM. S., Berg, A. C., and Fei-Fei, L. Imagenet large scale\\nvisual recognition challenge. International Journal of\\nComputer Vision , 115:211 – 252, 2014. URL https:\\n//api.semanticscholar.org/CorpusID:29\\n30547 .\\n15', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 14}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nSaharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans,\\nT., Fleet, D., and Norouzi, M. Palette: Image-to-image\\ndiffusion models. In ACM SIGGRAPH 2022 Conference\\nProceedings , pp. 1–10, 2022a.\\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Den-\\nton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi,\\nS. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J.,\\nand Norouzi, M. Photorealistic text-to-image diffusion\\nmodels with deep language understanding, 2022b.\\nSaharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D. J.,\\nand Norouzi, M. Image super-resolution via iterative\\nrefinement. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence , 45(4):4713–4726, 2022c.\\nSauer, A., Chitta, K., M ¨uller, J., and Geiger, A. Projected\\ngans converge faster. Advances in Neural Information\\nProcessing Systems , 2021.\\nSauer, A., Lorenz, D., Blattmann, A., and Rombach,\\nR. Adversarial diffusion distillation. arXiv preprint\\narXiv:2311.17042 , 2023.\\nSheynin, S., Polyak, A., Singer, U., Kirstain, Y ., Zohar, A.,\\nAshual, O., Parikh, D., and Taigman, Y . Emu edit: Precise\\nimage editing via recognition and generation tasks. arXiv\\npreprint arXiv:2311.10089 , 2023.\\nSinger, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang,\\nS., Hu, Q., Yang, H., Ashual, O., Gafni, O., Parikh, D.,\\nGupta, S., and Taigman, Y . Make-a-video: Text-to-video\\ngeneration without text-video data, 2022.\\nSohl-Dickstein, J. N., Weiss, E. A., Maheswaranathan,\\nN., and Ganguli, S. Deep unsupervised learning using\\nnonequilibrium thermodynamics. ArXiv , abs/1503.03585,\\n2015. URL https://api.semanticscholar.\\norg/CorpusID:14888175 .\\nSomepalli, G., Singla, V ., Goldblum, M., Geiping, J., and\\nGoldstein, T. Diffusion art or digital forgery? investigat-\\ning data replication in diffusion models. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition , pp. 6048–6058, 2023a.\\nSomepalli, G., Singla, V ., Goldblum, M., Geiping, J., and\\nGoldstein, T. Understanding and mitigating copying\\nin diffusion models. arXiv preprint arXiv:2305.20086 ,\\n2023b.\\nSong, J., Meng, C., and Ermon, S. Denoising diffusion\\nimplicit models, 2022.\\nSong, Y . and Ermon, S. Generative modeling by estimating\\ngradients of the data distribution, 2020.Song, Y ., Sohl-Dickstein, J. N., Kingma, D. P., Kumar,\\nA., Ermon, S., and Poole, B. Score-based generative\\nmodeling through stochastic differential equations. ArXiv ,\\nabs/2011.13456, 2020. URL https://api.semant\\nicscholar.org/CorpusID:227209335 .\\nTong, A., Malkin, N., Huguet, G., Zhang, Y ., Rector-Brooks,\\nJ., Fatras, K., Wolf, G., and Bengio, Y . Improving and\\ngeneralizing flow-based generative models with mini-\\nbatch optimal transport, 2023.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\\nis all you need, 2017.\\nVillani, C. Optimal transport: Old and new. 2008. URL\\nhttps://api.semanticscholar.org/Corp\\nusID:118347220 .\\nVincent, P. A connection between score matching and de-\\nnoising autoencoders. Neural Computation , 23:1661–\\n1674, 2011. URL https://api.semanticscho\\nlar.org/CorpusID:5560643 .\\nWallace, B., Dang, M., Rafailov, R., Zhou, L., Lou, A., Pu-\\nrushwalkam, S., Ermon, S., Xiong, C., Joty, S., and Naik,\\nN. Diffusion Model Alignment Using Direct Preference\\nOptimization. arXiv:2311.12908 , 2023.\\nWang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y ., Ji,\\nJ., Yang, Z., Zhao, L., Song, X., et al. Cogvlm: Visual\\nexpert for pretrained language models. arXiv preprint\\narXiv:2311.03079 , 2023.\\nWortsman, M., Liu, P. J., Xiao, L., Everett, K., Alemi, A.,\\nAdlam, B., Co-Reyes, J. D., Gur, I., Kumar, A., Novak,\\nR., Pennington, J., Sohl-dickstein, J., Xu, K., Lee, J.,\\nGilmer, J., and Kornblith, S. Small-scale proxies for\\nlarge-scale transformer training instabilities, 2023.\\nYu, J., Xu, Y ., Koh, J. Y ., Luong, T., Baid, G., Wang, Z., Va-\\nsudevan, V ., Ku, A., Yang, Y ., Ayan, B. K., et al. Scaling\\nAutoregressive Models for Content-Rich Text-to-Image\\nGeneration. arXiv:2206.10789 , 2022.\\nZhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling\\nvision transformers. In CVPR , pp. 12104–12113, 2022.\\nZhang, B. and Sennrich, R. Root mean square layer normal-\\nization, 2019.\\n16', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 15}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nSupplementary\\nA. Background\\nDiffusion Models (Sohl-Dickstein et al., 2015; Song et al., 2020; Ho et al., 2020) generate data by approximating the\\nreverse ODE to a stochastic forward process which transforms data to noise. They have become the standard approach for\\ngenerative modeling of images (Dhariwal & Nichol, 2021; Ramesh et al., 2022; Saharia et al., 2022b; Rombach et al., 2022;\\nBalaji et al., 2022) and videos (Singer et al., 2022; Ho et al., 2022; Esser et al., 2023; Blattmann et al., 2023b; Gupta et al.,\\n2023). Since these models can be derived both via a variational lower bound on the negative likelihood (Sohl-Dickstein et al.,\\n2015) and score matching (Hyv ¨arinen, 2005; Vincent, 2011; Song & Ermon, 2020), various formulations of forward- and\\nreverse processes (Song et al., 2020; Dockhorn et al., 2021), model parameterizations (Ho et al., 2020; Ho & Salimans, 2022;\\nKarras et al., 2022), loss weightings (Ho et al., 2020; Karras et al., 2022) and ODE solvers (Song et al., 2022; Lu et al., 2023;\\nDockhorn et al., 2022) have led to a large number of different training objectives and sampling procedures. More recently,\\nthe seminal works of Kingma & Gao (2023) and Karras et al. (2022) have proposed unified formulations and introduced\\nnew theoretical and practical insights for training (Karras et al., 2022; Kingma & Gao, 2023) and inference (Karras et al.,\\n2022). However, despite these improvements, the trajectories of common ODEs involve partly significant amounts of\\ncurvature (Karras et al., 2022; Liu et al., 2022), which requires increased amounts of solver steps and, thus, renders fast\\ninference difficult. To overcome this, we adopt rectified flow models whose formulation allows for learning straight ODE\\ntrajectories.\\nRectified Flow Models (Liu et al., 2022; Albergo & Vanden-Eijnden, 2022; Lipman et al., 2023) approach generative\\nmodeling by constructing a transport map between two distributions through an ordinary differential equation (ODE). This\\napproach has close connections to continuous normalizing flows (CNF) (Chen et al., 2018) as well as diffusion models.\\nCompared to CNFs, Rectified Flows and Stochastic Interpolants have the advantage that they do not require simulation\\nof the ODE during training. Compared to diffusion models, they can result in ODEs that are faster to simulate than the\\nprobability flow ODE (Song et al., 2020) associated with diffusion models. Nevertheless, they do not result in optimal\\ntransport solutions, and multiple works aim to minimize the trajectory curvature further (Lee et al., 2023; Tong et al., 2023;\\nPooladian et al., 2023). (Dao et al., 2023; Ma et al., 2024) demonstrate the feasibility of rectified flow formulations for\\nclass-conditional image synthesis, (Fischer et al., 2023) for latent-space upsampling, and (Liu et al., 2023) apply the reflow\\nprocedure of (Liu et al., 2022) to distill a pretrained text-to-image model (Rombach et al., 2022). Here, we are interested in\\nrectified flows as the foundation for text-to-image synthesis with fewer sampling steps. We perform an extensive comparison\\nbetween different formulations and loss weightings and propose a new timestep schedule for training of rectified flows with\\nimproved performance.\\nScaling Diffusion Models The transformer architecture (Vaswani et al., 2017) is well known for its scaling properties in\\nNLP (Kaplan et al., 2020) and computer vision tasks (Dosovitskiy et al., 2020; Zhai et al., 2022). For diffusion models,\\nU-Net architectures (Ronneberger et al., 2015) have been the dominant choice (Ho et al., 2020; Rombach et al., 2022; Balaji\\net al., 2022). While some recent works explore diffusion transformer backbones (Peebles & Xie, 2023; Chen et al., 2023;\\nMa et al., 2024), scaling laws for text-to-image diffusion models remain unexplored.\\n17', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 16}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nDetailed pen and ink drawing of a happy pig butcher selling meat in its shop.\\n a massive alien space ship that is shaped like a pretzel.\\nA kangaroo holding a beer,\\nwearing ski goggles and\\npassionately singing silly\\nsongs.\\nAn entire universe inside a\\nbottle sitting on the shelf at\\nwalmart on sale.\\nA cheesburger surfing the\\nvibe wave at night\\nA swamp ogre with a pearl\\nearring by Johannes Vermeer\\nA car made out of vegetables.\\n heat death of the universe,\\nline art\\nA crab made of cheese on a plate\\n Dystopia of thousand of workers picking cherries and feeding them into a machine that runs on steam\\nand is as large as a skyscraper. Written on the side of the machine: ”SD3 Paper”\\ntranslucent pig, inside is a smaller pig.\\n Film still of a long-legged cute big-eye anthropomorphic cheeseburger wearing sneakers relaxing on\\nthe couch in a sparsely decorated living room.\\n18', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 17}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\ndetailed pen and ink drawing of a massive complex alien space ship above a farm in the middle of\\nnowhere.\\nphoto of a bear wearing a suit and tophat in a river in the middle of a forest holding a sign that says ”I\\ncant bear it”.\\ntilt shift aerial photo of a cute city made of sushi on a wooden table in the evening.\\n dark high contrast render of a psychedelic tree of life illuminating dust in a mystical cave.\\nan anthropomorphic fractal person behind the counter at a fractal themed restaurant.\\n beautiful oil painting of a steamboat in a river in the afternoon. On the side of the river is a large brick\\nbuilding with a sign on top that says ¨SD3¨.\\nan anthopomorphic pink donut with a mustache and cowboy hat standing by a log cabin in a forest\\nwith an old 1970s orange truck in the driveway\\nfox sitting in front of a computer in a messy room at night. On the screen is a 3d modeling program\\nwith a line render of a zebra.\\n19', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 18}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nB. On Flow Matching\\nB.1. Details on Simulation-Free Training of Flows\\nFollowing (Lipman et al., 2023), to see that ut(z)generates pt, we note that the continuity equation provides a necessary\\nand sufficient condition (Villani, 2008):\\nd\\ndtpt(x) +∇ ·[pt(x)vt(x)] = 0 ↔vtgenerates probability density path pt. (26)\\nTherefore it suffices to show that\\n−∇ · [ut(z)pt(z)] =−∇ · [Eϵ∼N(0,I)ut(z|ϵ)pt(z|ϵ)\\npt(z)pt(z)] (27)\\n=Eϵ∼N(0,I)− ∇ · [ut(z|ϵ)pt(z|ϵ)] (28)\\n=Eϵ∼N(0,I)d\\ndtpt(z|ϵ) =d\\ndtpt(z), (29)\\nwhere we used the continuity equation Equation (26) for ut(z|ϵ)in line Equation (28) to Equation (29) since ut(z|ϵ)\\ngenerates pt(z|ϵ)and the definition of Equation (6) in line Equation (27)\\nThe equivalence of objectives LFM⇋LCFM (Lipman et al., 2023) follows from\\nLFM(Θ) = Et,pt(z)||vΘ(z, t)−ut(z)||2\\n2 (30)\\n=Et,pt(z)||vΘ(z, t)||2\\n2−2Et,pt(z)⟨vΘ(z, t)|ut(z)⟩+c (31)\\n=Et,pt(z)||vΘ(z, t)||2\\n2−2Et,pt(z|ϵ),p(ϵ)⟨vΘ(z, t)|ut(z|ϵ)⟩+c (32)\\n=Et,pt(z|ϵ),p(ϵ)||vΘ(z, t)−ut(z|ϵ)||2\\n2+c′=LCFM(Θ) + c′(33)\\nwhere c, c′do not depend on Θand line Equation (31) to line Equation (32) follows from:\\nEpt(z|ϵ),p(ϵ)⟨vΘ(z, t)|ut(z|ϵ)⟩=Z\\ndzZ\\ndϵpt(z|ϵ)p(ϵ)⟨vΘ(z, t)|ut(z|ϵ)⟩ (34)\\n=Z\\ndzpt(z)⟨vΘ(z, t)|Z\\ndϵpt(z|ϵ)\\npt(z)p(ϵ)ut(z|ϵ)⟩ (35)\\n=Z\\ndzpt(z)⟨vΘ(z, t)|ut(z)⟩=Ept(z)⟨vΘ(z, t)|ut(z)⟩ (36)\\nwhere we extended withpt(z)\\npt(z)in line Equation (35) and used the definition of Equation (6) in line Equation (35) to\\nEquation (36).\\nB.2. Details on Image and Text Representations\\nLatent Image Representation We follow LDM (Rombach et al., 2022) and use a pretrained autoencoder to represent RGB\\nimages X∈RH×W×3in a smaller latent space x=E(X)∈Rh×w×d. We use a spatial downsampling factor of 8, such\\nthath=H\\n8andw=W\\n8, and experiment with different values for din Section 5.2.1. We always apply the forward process\\nfrom Equation 2 in the latent space, and when sampling a representation xvia Equation 1, we decode it back into pixel\\nspace X=D(x)via the decoder D. We follow Rombach et al. (2022) and normalize the latents by their mean and standard\\ndeviation, which are globally computed over a subset of the training data. Figure 10 shows how generative model training\\nfor different devolves as a function of model capacity, as discussed in Section 5.2.1.\\nText Representation Similar to the encoding of images to latent representations, we also follow previous approaches\\n(Saharia et al., 2022b; Balaji et al., 2022) and encode the text conditioning cusing pretrained, frozen text models. In\\nparticular, for all experiments, we use a combination of CLIP (Radford et al., 2021) models and a encoder-decoder text model.\\nSpecifically, we encode cwith the text encoders of both a CLIP L/14 model of Radford et al. (2021) as well as an OpenCLIP\\nbigG/14 model of Cherti et al. (2023). We concatenate the pooled outputs, of sizes 768and1280 respectively, to obtain\\na vector conditioning cvec∈R2048. We also concatenate the penultimate hidden representations channel-wise to a CLIP\\n20', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 19}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nFigure 10. FID scores after training flow models with different sizes (parameterized via their depth) on the latent space of different\\nautoencoders (4 latent channels, 8 channels and 16 channels) as discussed in Section 5.2.1. As expected, the flow model trained on the\\n16-channel autoencoder space needs more model capacity to achieve similar performance. At depth d= 22 , the gap between 8-chn and\\n16-chn becomes negligible. We opt for the 16-chn model as we ultimately aim to scale to much larger model sizes.\\ncontext conditioning cCLIP\\nctxt∈R77×2048. Next, we encode calso to the final hidden representation, cT5\\nctxt∈R77×4096, of the\\nencoder of a T5-v1.1-XXL model (Raffel et al., 2019). Finally, we zero-pad cCLIP\\nctxtalong the channel axis to 4096 dimensions\\nto match the T5 representation and concatenate it along the sequence axis with cT5\\nctxtto obtain the final context representation\\ncctxt∈R154×4096. These two caption representations, cvecandcctxt, are used in two different ways as described in Section 4.\\nB.3. Preliminaries for the Experiments in Section 5.1.\\nDatasets We use two datasets to account for the missing of a standard text-to-image benchmark. As a widely used dataset,\\nwe convert the ImageNet dataset (Russakovsky et al., 2014) into a dataset suitable for text-to-image models by adding\\ncaptions of the form “a photo of a 〈class name 〉” to images, where 〈class name 〉is randomly chosen from one\\nof the provided names for the image’s class label. As a more realistic text-to-image dataset, we use the CC12M dataset\\n(Changpinyo et al., 2021) for training.\\nOptimization In this experiment, we train all models using a global batch size of 1024 using the AdamW optimizer\\n(Loshchilov & Hutter, 2017) with a learning rate of 10−4and 1000 linear warmup steps. We use mixed-precision training\\nand keep a copy of the model weights which gets updated every 100 training batches with an exponential moving average\\n(EMA) using a decay factor of 0.99. For unconditional diffusion guidance (Ho & Salimans, 2022), we set the outputs of each\\nof the three text encoders independently to zero with a probability of 46.4%, such that we roughly train an unconditional\\nmodel in 10% of all steps.\\nEvaluation As described in Section 5.1, we use CLIP scores, FID and validation losses to evaluate our models regularly\\nduring training on the COCO-2014 validation split (Lin et al., 2014).\\nAs the loss values differ widely in magnitude and variance for different timesteps, we evaluate them in a stratified way on\\neight equally spaced values in the time interval [0,1].\\nTo analyze how different approaches behave under different sampler settings, we produce 1000 samples for each of the\\nsamplers which differ in guidance scales as well as number of sampling steps. We evaluate these samples with CLIP scores\\nusing CLIP L/14 (Radford et al., 2021) and also compute FID between CLIP L/14 image features of these samples and the\\nimages of the validation set. For sampling, we always use a Euler discretization (Euler, 1768) of Equation 1 and six different\\nsettings: 50 steps with classifier-free-guidance scales 1.0, 2.5, 5.0, and 5, 10, 25 steps with classifier-free-guidance scale 5.0.\\nB.4. Improving SNR Samplers for Rectified Flow Models\\nAs described in Section 2, we introduce novel densities π(t)for the timesteps that we use to train our rectified flow models.\\nFigure 11 visualizes the distributions of the logit-normal sampler and the mode sampler introduced in Section 3.1. Notably,\\nas we demonstrate in Section 5.1, the logit-normal sampler outperforms the classic uniform rectified flow formulation (Liu\\net al., 2022) and established diffusion baselines such as EDM (Karras et al., 2022) and LDM-Linear (Rombach et al., 2022).\\n21', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 20}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nFigure 11. The mode (left) and logit-normal (right) distributions that we explore for biasing the sampling of training timesteps.\\n“A raccoon wearing formal clothes, wearing a\\ntophat and holding a cane. The raccoon is\\nholding a garbage bag. Oil painting in the style\\nof abstract cubism.”“A bowl of soup that looks like a monster made\\nout of plasticine”“Two cups of coffee, one with latte art of a\\nheart. The other has latte art of stars.”“A smiling sloth is wearing a leather jacket, a\\ncowboy hat, a kilt and a bowtie. The sloth is\\nholding a quarterstaff and a big book. The sloth\\nis standing on grass a few feet in front of a\\nshiny VW van with flowers painted on it.\\nwide-angle lens from below.”\\nFigure 12. Qualitative effects of scaling. Displayed are examples demonstrating the impact of scaling training steps (left to right: 50k,\\n200k, 350k, 500k) and model sizes (top to bottom: depth=15, 30, 38) on PartiPrompts, highlighting the influence of training duration and\\nmodel complexity.\\n22', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 21}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nC. Direct Preference Optimization\\n“a peaceful lakeside landscape with\\nmigrating herd of sauropods”“a book with the words ‘Don’t Panic¡,\\nwritten on it”2B base\\n 2B w/ DPO\\n 8b base\\n 8b w/ DPO\\nFigure 13. Comparison between base models and DPO-finetuned models. DPO-finetuning generally results in more aesthetically pleasing\\nsamples with better spelling.\\nDirect Preference Optimization (DPO) (Rafailov et al., 2023) is a technique to finetune LLMs with preference data. Recently,\\nthis method has been adapted to preference finetuning of text-to-image diffusion models (Wallace et al., 2023). In this\\nsection, we verify that our model is also amenable to preference optimization. In particular, we apply the method introduced\\nin Wallace et al. (2023) to our 2B and 8B parameter base model. Rather than finetuning the entire model, we introduce\\nlearnable Low-Rank Adaptation (LoRA) matrices (of rank 128) for all linear layers as is common practice. We finetune\\nthese new parameters for 4k and 2k iteration for the 2B and 8B base model, respectively. We then evaluate the resulting\\nmodel in a human preference study using a subset of 128 captions from the Partiprompts set (Yu et al., 2022) (roughly three\\nvoter per prompt and comparison). Figure 14 shows that our base models can be effectively tuned for human preference.\\nFigure 13 shows samples of the respective base models and DPO-finetuned models.\\nD. Finetuning for instruction-based image editing\\nA common approach for training instruction based image editing and general image-to-image diffusion models is to\\nconcatenate the latents of the input image to the noised latents of the diffusion target along the channel dimension before\\nfeeding the input into a U-Net (Brooks et al., 2023; Sheynin et al., 2023; Saharia et al., 2022a;c). We follow the same\\napproach, concatenating input and target along the channels before patching, and demonstrate that the same method is\\napplicable to our proposed architecture. We finetune the 2B parameter base model on a dataset consisting of image-to-image\\nediting tasks similar to the distribution of the InstructPix2Pix dataset (Brooks et al., 2023) as well as inpainting, segmentation,\\ncolorization, deblurring and controlnet tasks similar to Emu Edit and Palette (Sheynin et al., 2023; Saharia et al., 2022a).\\nAs shown in Fig 15 we observe that the resulting 2B Edit model has the capability to manipulate text in a given image, even\\n23', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 22}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nPrompt Quality0102030405060Human Preference [ % ]depth=24 (2B)\\nbase\\nw/ DPO\\nPrompt Qualitydepth=38 (8B)\\nbase\\nw/ DPO\\nFigure 14. Human preference evaluation between base models and DPO-finetuned models. Human evaluators prefer DPO-finetuned\\nmodels for both prompt following and general quality.\\nModel Mem [GB] FP [ms] Storage [kB] Delta [%]\\nV AE (Enc) 0.14 2.45 65.5 13.8\\nCLIP-L 0.49 0.45 121.3 2.6\\nCLIP-G 2.78 2.77 202.2 15.6\\nT5 19.05 17.46 630.7 98.3\\nTable 7. Key figures for preencoding frozen input networks. Mem is the memory required to load the model on the GPU. FP [ms] is\\nthe time per sample for the forward pass with per-device batch size of 32. Storage is the size to save a single sample. Delta [%] is how\\nmuch longer a training step takes, when adding this into the loop for the 2B MMDiT-Model (568ms/it).\\nthough no text manipulation tasks were included in the training data. We were not able to reproduce similar results when\\ntraining a SDXL-based (Podell et al., 2023) editing model on the same data.\\nE. Data Preprocessing for Large-Scale Text-to-Image Training\\nE.1. Precomputing Image and Text Embeddings\\nOur model uses the output of multiple pretrained, frozen networks as inputs (autoencoder latents and text encoder repre-\\nsentations). Since these outputs are constant during training, we precompute them once for the entire dataset. This comes\\nwith two main advantages: (i) The encoders do not need to be available on the GPU during training, lowering the required\\nmemory. (ii) The forward encoding pass is skipped during training, saving time and total needed compute after the first\\nepoch, see Tab. 7.\\nThis approach has two disadvantages: First, random augmentation for each sample every epoch is not possible and we use\\nsquare-center cropping during precomputation of image latents. For finetuning our model at higher resolutions, we specify\\na number of aspect ratio buckets, and resize and crop to the closest bucket first and then precompute in that aspect ratio.\\nSecond, the dense output of the text encoders is particularly large, creating additional storage cost and longer loading times\\nduring training ( c.f. Tab. 7). We save the embeddings of the language models in half precision, as we do not observe a\\ndeterioration in performance in practice.\\nE.2. Preventing Image Memorization\\nIn the context of generative image models memorization of training samples can lead to a number of issues (Somepalli et al.,\\n2023a; Carlini et al., 2023; Somepalli et al., 2023b). To avoid verbatim copies of images by our trained models, we carefully\\nscan our training dataset for duplicated examples and remove them.\\n24', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 23}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nInput Output 1 Output 2\\nWrite ”go small\\ngo home”\\ninstead\\nGO BIG OR GO UNET\\nis written on\\nthe blackboard\\nchange the\\nword to\\nUNOT\\nmake the\\nsign say\\nMMDIT rules\\nFigure 15. Zero Shot Text manipulation and insertion with the 2B Edit model\\nDetails on Deduplication In accordance with the methods outlined by Carlini et al. (2023) and Somepalli et al. (2023a),\\nwe opt for SSCD (Pizzi et al., 2022) as the backbone for the deduplication process. The SSCD algorithm is a state-of-the-art\\ntechnique for detecting near-duplicate images at scale, and it generates high-quality image embeddings that can be used for\\nclustering and other downstream tasks. We also decided to follow Nichol (2022) to decide on a number of clusters N. For\\nour experiments, we use N= 16,000.\\nWe utilize autofaiss (2023) for clustering. autofaiss (2023) is a library that simplifies the process of using Faiss (Facebook AI\\nSimilarity Search) for large-scale clustering tasks. Specifically, leverage FAISS index factory1functionality to train a custom\\nindex with predefined number of centroids. This approach allows for efficient and accurate clustering of high-dimensional\\ndata, such as image embeddings.\\nAlgorithm 1 details our deduplication approach. We ran an experiment to see how much data is removed by different SSCD\\nthreshold as shown in Figure 16b. Based on these results we selected four thresholds for the final run Figure 16a.\\n1https://github.com/facebookresearch/faiss/wiki/The-index-factory\\n25', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 24}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nE.3. Assessing the Efficacy of our Deduplication Efforts\\nCarlini et al. (2023) devise a two-stage data extraction attack that generates images using standard approaches, and flags\\nthose that exceed certain membership inference scoring criteria. Carlini et al. (2023) bias their search towards duplicated\\ntraining examples because these are orders of magnitude more likely to be memorized than non-duplicated examples\\n(Somepalli et al., 2023a;a; Lee et al., 2021).\\nTo assess how well our SSCD-based deduplication works, we follow Carlini et al. (2023) to extract memorized samples from\\nsmall, specifically for this purpose trained models and compare them before and after deduplication. Two main step of the\\nmentioned procedure include: 1) Generate many examples using the diffusion model in the standard sampling manner and\\nwith the known prompts. 2) Perform membership inference to separate the model’s novel generations from those generations\\nwhich are memorized training examples. Algorithm 2 shows the steps to find the memorized samples based on Carlini et al.\\n(2023). Note that we run this techniques two times; one for SD-2.1 model with only exact dedup removal as baseline, and\\nfor a model with the SD2.1 architecture but trained on removed exact duplication and near-duplication using SSCD (Pizzi\\net al., 2022).\\nWe select the 350,000 most-duplicated examples from the training dataset based on SSCD (Pizzi et al., 2022) with threshold\\nof 0.5, and generate 500 candidate images for each text prompt to increase the likelihood of finding memorization. The\\nintuition is that for diffusion models, with high probability Gen(p;r1)≈dGen(p;r2)for two different random initial seeds\\nr1,r2. On the other hand, if Gen(p;r1)≈dGen(p;r2)under some distance measure d, it is likely that these generated\\nsamples are memorized examples. To compute the distance measure dbetween two images, we use a modified Euclidean\\nl2distance. In particular, we found that many generations were often spuriously similar according to l2distance (e.g.,\\nthey all had gray backgrounds). We therefore instead divide each image into 16 non-overlapping 128 ×128 tiles and\\nmeasure the maximum of the l2distance between any pair of image tiles between the two images. Figure 17 shows the\\ncomparison between number of memorized samples, before and after using SSCD with the threshold of 0.5 to remove\\nnear-duplicated samples. Carlini et al. (2023) mark images within clique size of 10 as memorized samples. Here we\\nalso explore different sizes for cliques. For all clique thresholds, SSCD is able to significantly reduce the number of\\nmemorized samples. Specifically, when the clique size is 10, trained SD models on the deduplicated training samples cut off\\nat SSCD = 0.5show a 5×reduction in potentially memorized examples.\\nAlgorithm 1 Finding Duplicate Items in a Cluster\\nRequire: vecs – List of vectors in a single cluster, items – List of item IDs corresponding to vecs, index – FAISS index\\nfor similarity search within the cluster, thresh – Threshold for determining duplicates\\nOutput: dups – Set of duplicate item IDs\\n1:dups←new set ()\\n2:fori←0tolength( vecs)−1do\\n3: qs←vecs[i]{Current vector }\\n4: qid←items [i]{Current item ID }\\n5: lims, D, I←index .range search( qs,thresh )\\n6: ifqid∈dups then\\n7: continue\\n8: end if\\n9: start ←lims[0]\\n10: end←lims[1]\\n11: duplicate indices ←I[start :end]\\n12: duplicate ids←new list()\\n13: forjinduplicate indices do\\n14: ifitems [j]̸=qidthen\\n15: duplicate ids.append( items [j])\\n16: end if\\n17: end for\\n18: dups.update( duplicate ids)\\n19:end for\\n20:Return dups{Final set of duplicate IDs }\\n26', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 25}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\n(a) Final result of SSCD deduplication over the entire dataset\\n (b) Result of SSCD deduplication with various thresholds over 1000\\nrandom clusters\\nFigure 16. Results of deduplicating our training datasets for various filtering thresholds.\\nAlgorithm 2 Detecting Memorization in Generated Images\\nRequire: Set of prompts P, Number of generations per prompt N, Similarity threshold ϵ= 0.15, Memorization threshold\\nT\\nEnsure: Detection of memorized images in generated samples\\n1:Initialize Dto the set of most-duplicated examples\\n2:foreach prompt p∈Pdo\\n3: fori= 1toNdo\\n4: Generate image Gen(p;ri)with random seed ri\\n5: end for\\n6:end for\\n7:foreach pair of generated images xi, xjdo\\n8: ifdistance d(xi, xj)< ϵthen\\n9: Connect xiandxjin graph G\\n10: end if\\n11:end for\\n12:foreach node in Gdo\\n13: Find largest clique containing the node\\n14: ifsize of clique ≥Tthen\\n15: Mark images in the clique as memorized\\n16: end if\\n17:end for\\n27', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 26}),\n",
       " Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nFigure 17. SSCD-based deduplication prevents memorization. To assess how well our SSCD-based deduplication works, we extract\\nmemorized samples from small, specifically for this purpose trained models and compare them before and after deduplication. We plot a\\ncomparison between number of memorized samples, before and after using SSCD with the threshold of 0.5 to remove near-duplicated\\nsamples. Carlini et al. (2023) mark images within clique size of 10 as memorized samples. Here we also explore different sizes for cliques.\\nFor all clique thresholds, SSCD is able to significantly reduce the number of memorized samples. Specifically, when the clique size is 10,\\nmodels on the deduplicated training samples cut off at SSCD = 0.5show a 5×reduction in potentially memorized examples.\\n28', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 27})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"./data/2403.03206 Stable Diffusion 3.pdf\")\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\n",
      "Patrick Esser*Sumith Kulal Andreas Blattmann Rahim Entezari Jonas M ¨uller Harry Saini Yam Levi\n",
      "Dominik Lorenz Axel Sauer Frederic Boesel Dustin Podell Tim Dockhorn Zion English\n",
      "Kyle Lacey Alex Goodwin Yannik Marek Robin Rombach*\n",
      "Stability AI\n",
      "Figure 1. High-resolution samples from our 8B rectified flow model, showcasing its capabilities in typography, precise prompt following\n",
      "and spatial reasoning, attention to fine details, and high image quality across a wide variety of styles.\n",
      "Abstract\n",
      "Diffusion models create data from noise by invert-\n",
      "ing the forward paths of data towards noise and\n",
      "have emerged as a powerful generative modeling\n",
      "technique for high-dimensional, perceptual data\n",
      "such as images and videos. Rectified flow is a re-\n",
      "cent generative model formulation that connects\n",
      "data and noise in a straight line. Despite its better\n",
      "theoretical properties and conceptual simplicity, it\n",
      "is not yet decisively established as standard prac-\n",
      "tice. In this work, we improve existing noise sam-\n",
      "pling techniques for training rectified flow mod-\n",
      "els by biasing them towards perceptually relevant\n",
      "scales. Through a large-scale study, we demon-\n",
      "*Equal contribution . <first.last >@stability.ai.strate the superior performance of this approach\n",
      "compared to established diffusion formulations\n",
      "for high-resolution text-to-image synthesis. Ad-\n",
      "ditionally, we present a novel transformer-based\n",
      "architecture for text-to-image generation that uses\n",
      "separate weights for the two modalities and en-\n",
      "ables a bidirectional flow of information between\n",
      "image and text tokens, improving text comprehen-\n",
      "sion, typography, and human preference ratings.\n",
      "We demonstrate that this architecture follows pre-\n",
      "dictable scaling trends and correlates lower vali-\n",
      "dation loss to improved text-to-image synthesis as\n",
      "measured by various metrics and human evalua-\n",
      "tions. Our largest models outperform state-of-the-\n",
      "art models, and we will make our experimental\n",
      "data, code, and model weights publicly available.\n",
      "1arXiv:2403.03206v1  [cs.CV]  5 Mar 2024\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.character.RecursiveCharacterTextSplitter at 0x15787596560>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对文本进行分块\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=150)\n",
    "text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\\nPatrick Esser*Sumith Kulal Andreas Blattmann Rahim Entezari Jonas M ¨uller Harry Saini Yam Levi\\nDominik Lorenz Axel Sauer Frederic Boesel Dustin Podell Tim Dockhorn Zion English\\nKyle Lacey Alex Goodwin Yannik Marek Robin Rombach*\\nStability AI\\nFigure 1. High-resolution samples from our 8B rectified flow model, showcasing its capabilities in typography, precise prompt following', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 0}),\n",
       " Document(page_content='Stability AI\\nFigure 1. High-resolution samples from our 8B rectified flow model, showcasing its capabilities in typography, precise prompt following\\nand spatial reasoning, attention to fine details, and high image quality across a wide variety of styles.\\nAbstract\\nDiffusion models create data from noise by invert-\\ning the forward paths of data towards noise and\\nhave emerged as a powerful generative modeling\\ntechnique for high-dimensional, perceptual data', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 0}),\n",
       " Document(page_content='ing the forward paths of data towards noise and\\nhave emerged as a powerful generative modeling\\ntechnique for high-dimensional, perceptual data\\nsuch as images and videos. Rectified flow is a re-\\ncent generative model formulation that connects\\ndata and noise in a straight line. Despite its better\\ntheoretical properties and conceptual simplicity, it\\nis not yet decisively established as standard prac-\\ntice. In this work, we improve existing noise sam-', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 0}),\n",
       " Document(page_content='is not yet decisively established as standard prac-\\ntice. In this work, we improve existing noise sam-\\npling techniques for training rectified flow mod-\\nels by biasing them towards perceptually relevant\\nscales. Through a large-scale study, we demon-\\n*Equal contribution . <first.last >@stability.ai.strate the superior performance of this approach\\ncompared to established diffusion formulations\\nfor high-resolution text-to-image synthesis. Ad-\\nditionally, we present a novel transformer-based', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 0}),\n",
       " Document(page_content='compared to established diffusion formulations\\nfor high-resolution text-to-image synthesis. Ad-\\nditionally, we present a novel transformer-based\\narchitecture for text-to-image generation that uses\\nseparate weights for the two modalities and en-\\nables a bidirectional flow of information between\\nimage and text tokens, improving text comprehen-\\nsion, typography, and human preference ratings.\\nWe demonstrate that this architecture follows pre-\\ndictable scaling trends and correlates lower vali-', metadata={'source': './data/2403.03206 Stable Diffusion 3.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs = text_splitter.split_documents(documents)\n",
    "split_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\mm\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# 加载开源词向量模型\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"./sentence-transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.452787309885025,\n",
       " 0.2178151160478592,\n",
       " -0.03589637577533722,\n",
       " -0.14825622737407684,\n",
       " -0.18124639987945557,\n",
       " -0.11459941416978836,\n",
       " 0.021594498306512833,\n",
       " -0.07717909663915634,\n",
       " 0.06909018754959106,\n",
       " -0.1529010534286499]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = embeddings.embed_query(split_docs[0].page_content)\n",
    "print(len(embedding))\n",
    "embedding[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
