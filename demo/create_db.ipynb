{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¦–å…ˆå¯¼å…¥æ‰€éœ€ç¬¬ä¸‰æ–¹åº“\n",
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "# from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·å–æ–‡ä»¶è·¯å¾„å‡½æ•°\n",
    "def get_files(dir_path):\n",
    "    # argsï¼šdir_pathï¼Œç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„\n",
    "    file_list = []\n",
    "    for filepath, dirnames, filenames in os.walk(dir_path):\n",
    "        # os.walk å‡½æ•°å°†é€’å½’éå†æŒ‡å®šæ–‡ä»¶å¤¹\n",
    "        for filename in filenames:\n",
    "            # é€šè¿‡åç¼€ååˆ¤æ–­æ–‡ä»¶ç±»å‹æ˜¯å¦æ»¡è¶³è¦æ±‚\n",
    "            if filename.endswith(\".md\"):\n",
    "                # å¦‚æœæ»¡è¶³è¦æ±‚ï¼Œå°†å…¶ç»å¯¹è·¯å¾„åŠ å…¥åˆ°ç»“æœåˆ—è¡¨\n",
    "                file_list.append(os.path.join(filepath, filename))\n",
    "            elif filename.endswith(\".txt\"):\n",
    "                file_list.append(os.path.join(filepath, filename))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ–‡ä»¶å‡½æ•°\n",
    "def get_text(dir_path):\n",
    "    # argsï¼šdir_pathï¼Œç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„\n",
    "    # é¦–å…ˆè°ƒç”¨ä¸Šæ–‡å®šä¹‰çš„å‡½æ•°å¾—åˆ°ç›®æ ‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨\n",
    "    file_lst = get_files(dir_path)\n",
    "    # docs å­˜æ”¾åŠ è½½ä¹‹åçš„çº¯æ–‡æœ¬å¯¹è±¡\n",
    "    docs = []\n",
    "    # éå†æ‰€æœ‰ç›®æ ‡æ–‡ä»¶\n",
    "    for one_file in tqdm(file_lst):\n",
    "        file_type = one_file.split('.')[-1]\n",
    "        if file_type == 'md':\n",
    "            loader = UnstructuredMarkdownLoader(one_file)\n",
    "        elif file_type == 'txt':\n",
    "            loader = UnstructuredFileLoader(one_file)\n",
    "        else:\n",
    "            # å¦‚æœæ˜¯ä¸ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶ï¼Œç›´æ¥è·³è¿‡\n",
    "            continue\n",
    "        docs.extend(loader.load())\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›®æ ‡æ–‡ä»¶å¤¹\n",
    "tar_dir = [\n",
    "    \"./data/InternLM\",\n",
    "    \"./data/InternLM-XComposer\",\n",
    "    \"./data/lagent\",\n",
    "    \"./data/lmdeploy\",\n",
    "    \"./data/opencompass\",\n",
    "    \"./data/xtuner\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:01<00:00, 21.54it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:01<00:00, 18.94it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 31.03it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:03<00:00, 21.90it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 130/130 [00:07<00:00, 16.56it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:02<00:00, 15.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='', metadata={'source': './data/InternLM\\\\CHANGE_LOG.md'}),\n",
       " Document(page_content='InternLM\\n\\nInternLM\\n\\nHOT\\n\\nã¯ã˜ã‚ã«\\n\\nInternLM ã¯ã€70 å„„ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã€å®Ÿç”¨çš„ãªã‚·ãƒŠãƒªã‚ªã«åˆã‚ã›ãŸãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã—ã¦ã„ã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã«ã¯ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™:\\n\\nä½•å…†ã‚‚ã®é«˜å“è³ªãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«æ´»ç”¨ã—ã€å¼·åŠ›ãªçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ç¢ºç«‹ã—ã¾ã™ã€‚\\n\\n8k ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é•·ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€ã‚ˆã‚Šé•·ã„å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨å¼·åŠ›ãªæ¨è«–æ©Ÿèƒ½ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚\\n\\nãƒ¦ãƒ¼ã‚¶ãŒç‹¬è‡ªã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æŸ”è»Ÿã«æ§‹ç¯‰ã§ãã‚‹ã‚ˆã†ã€æ±ç”¨æ€§ã®é«˜ã„ãƒ„ãƒ¼ãƒ«ã‚»ãƒƒãƒˆã‚’æä¾›ã—ã¾ã™ã€‚\\n\\nã•ã‚‰ã«ã€å¤§è¦æ¨¡ãªä¾å­˜é–¢ä¿‚ã‚’å¿…è¦ã¨ã›ãšã«ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹è»½é‡ãªå­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒæä¾›ã•ã‚Œã¾ã™ã€‚å˜ä¸€ã®ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§ã€æ•°åƒã® GPU ã‚’æŒã¤å¤§è¦æ¨¡ã‚¯ãƒ©ã‚¹ã‚¿ã§ã®äº‹å‰å­¦ç¿’ã¨ã€å˜ä¸€ã® GPU ã§ã®å¾®èª¿æ•´ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€é¡•è‘—ãªæ€§èƒ½æœ€é©åŒ–ã‚’é”æˆã—ã¾ã™ã€‚InternLM ã¯ã€1024GPU ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦ 90% è¿‘ã„ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŠ¹ç‡ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚\\n\\næ–°é—»\\n\\nInternLM-7B-Chat v1.1 ã¯ã€ã‚³ãƒ¼ãƒ‰ ã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã¨é–¢æ•°å‘¼ã³å‡ºã—æ©Ÿèƒ½ã‚’å‚™ãˆã¦ãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¾ã—ãŸã€‚ Lagent ã§è©¦ã™ã“ã¨ãŒã§ãã¾ã™ã€‚\\n\\nInternLM-7B\\n\\nãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡\\n\\nã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®è©•ä¾¡ãƒ„ãƒ¼ãƒ« OpenCompass ã‚’ç”¨ã„ã¦ã€InternLM ã®ç·åˆçš„ãªè©•ä¾¡ã‚’è¡Œã£ãŸã€‚ã“ã®è©•ä¾¡ã§ã¯ã€åˆ†é‡åˆ¥èƒ½åŠ›ã€è¨€èªèƒ½åŠ›ã€çŸ¥è­˜èƒ½åŠ›ã€æ¨è«–èƒ½åŠ›ã€ç†è§£èƒ½åŠ›ã® 5 ã¤ã®æ¬¡å…ƒã‚’ã‚«ãƒãƒ¼ã—ã¾ã—ãŸã€‚ä»¥ä¸‹ã¯è©•ä¾¡çµæœã®ä¸€éƒ¨ã§ã‚ã‚Šã€ãã®ä»–ã®è©•ä¾¡çµæœã«ã¤ã„ã¦ã¯ OpenCompass leaderboard ã‚’ã”è¦§ãã ã•ã„ã€‚\\n\\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\\\\ãƒ¢ãƒ‡ãƒ« InternLM-Chat-7B InternLM-7B LLaMA-7B Baichuan-7B ChatGLM2-6B Alpaca-7B Vicuna-7B C-Eval(Val) 53.2 53.4 24.2 42.7 50.9 28.9 31.2 MMLU 50.8 51.0 35.2* 41.5 46.0 39.7 47.3 AGIEval 42.5 37.6 20.8 24.6 39.0 24.1 26.4 CommonSenseQA 75.2 59.5 65.0 58.8 60.0 68.7 66.7 BUSTM 74.3 50.6 48.5 51.3 55.0 48.8 62.5 CLUEWSC 78.6 59.1 50.3 52.8 59.8 50.3 52.2 MATH 6.4 7.1 2.8 3.0 6.6 2.2 2.8 GSM8K 34.5 31.2 10.1 9.7 29.2 6.0 15.3 HumanEval 14.0 10.4 14.0 9.2 9.2 9.2 11.0 RACE(High) 76.3 57.4 46.9* 28.1 66.3 40.7 54.0\\n\\nè©•ä¾¡çµæœã¯ OpenCompass 20230706 (*å°ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯åŸè‘—è«–æ–‡ã‹ã‚‰ã®å¼•ç”¨ã‚’æ„å‘³ã™ã‚‹)ã‹ã‚‰å–å¾—ã—ãŸã‚‚ã®ã§ã€è©•ä¾¡è¨­å®šã¯ OpenCompass ãŒæä¾›ã™ã‚‹è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚\\n\\nè©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã¯ã€OpenCompass ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚¢ãƒƒãƒ—ã«ã‚ˆã‚Šæ•°å€¤çš„ãªå·®ç•°ãŒç”Ÿã˜ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã®ã§ã€OpenCompass ã®æœ€æ–°ã®è©•ä¾¡çµæœã‚’ã”å‚ç…§ãã ã•ã„ã€‚\\n\\nModel Zoo\\n\\nInternLM 7B ã¨ InternLM 7B ãƒãƒ£ãƒƒãƒˆã¯ã€InternLM ã‚’ä½¿ã£ã¦è¨“ç·´ã•ã‚Œã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¯ 2 ã¤ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚Transformers ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã ã‘ã§ãªãã€InternLM ã‚’ä½¿ã£ã¦ç›´æ¥é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ã•ã‚‰ã«äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„äººé–“ã®å¥½ã¿ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ã“ã¨ã‚‚ã§ãã¾ã™ã€‚\\n\\nãƒ¢ãƒ‡ãƒ« InternLM ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ Weight ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ Transformers ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ Weight ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ InternLM 7B ğŸ¤—internlm/intern-7b InternLM Chat 7B ğŸ¤—internlm/intern-chat-7b InternLM Chat 7B 8k ğŸ¤—internlm/intern-chat-7b-8k\\n\\nåˆ¶é™äº‹é …: å­¦ç¿’éç¨‹ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«ã®å®‰å…¨æ€§ã‚’ç¢ºä¿ã—ã€å€«ç†çš„ãƒ»æ³•çš„è¦ä»¶ã«æº–æ‹ ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã‚ˆã†ãƒ¢ãƒ‡ãƒ«ã«ä¿ƒã™åŠªåŠ›ã‚’è¡Œã£ã¦ããŸãŒã€ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã¨ç¢ºç‡çš„ç”Ÿæˆãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã¯ä¾ç„¶ã¨ã—ã¦äºˆæœŸã›ã¬å‡ºåŠ›ã‚’ç”Ÿæˆã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚ä¾‹ãˆã°ã€ç”Ÿæˆã•ã‚ŒãŸå›ç­”ã«ã¯åè¦‹ã‚„å·®åˆ¥ã€ãã®ä»–ã®æœ‰å®³ãªå†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãã®ã‚ˆã†ãªå†…å®¹ã‚’ä¼æ’­ã—ãªã„ã§ãã ã•ã„ã€‚æœ‰å®³ãªæƒ…å ±ã®ä¼æ’­ã«ã‚ˆã£ã¦ç”Ÿã˜ã‚‹ã„ã‹ãªã‚‹çµæœã«å¯¾ã—ã¦ã‚‚ã€ç§ãŸã¡ã¯è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚\\n\\nTransformers ã‹ã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\\n\\nTransformers ã‚’ä½¿ç”¨ã—ã¦ InternLM 7B ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™:\\n\\n```python\\n\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\ntokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm-chat-7b-v1_1\", trust_remote_code=True)\\nmodel = AutoModelForCausalLM.from_pretrained(\"internlm/internlm-chat-7b-v1_1\", trust_remote_code=True).cuda()\\nmodel = model.eval()\\nresponse, history = model.chat(tokenizer, \"ã“ã‚“ã«ã¡ã¯\", history=[])\\nprint(response)\\nã“ã‚“ã«ã¡ã¯ï¼ã©ã®ã‚ˆã†ã«ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã‹ï¼Ÿ\\nresponse, history = model.chat(tokenizer, \"æ™‚é–“ç®¡ç†ã«ã¤ã„ã¦3ã¤ã®ææ¡ˆã‚’ãŠé¡˜ã„ã—ã¾ã™\", history=history)\\nprint(response)\\nã‚‚ã¡ã‚ã‚“ã§ã™ï¼ä»¥ä¸‹ã«ç°¡æ½”ãªå½¢ã§æ™‚é–“ç®¡ç†ã«é–¢ã™ã‚‹3ã¤ã®ææ¡ˆã‚’ç¤ºã—ã¾ã™ã€‚\\n\\nTo-Doãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã€å„ªå…ˆé †ä½ã‚’ä»˜ã‘ã‚‹: ã‚¿ã‚¹ã‚¯ã‚’æ˜ç¢ºã«ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã€ãã‚Œãã‚Œã®å„ªå…ˆåº¦ã‚’åˆ¤æ–­ã—ã¾ã—ã‚‡ã†ã€‚é‡è¦ã§ç·Šæ€¥ãªã‚¿ã‚¹ã‚¯ã‹ã‚‰é †ã«å–ã‚Šçµ„ã‚€ã“ã¨ã§ã€åŠ¹ç‡çš„ã«ä½œæ¥­ã‚’é€²ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\\n\\næ™‚é–“ã®ãƒ–ãƒ­ãƒƒã‚¯åŒ–ã‚’å®Ÿè·µã™ã‚‹: ä½œæ¥­ã‚’ç‰¹å®šã®æ™‚é–“æ ã«é›†ä¸­ã•ã›ã‚‹ãŸã‚ã€æ™‚é–“ã‚’ãƒ–ãƒ­ãƒƒã‚¯åŒ–ã—ã¾ã—ã‚‡ã†ã€‚ä¾‹ãˆã°ã€æœã®2æ™‚é–“ã‚’ãƒ¡ãƒ¼ãƒ«å¯¾å¿œã«å‰²ã‚Šå½“ã¦ã€åˆå¾Œã®3æ™‚é–“ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«é›†ä¸­ã™ã‚‹ãªã©ã€ã‚¿ã‚¹ã‚¯ã”ã¨ã«æ™‚é–“ã‚’ç¢ºä¿ã™ã‚‹ã“ã¨ãŒåŠ¹æœçš„ã§ã™ã€‚\\n\\nãƒ‡ã‚£ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ’é™¤ã™ã‚‹: é›†ä¸­åŠ›ã‚’ä¿ã¤ãŸã‚ã«ã€ãƒ‡ã‚£ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’æœ€å°é™ã«æŠ‘ãˆã¾ã—ã‚‡ã†ã€‚é€šçŸ¥ã‚’ã‚ªãƒ•ã«ã—ã€SNSã‚„ãƒ¡ãƒ¼ãƒ«ã«æ°—ã‚’å–ã‚‰ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã§ã€ä½œæ¥­åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\\n\\nã“ã‚Œã‚‰ã®ææ¡ˆã‚’å®Ÿè·µã™ã‚‹ã“ã¨ã§ã€æ™‚é–“ç®¡ç†ã®ã‚¹ã‚­ãƒ«ã‚’å‘ä¸Šã•ã›ã€åŠ¹æœçš„ã«æ—¥ã€…ã®ã‚¿ã‚¹ã‚¯ã‚’ã“ãªã—ã¦ã„ãã“ã¨ãŒã§ãã¾ã™ã€‚\\n```\\n\\nå¯¾è©±\\n\\nä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã€ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’é€šã—ã¦ InternLM Chat 7B ãƒ¢ãƒ‡ãƒ«ã¨å¯¾è©±ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™:\\n\\nbash\\npip install streamlit==1.24.0\\npip install transformers==4.30.2\\nstreamlit run web_demo.py\\n\\nãã®åŠ¹æœã¯ä»¥ä¸‹ã®é€šã‚Š\\n\\nãƒ‡ãƒ—ãƒ­ã‚¤\\n\\nLMDeploy ã‚’ä½¿ã£ã¦ã€InternLM ã‚’ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã§ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã€‚\\n\\nã¾ãšã€LMDeploy ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹:\\n\\npython3 -m pip install lmdeploy\\n\\nã‚¯ã‚¤ãƒƒã‚¯ãƒ‡ãƒ—ãƒ­ã‚¤ã«ã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™:\\n\\npython3 -m lmdeploy.serve.turbomind.deploy InternLM-7B /path/to/internlm-7b/model hf\\n\\nãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸå¾Œã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã£ã¦ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã€ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¨ä¼šè©±ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™:\\n\\npython3 -m lmdeploy.serve.client {server_ip_addresss}:33337\\n\\nLMDeploy ã¯ã€InternLM ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ãŸã‚ã®å®Œå…¨ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æä¾›ã—ã¾ã™ã€‚InternLM ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€ãƒ‡ãƒ—ãƒ­ã‚¤ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\\n\\nãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\\n\\nãƒ—ãƒªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«\\n\\nInternLMã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã€ãƒ—ãƒ¬ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å§‹ã‚ã‚‹ã«ã¯ã€ä½¿ç”¨æ³•ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\\n\\nTransformers ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¸ã®å¤‰æ›\\n\\nInternLM ã«ã‚ˆã£ã¦å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®æ§˜ã€…ãªã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¨ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«ãƒ‰ãƒƒã‚­ãƒ³ã‚°ã™ã‚‹ã®ã«ä¾¿åˆ©ãª Hugging Face Transformers å½¢å¼ã«ç°¡å˜ã«å¤‰æ›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚tools/convert2hf.py ã®åŠ©ã‘ã‚’å€Ÿã‚Šã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ä¿å­˜ã•ã‚ŒãŸ weights ã¯ 1 ã¤ã®ã‚³ãƒãƒ³ãƒ‰ã§ transformers å½¢å¼ã«å¤‰æ›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™\\n\\nbash\\npython convert2hf.py --src_folder origin_ckpt/ --tgt_folder hf_ckpt/ --tokenizer tokenizes/tokenizer.model\\n\\nå¤‰æ›å¾Œã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã§ transformers ã¨ã—ã¦èª­ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™\\n\\n```python\\n\\nfrom transformers import AutoTokenizer, AutoModel\\nmodel = AutoModel.from_pretrained(\"hf_ckpt/\", trust_remote_code=True).cuda()\\n```\\n\\nãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ \\n\\nã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£\\n\\nè©³ç´°ã«ã¤ã„ã¦ã¯ã€ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\\n\\nãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\\n\\nInternLM ã¯ã€Flash-Attentionã€Apex ãã®ä»–ã®é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ã‚’æ·±ãçµ±åˆã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚Hybrid Zero æŠ€è¡“ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã§ã€è¨ˆç®—ã¨é€šä¿¡ã®åŠ¹ç‡çš„ãªã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’å®Ÿç¾ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã®ãƒãƒ¼ãƒ‰é–“ã®é€šä¿¡ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¾ã™ã€‚InternLM ã¯ 7B ãƒ¢ãƒ‡ãƒ«ã‚’ 8GPU ã‹ã‚‰ 1024GPU ã¾ã§æ‹¡å¼µã™ã‚‹ã“ã¨ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€1000GPU ã‚¹ã‚±ãƒ¼ãƒ«ã§æœ€å¤§ 90% ã®ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŠ¹ç‡ã€180TFLOPS ä»¥ä¸Šã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã€GPU ã‚ãŸã‚Šå¹³å‡ 3600 ãƒˆãƒ¼ã‚¯ãƒ³/ç§’ä»¥ä¸Šã‚’å®Ÿç¾ã—ã¾ã™ã€‚æ¬¡ã®è¡¨ã¯ã€ç•°ãªã‚‹æ§‹æˆã«ãŠã‘ã‚‹ InternLM ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã™:\\n\\nGPU Number 8 16 32 64 128 256 512 1024 TGS 4078 3939 3919 3944 3928 3920 3835 3625 TFLOPS 193 191 188 188 187 185 186 184\\n\\nTGSã¯ã€GPUã‚ãŸã‚Š1ç§’é–“ã«å‡¦ç†ã•ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®å¹³å‡æ•°ã‚’è¡¨ã—ã¾ã™ã€‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\\n\\nã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ãƒˆ\\n\\næˆ‘ã€…ã¯ã€InternLM ã‚’æ”¹å–„ã—ã€å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«å°½åŠ›ã—ã¦ãã‚ŒãŸã™ã¹ã¦ã®è²¢çŒ®è€…ã«æ„Ÿè¬ã—ã¦ã„ã‚‹ã€‚ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®å‚åŠ ãŒå¼·ãæ¨å¥¨ã•ã‚Œã¾ã™ã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®è²¢çŒ®æ–¹æ³•ã«ã¤ã„ã¦ã¯ã€è²¢çŒ®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\\n\\nè¬è¾\\n\\nInternLM ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã¯ã€ä¸Šæµ· AI ç ”ç©¶æ‰€ã¨æ§˜ã€…ãªå¤§å­¦ã‚„ä¼æ¥­ã®ç ”ç©¶è€…ã«ã‚ˆã£ã¦è²¢çŒ®ã•ã‚ŒãŸã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«æ–°æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ãã‚ŒãŸã™ã¹ã¦ã®è²¢çŒ®è€…ã¨ã€è²´é‡ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã—ã¦ãã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æ„Ÿè¬ã—ãŸã„ã€‚ç§ãŸã¡ã¯ã€ã“ã®ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒã€InternLM ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ç‹¬è‡ªã®ãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™ºã™ã‚‹ãŸã‚ã®æŸ”è»Ÿã§åŠ¹ç‡çš„ãªã‚³ãƒ¼ãƒ‰ãƒ„ãƒ¼ãƒ«ã‚’ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«æä¾›ã—ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«ç¶™ç¶šçš„ã«è²¢çŒ®ã§ãã‚‹ã“ã¨ã‚’é¡˜ã£ã¦ã„ã¾ã™ã€‚2 ã¤ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€flash-attention ã¨ ColossalAI ã«æ„Ÿè¬ã—ã¾ã™ã€‚\\n\\nãƒ©ã‚¤ã‚»ãƒ³ã‚¹\\n\\nã‚³ãƒ¼ãƒ‰ã¯ Apache-2.0 ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ãŠã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã•ã¯å­¦è¡“ç ”ç©¶ã®ãŸã‚ã«å®Œå…¨ã«ã‚ªãƒ¼ãƒ—ãƒ³ã§ã€ç„¡æ–™ ã®å•†ç”¨åˆ©ç”¨ã‚‚è¨±å¯ã•ã‚Œã¦ã„ã¾ã™ã€‚å•†ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ç”³è«‹ã¯ã€ç”³è«‹ãƒ•ã‚©ãƒ¼ãƒ ï¼ˆè‹±èªï¼‰/ç”³è¯·è¡¨ï¼ˆä¸­æ–‡ï¼‰ã«ã”è¨˜å…¥ãã ã•ã„ã€‚ãã®ä»–ã®ã”è³ªå•ã‚„ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦ã¯ã€internlm@pjlab.org.cn ã¾ã§ã”é€£çµ¡ãã ã•ã„ã€‚\\n\\nå¼•ç”¨\\n\\n@misc{2023internlm,\\n    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},\\n    author={InternLM Team},\\n    howpublished = {\\\\url{https://github.com/InternLM/InternLM}},\\n    year={2023}\\n}', metadata={'source': './data/InternLM\\\\README-ja-JP.md'}),\n",
       " Document(page_content='InternLM\\n\\nä¹¦ç”ŸÂ·æµ¦è¯­ å®˜ç½‘\\n\\nHOT\\n\\nğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„ Discord å’Œ å¾®ä¿¡ç¤¾åŒº\\n\\nç®€ä»‹\\n\\nInternLM æ˜¯ä¸€ä¸ªå¼€æºçš„è½»é‡çº§è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æ”¯æŒå¤§æ¨¡å‹è®­ç»ƒè€Œæ— éœ€å¤§é‡çš„ä¾èµ–ã€‚é€šè¿‡å•ä¸€çš„ä»£ç åº“ï¼Œå®ƒæ”¯æŒåœ¨æ‹¥æœ‰æ•°åƒä¸ª GPU çš„å¤§å‹é›†ç¾¤ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨å•ä¸ª GPU ä¸Šè¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶å®ç°äº†å“è¶Šçš„æ€§èƒ½ä¼˜åŒ–ã€‚åœ¨1024ä¸ª GPU ä¸Šè®­ç»ƒæ—¶ï¼ŒInternLM å¯ä»¥å®ç°è¿‘90%çš„åŠ é€Ÿæ•ˆç‡ã€‚\\n\\nåŸºäºInternLMè®­ç»ƒæ¡†æ¶ï¼Œæˆ‘ä»¬å·²ç»å‘å¸ƒäº†ä¸¤ä¸ªå¼€æºçš„é¢„è®­ç»ƒæ¨¡å‹ï¼šInternLM-7B å’Œ InternLM-20Bã€‚\\n\\næ›´æ–°\\n\\n[20230920] InternLM-20B å·²å‘å¸ƒï¼ŒåŒ…æ‹¬åŸºç¡€ç‰ˆå’Œå¯¹è¯ç‰ˆã€‚\\n\\n[20230822] InternLM-7B-Chat v1.1 å·²å‘å¸ƒï¼Œå¢åŠ äº†ä»£ç è§£é‡Šå™¨å’Œå‡½æ•°è°ƒç”¨èƒ½åŠ›ã€‚æ‚¨å¯ä»¥ä½¿ç”¨\\n\\nLagent è¿›è¡Œå°è¯•ã€‚\\n\\nModel Zoo\\n\\næˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸‰ä¸ªå¹³å°ä¸Šå‘å¸ƒï¼šTransformersã€ModelScope å’Œ OpenXLabã€‚\\n\\nModel Transformers ModelScope OpenXLab å‘å¸ƒæ—¥æœŸ InternLM Chat 20B ğŸ¤—internlm/internlm-chat-20b Shanghai_AI_Laboratory/internlm-chat-20b 2023-09-20 InternLM 20B ğŸ¤—internlm/internlm-20b Shanghai_AI_Laboratory/internlm-20b 2023-09-20 InternLM Chat 7B v1.1 ğŸ¤—internlm/internlm-chat-7b-v1.1 Shanghai_AI_Laboratory/internlm-chat-7b-v1_1 2023-08-22 InternLM 7B ğŸ¤—internlm/internlm-7b Shanghai_AI_Laboratory/internlm-7b 2023-07-06 InternLM Chat 7B ğŸ¤—internlm/internlm-chat-7b Shanghai_AI_Laboratory/internlm-chat-7b 2023-07-06 InternLM Chat 7B 8k ğŸ¤—internlm/internlm-chat-7b-8k Shanghai_AI_Laboratory/internlm-chat-7b-8k 2023-07-06\\n\\nä½¿ç”¨æ¡ˆä¾‹\\n\\né€šè¿‡ Transformers åŠ è½½\\n\\né€šè¿‡ä»¥ä¸‹çš„ä»£ç ä» Transformers åŠ è½½ InternLM æ¨¡å‹ ï¼ˆå¯ä¿®æ”¹æ¨¡å‹åç§°æ›¿æ¢ä¸åŒçš„æ¨¡å‹ï¼‰\\n\\n```python\\n\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\ntokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm-chat-7b\", trust_remote_code=True)\\nmodel = AutoModelForCausalLM.from_pretrained(\"internlm/internlm-chat-7b\", trust_remote_code=True).cuda()\\nmodel = model.eval()\\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\\nprint(response)\\nä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\\nresponse, history = model.chat(tokenizer, \"è¯·æä¾›ä¸‰ä¸ªç®¡ç†æ—¶é—´çš„å»ºè®®ã€‚\", history=history)\\nprint(response)\\nå½“ç„¶å¯ä»¥ï¼ä»¥ä¸‹æ˜¯ä¸‰ä¸ªç®¡ç†æ—¶é—´çš„å»ºè®®ï¼š\\n1. åˆ¶å®šè®¡åˆ’ï¼šåˆ¶å®šä¸€ä¸ªè¯¦ç»†çš„è®¡åˆ’ï¼ŒåŒ…æ‹¬æ¯å¤©è¦å®Œæˆçš„ä»»åŠ¡å’Œæ´»åŠ¨ã€‚è¿™å°†æœ‰åŠ©äºæ‚¨æ›´å¥½åœ°ç»„ç»‡æ—¶é—´ï¼Œå¹¶ç¡®ä¿æ‚¨èƒ½å¤ŸæŒ‰æ—¶å®Œæˆä»»åŠ¡ã€‚\\n2. ä¼˜å…ˆçº§ï¼šå°†ä»»åŠ¡æŒ‰ç…§ä¼˜å…ˆçº§æ’åºï¼Œå…ˆå®Œæˆæœ€é‡è¦çš„ä»»åŠ¡ã€‚è¿™å°†ç¡®ä¿æ‚¨èƒ½å¤Ÿåœ¨æœ€çŸ­çš„æ—¶é—´å†…å®Œæˆæœ€é‡è¦çš„ä»»åŠ¡ï¼Œä»è€ŒèŠ‚çœæ—¶é—´ã€‚\\n3. é›†ä¸­æ³¨æ„åŠ›ï¼šé¿å…åˆ†å¿ƒï¼Œé›†ä¸­æ³¨æ„åŠ›å®Œæˆä»»åŠ¡ã€‚å…³é—­ç¤¾äº¤åª’ä½“å’Œç”µå­é‚®ä»¶é€šçŸ¥ï¼Œä¸“æ³¨äºä»»åŠ¡ï¼Œè¿™å°†å¸®åŠ©æ‚¨æ›´å¿«åœ°å®Œæˆä»»åŠ¡ï¼Œå¹¶å‡å°‘é”™è¯¯çš„å¯èƒ½æ€§ã€‚\\n```\\n\\né€šè¿‡ ModelScope åŠ è½½\\n\\né€šè¿‡ä»¥ä¸‹çš„ä»£ç ä» ModelScope åŠ è½½ InternLM æ¨¡å‹ ï¼ˆå¯ä¿®æ”¹æ¨¡å‹åç§°æ›¿æ¢ä¸åŒçš„æ¨¡å‹ï¼‰\\n\\npython\\nfrom modelscope import snapshot_download, AutoTokenizer, AutoModelForCausalLM\\nimport torch\\nmodel_dir = snapshot_download(\\'Shanghai_AI_Laboratory/internlm-chat-7b-v1_1\\', revision=\\'v1.0.0\\')\\ntokenizer = AutoTokenizer.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True,torch_dtype=torch.float16)\\nmodel = AutoModelForCausalLM.from_pretrained(model_dir,device_map=\"auto\",  trust_remote_code=True,torch_dtype=torch.float16)\\nmodel = model.eval()\\nresponse, history = model.chat(tokenizer, \"hello\", history=[])\\nprint(response)\\nresponse, history = model.chat(tokenizer, \"please provide three suggestions about time management\", history=history)\\nprint(response)\\n\\né€šè¿‡å‰ç«¯ç½‘é¡µå¯¹è¯\\n\\nå¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç å¯åŠ¨ä¸€ä¸ªå‰ç«¯çš„ç•Œé¢æ¥ä¸ InternLM Chat 7B æ¨¡å‹è¿›è¡Œäº¤äº’\\n\\nbash\\npip install streamlit==1.24.0\\npip install transformers==4.30.2\\nstreamlit run web_demo.py\\n\\næ•ˆæœå¦‚ä¸‹\\n\\nåŸºäºInternLMé«˜æ€§èƒ½éƒ¨ç½²\\n\\næˆ‘ä»¬ä½¿ç”¨ LMDeploy å®Œæˆ InternLM çš„ä¸€é”®éƒ¨ç½²ã€‚\\n\\né¦–å…ˆå®‰è£… LMDeploy:\\n\\npython3 -m pip install lmdeploy\\n\\nå¿«é€Ÿçš„éƒ¨ç½²å‘½ä»¤å¦‚ä¸‹ï¼š\\n\\npython3 -m lmdeploy.serve.turbomind.deploy InternLM-7B /path/to/internlm-7b/model hf\\n\\nåœ¨å¯¼å‡ºæ¨¡å‹åï¼Œä½ å¯ä»¥ç›´æ¥é€šè¿‡å¦‚ä¸‹å‘½ä»¤å¯åŠ¨æœåŠ¡ä¸€ä¸ªæœåŠ¡å¹¶å’Œéƒ¨ç½²åçš„æ¨¡å‹å¯¹è¯\\n\\npython3 -m lmdeploy.serve.client {server_ip_addresss}:33337\\n\\nLMDeploy æ”¯æŒäº† InternLM éƒ¨ç½²çš„å®Œæ•´æµç¨‹ï¼Œè¯·å‚è€ƒ éƒ¨ç½²æ•™ç¨‹ äº†è§£ InternLM çš„æ›´å¤šéƒ¨ç½²ç»†èŠ‚ã€‚\\n\\nå¾®è°ƒ&è®­ç»ƒ\\n\\né¢„è®­ç»ƒä¸å¾®è°ƒä½¿ç”¨æ•™ç¨‹\\n\\nè¯·å‚è€ƒä½¿ç”¨æ•™ç¨‹å¼€å§‹InternLMçš„å®‰è£…ã€æ•°æ®å¤„ç†ã€é¢„è®­ç»ƒä¸å¾®è°ƒã€‚\\n\\nè½¬æ¢ä¸º Transformers æ ¼å¼ä½¿ç”¨\\n\\né€šè¿‡ InternLM è¿›è¡Œè®­ç»ƒçš„æ¨¡å‹å¯ä»¥å¾ˆè½»æ¾åœ°è½¬æ¢ä¸º HuggingFace Transformers æ ¼å¼ï¼Œæ–¹ä¾¿ä¸ç¤¾åŒºå„ç§å¼€æºé¡¹ç›®æ— ç¼å¯¹æ¥ã€‚å€ŸåŠ© tools/transformers/convert2hf.py å¯ä»¥å°†è®­ç»ƒä¿å­˜çš„æƒé‡ä¸€é”®è½¬æ¢ä¸º transformers æ ¼å¼\\n\\nbash\\npython tools/transformers/convert2hf.py --src_folder origin_ckpt/ --tgt_folder hf_ckpt/ --tokenizer ./tools/V7_sft.model\\n\\nè½¬æ¢ä¹‹åå¯ä»¥é€šè¿‡ä»¥ä¸‹çš„ä»£ç åŠ è½½ä¸º transformers\\n\\n```python\\n\\nfrom transformers import AutoTokenizer, AutoModel\\nmodel = AutoModel.from_pretrained(\"hf_ckpt/\", trust_remote_code=True).cuda()\\n```\\n\\nè®­ç»ƒç³»ç»Ÿ\\n\\nç³»ç»Ÿç»“æ„\\n\\nè¯·å‚è€ƒç³»ç»Ÿç»“æ„æ–‡æ¡£è¿›ä¸€æ­¥äº†è§£ã€‚\\n\\nè®­ç»ƒæ€§èƒ½\\n\\nInternLM æ·±åº¦æ•´åˆäº† Flash-Attention, Apex ç­‰é«˜æ€§èƒ½æ¨¡å‹ç®—å­ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚é€šè¿‡æ„å»º Hybrid Zero æŠ€æœ¯ï¼Œå®ç°è®¡ç®—å’Œé€šä¿¡çš„é«˜æ•ˆé‡å ï¼Œå¤§å¹…é™ä½äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„è·¨èŠ‚ç‚¹é€šä¿¡æµé‡ã€‚InternLM æ”¯æŒ 7B æ¨¡å‹ä» 8 å¡æ‰©å±•åˆ° 1024 å¡ï¼Œåƒå¡è§„æ¨¡ä¸‹åŠ é€Ÿæ•ˆç‡å¯é«˜è¾¾ 90%ï¼Œè®­ç»ƒååè¶…è¿‡ 180TFLOPSï¼Œå¹³å‡å•å¡æ¯ç§’å¤„ç†çš„ token æ•°é‡è¶…è¿‡3600ã€‚ä¸‹è¡¨ä¸º InternLM åœ¨ä¸åŒé…ç½®ä¸‹çš„æ‰©å±•æ€§æµ‹è¯•æ•°æ®ï¼š\\n\\nGPU Number 8 16 32 64 128 256 512 1024 TGS 4078 3939 3919 3944 3928 3920 3835 3625 TFLOPS 193 191 188 188 187 185 186 184\\n\\nTGS ä»£è¡¨å¹³å‡æ¯GPUæ¯ç§’å¯ä»¥å¤„ç†çš„ Token æ•°é‡ã€‚æ›´å¤šçš„æ€§èƒ½æµ‹è¯•æ•°æ®å¯å‚è€ƒè®­ç»ƒæ€§èƒ½æ–‡æ¡£è¿›ä¸€æ­¥äº†è§£ã€‚\\n\\nè´¡çŒ®\\n\\næˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ä¸ºæ”¹è¿›å’Œæå‡ InternLM æ‰€ä½œå‡ºçš„åŠªåŠ›ã€‚éå¸¸æ¬¢è¿ç¤¾åŒºç”¨æˆ·èƒ½å‚ä¸è¿›é¡¹ç›®ä¸­æ¥ã€‚è¯·å‚è€ƒè´¡çŒ®æŒ‡å—æ¥äº†è§£å‚ä¸é¡¹ç›®è´¡çŒ®çš„ç›¸å…³æŒ‡å¼•ã€‚\\n\\nè‡´è°¢\\n\\nInternLM ä»£ç åº“æ˜¯ä¸€æ¬¾ç”±ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤å’Œæ¥è‡ªä¸åŒé«˜æ ¡ã€ä¼ä¸šçš„ç ”å‘äººå‘˜å…±åŒå‚ä¸è´¡çŒ®çš„å¼€æºé¡¹ç›®ã€‚æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰ä¸ºé¡¹ç›®æä¾›æ–°åŠŸèƒ½æ”¯æŒçš„è´¡çŒ®è€…ï¼Œä»¥åŠæä¾›å®è´µåé¦ˆçš„ç”¨æˆ·ã€‚ æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªå·¥å…·ç®±å’ŒåŸºå‡†æµ‹è¯•å¯ä»¥ä¸ºç¤¾åŒºæä¾›çµæ´»é«˜æ•ˆçš„ä»£ç å·¥å…·ï¼Œä¾›ç”¨æˆ·å¾®è°ƒ InternLM å¹¶å¼€å‘è‡ªå·±çš„æ–°æ¨¡å‹ï¼Œä»è€Œä¸æ–­ä¸ºå¼€æºç¤¾åŒºæä¾›è´¡çŒ®ã€‚ç‰¹åˆ«é¸£è°¢flash-attention ä¸ ColossalAI ä¸¤é¡¹å¼€æºé¡¹ç›®ã€‚\\n\\nå¼€æºè®¸å¯è¯\\n\\næœ¬ä»“åº“çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºã€‚æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œä¹Ÿå¯ç”³è¯·å…è´¹çš„å•†ä¸šä½¿ç”¨æˆæƒï¼ˆç”³è¯·è¡¨ï¼‰ã€‚å…¶ä»–é—®é¢˜ä¸åˆä½œè¯·è”ç³» internlm@pjlab.org.cnã€‚\\n\\nå¼•ç”¨\\n\\n@misc{2023internlm,\\n    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},\\n    author={InternLM Team},\\n    howpublished = {\\\\url{https://github.com/InternLM/InternLM}},\\n    year={2023}\\n}', metadata={'source': './data/InternLM\\\\README-zh-Hans.md'}),\n",
       " Document(page_content='InternLM\\n\\nInternLM\\n\\nHOT\\n\\nğŸ‘‹ join us on Discord and WeChat\\n\\nIntroduction\\n\\nInternLM is an open-sourced lightweight training framework aims to  support model pre-training without the need for extensive dependencies. With a single codebase, it supports pre-training on large-scale clusters with thousands of GPUs, and fine-tuning on a single GPU while achieving remarkable performance optimizations. InternLM achieves nearly 90% acceleration efficiency during training on 1024 GPUs.\\n\\nBased on the InternLM training framework, we have released two open-sourced pretrained model InternLM-7B and InternLM-20B.\\n\\nNews\\n\\n[20230920] InternLM-20B is released with base and chat versions.\\n\\n[20230822] InternLM-7B-Chat v1.1 is released with code interpreter and function calling capability. You can try it with\\n\\nLagent.\\n\\nModel Zoo\\n\\nOur models are released in three platforms: Transformers, ModelScope and OpenXLab.\\n\\nThere are two kinds of model weights: \\n  1. huggingface type(marked as HF)\\n  2. original model weight(marked as Original), providing in OpenXLab, which can be loaded by InternLM and finetuned directly.\\n\\nModel Transformers(HF) ModelScope(HF) OpenXLab(HF) OpenXLab(Original) Release Date InternLM Chat 20B ğŸ¤—internlm/internlm-chat-20b Shanghai_AI_Laboratory/internlm-chat-20b 2023-09-20 InternLM 20B ğŸ¤—internlm/internlm-20b Shanghai_AI_Laboratory/internlm-20b 2023-09-20 InternLM Chat 7B v1.1 ğŸ¤—internlm/internlm-chat-7b-v1.1 Shanghai_AI_Laboratory/internlm-chat-7b-v1_1 2023-08-22 InternLM 7B ğŸ¤—internlm/internlm-7b Shanghai_AI_Laboratory/internlm-7b 2023-07-06 InternLM Chat 7B ğŸ¤—internlm/internlm-chat-7b Shanghai_AI_Laboratory/internlm-chat-7b 2023-07-06 InternLM Chat 7B 8k ğŸ¤—internlm/internlm-chat-7b-8k Shanghai_AI_Laboratory/internlm-chat-7b-8k 2023-07-06\\n\\nIntroduction\\n\\nInternLM-20B was pre-trained on over 2.3T Tokens containing high-quality English, Chinese, and code data. Additionally, the Chat version has undergone SFT and RLHF training, enabling it to better and more securely meet users\\' needs.\\n\\nIn terms of model structure, InternLM-20B opted for a deeper architecture, with a depth set at 60 layers. This surpasses the conventional 7B and 13B models that utilize 32 or 40 layers. When parameters are limited, increasing the number of layers can enhance the model\\'s overall capability. Furthermore, compared to InternLM-7B, the pre-training data used for InternLM-20B underwent higher quality cleansing and was supplemented with data rich in knowledge and designed for reinforcing understanding and reasoning capabilities. As a result, it exhibits significant improvements in understanding, reasoning, mathematical, and programming abilitiesâ€”all of which test the technical proficiency of language models. Overall, InternLM-20B features the following characteristics:\\n- Outstanding overall performance\\n- Strong utility invocation capability\\n- Supports a 16k context length (Through inference extrapolation)\\n- Better value alignment.\\n\\nPerformance Evaluation\\n\\nOn the 5 capability dimensions proposed by OpenCompass, InternLM-20B has achieved excellent results (the bolded scores represent the best performances within the 13B-33B parameter range).\\n\\nCapability Llama-13B Llama2-13B Baichuan2-13B InternLM-20B Llama-33B Llama-65B Llama2-70B Language 42.5 47 47.5 55 44.6 47.1 51.6 Knowledge 58.2 58.3 48.9 60.1 64 66 67.7 Understanding 45.5 50.9 58.1 67.3 50.6 54.2 60.8 Reasoning 42.7 43.6 44.2 54.9 46.4 49.8 55 Examination 37.3 45.2 51.8 62.5 47.4 49.7 57.3 Overall 43.8 47.3 49.4 59.2 48.9 51.9 57.4\\n\\nThe table below compares the performance of mainstream open-source models on some influential and typical datasets.\\n\\nBenchmarks Llama-13B Llama2-13B Baichuan2-13B InternLM-20B Llama-33B Llama-65B Llama2-70B Examination MMLU 47.73 54.99 59.55 62.05 58.73 63.71 69.75 C-Eval (val) 31.83 41.4 59.01 58.8 37.47 40.36 50.13 AGI-Eval 22.03 30.93 37.37 44.58 33.53 33.92 40.02 Knowledge BoolQ 78.75 82.42 67 87.46 84.43 86.61 87.74 TriviaQA 52.47 59.36 46.61 57.26 66.24 69.79 70.71 NaturalQuestions 20.17 24.85 16.32 25.15 30.89 33.41 34.16 Understanding CMRC 9.26 31.59 29.85 68.78 14.17 34.73 43.74 CSL 55 58.75 63.12 65.62 57.5 59.38 60 RACE (middle) 53.41 63.02 68.94 86.35 64.55 72.35 81.55 RACE (high) 47.63 58.86 67.18 83.28 62.61 68.01 79.93 XSum 20.37 23.37 25.23 35.54 20.55 19.91 25.38 Reasoning WinoGrande 64.64 64.01 67.32 69.38 66.85 69.38 69.77 BBH 37.93 45.62 48.98 52.51 49.98 58.38 64.91 GSM8K 20.32 29.57 52.62 52.62 42.3 54.44 63.31 PIQA 79.71 79.76 78.07 80.25 81.34 82.15 82.54 Programming HumanEval 14.02 18.9 17.07 25.61 17.68 18.9 26.22 MBPP 20.6 26.8 30.8 35.6 28.4 33.6 39.6\\n\\nOverall, InternLM-20B comprehensively outperforms open-source models in the 13B parameter range in terms of overall capabilities, and on inference evaluation sets, it approaches or even surpasses the performance of Llama-65B.\\n\\nThe evaluation results were obtained from OpenCompass 20230920.\\n\\nThe evaluation data may have numerical differences due to the version iteration of OpenCompass, so please refer to the latest evaluation results of OpenCompass.\\n\\nLimitations: Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information.\\n\\nUsage Examples\\n\\nImport from Transformers\\n\\nTo load the InternLM 7B Chat model using Transformers, use the following code:\\n\\n```python\\n\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\ntokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm-chat-7b-v1_1\", trust_remote_code=True)\\nmodel = AutoModelForCausalLM.from_pretrained(\"internlm/internlm-chat-7b-v1_1\", trust_remote_code=True).cuda()\\nmodel = model.eval()\\nresponse, history = model.chat(tokenizer, \"hello\", history=[])\\nprint(response)\\nHello! How can I help you today?\\nresponse, history = model.chat(tokenizer, \"please provide three suggestions about time management\", history=history)\\nprint(response)\\nSure, here are three tips for effective time management:\\n\\nPrioritize tasks based on importance and urgency: Make a list of all your tasks and categorize them into \"important and urgent,\" \"important but not urgent,\" and \"not important but urgent.\" Focus on completing the tasks in the first category before moving on to the others.\\n\\nUse a calendar or planner: Write down deadlines and appointments in a calendar or planner so you don\\'t forget them. This will also help you schedule your time more effectively and avoid overbooking yourself.\\n\\nMinimize distractions: Try to eliminate any potential distractions when working on important tasks. Turn off notifications on your phone, close unnecessary tabs on your computer, and find a quiet place to work if possible.\\n\\nRemember, good time management skills take practice and patience. Start with small steps and gradually incorporate these habits into your daily routine.\\n```\\n\\nImport from ModelScope\\n\\nTo load the InternLM model using ModelScope, use the following code:\\n\\npython\\nfrom modelscope import snapshot_download, AutoTokenizer, AutoModelForCausalLM\\nimport torch\\nmodel_dir = snapshot_download(\\'Shanghai_AI_Laboratory/internlm-chat-7b-v1_1\\', revision=\\'v1.0.0\\')\\ntokenizer = AutoTokenizer.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True,torch_dtype=torch.float16)\\nmodel = AutoModelForCausalLM.from_pretrained(model_dir,device_map=\"auto\",  trust_remote_code=True,torch_dtype=torch.float16)\\nmodel = model.eval()\\nresponse, history = model.chat(tokenizer, \"hello\", history=[])\\nprint(response)\\nresponse, history = model.chat(tokenizer, \"please provide three suggestions about time management\", history=history)\\nprint(response)\\n\\nDialogue\\n\\nYou can interact with the InternLM Chat 7B model through a frontend interface by running the following code:\\n\\nbash\\npip install streamlit==1.24.0\\npip install transformers==4.30.2\\nstreamlit run web_demo.py\\n\\nThe effect is as follows\\n\\nDeployment\\n\\nWe use LMDeploy to complete the one-click deployment of InternLM.\\n\\nFirst, install LMDeploy:\\n\\npython3 -m pip install lmdeploy\\n\\nUse the following command for quick deployment:\\n\\npython3 -m lmdeploy.serve.turbomind.deploy InternLM-7B /path/to/internlm-7b/model hf\\n\\nAfter exporting the model, you can start a server and have a conversation with the deployed model using the following command:\\n\\npython3 -m lmdeploy.serve.client {server_ip_addresss}:33337\\n\\nLMDeploy provides a complete workflow for deploying InternLM. Please refer to the deployment tutorial for more details on deploying InternLM.\\n\\nFine-tuning & Training\\n\\nPre-training and Fine-tuning Tutorial\\n\\nPlease refer to Usage Tutorial to start InternLM installation, data processing, pre-training and fine-tuning.\\n\\nConvert to Transformers Format\\n\\nThe model trained by InternLM can be easily converted to HuggingFace Transformers format, which is convenient for seamless docking with various open source projects in the community. With the help of tools/transformers/convert2hf.py, the weights saved during training can be converted into transformers format with one command\\n\\nbash\\npython tools/transformers/convert2hf.py --src_folder origin_ckpt/ --tgt_folder hf_ckpt/ --tokenizer ./tools/V7_sft.model\\n\\nAfter conversion, it can be loaded as transformers by the following code\\n\\n```python\\n\\nfrom transformers import AutoTokenizer, AutoModel\\nmodel = AutoModel.from_pretrained(\"hf_ckpt/\", trust_remote_code=True).cuda()\\n```\\n\\nTraining System\\n\\nSystem Architecture\\n\\nPlease refer to the System Architecture document for further details.\\n\\nTraining Performance\\n\\nInternLM deeply integrates Flash-Attention, Apex and other high-performance model operators to improve training efficiency. By building the Hybrid Zero technique, it achieves efficient overlap of computation and communication, significantly reducing cross-node communication traffic during training. InternLM supports expanding the 7B model from 8 GPUs to 1024 GPUs, with an acceleration efficiency of up to 90% at the thousand-GPU scale, a training throughput of over 180 TFLOPS, and an average of over 3600 tokens per GPU per second. The following table shows InternLM\\'s scalability test data at different configurations:\\n\\nGPU Number 8 16 32 64 128 256 512 1024 TGS 4078 3939 3919 3944 3928 3920 3835 3625 TFLOPS 193 191 188 188 187 185 186 184\\n\\nTGS represents the average number of tokens processed per GPU per second. For more performance test data, please refer to the Training Performance document for further details.\\n\\nContribution\\n\\nWe appreciate all the contributors for their efforts to improve and enhance InternLM. Community users are highly encouraged to participate in the project. Please refer to the contribution guidelines for instructions on how to contribute to the project.\\n\\nAcknowledgements\\n\\nInternLM codebase is an open-source project contributed by Shanghai AI Laboratory and researchers from different universities and companies. We would like to thank all the contributors for their support in adding new features to the project and the users for providing valuable feedback. We hope that this toolkit and benchmark can provide the community with flexible and efficient code tools for fine-tuning InternLM and developing their own models, thus continuously contributing to the open-source community. Special thanks to the two open-source projects, flash-attention and ColossalAI.\\n\\nLicense\\n\\nThe code is licensed under Apache-2.0, while model weights are fully open for academic research and also allow free commercial usage. To apply for a commercial license, please fill in the application form (English)/ç”³è¯·è¡¨ï¼ˆä¸­æ–‡ï¼‰. For other questions or collaborations, please contact internlm@pjlab.org.cn.\\n\\nCitation\\n\\n@misc{2023internlm,\\n    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},\\n    author={InternLM Team},\\n    howpublished = {\\\\url{https://github.com/InternLM/InternLM}},\\n    year={2023}\\n}', metadata={'source': './data/InternLM\\\\README.md'}),\n",
       " Document(page_content='0.2.0', metadata={'source': './data/InternLM\\\\version.txt'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åŠ è½½ç›®æ ‡æ–‡ä»¶\n",
    "docs = []\n",
    "for dir_path in tar_dir:\n",
    "    docs.extend(get_text(dir_path))\n",
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.character.RecursiveCharacterTextSplitter at 0x185e293e7d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å¯¹æ–‡æœ¬è¿›è¡Œåˆ†å—\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=150)\n",
    "text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='InternLM\\n\\nInternLM\\n\\nHOT\\n\\nã¯ã˜ã‚ã«\\n\\nInternLM ã¯ã€70 å„„ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã€å®Ÿç”¨çš„ãªã‚·ãƒŠãƒªã‚ªã«åˆã‚ã›ãŸãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã—ã¦ã„ã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã«ã¯ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™:\\n\\nä½•å…†ã‚‚ã®é«˜å“è³ªãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«æ´»ç”¨ã—ã€å¼·åŠ›ãªçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ç¢ºç«‹ã—ã¾ã™ã€‚\\n\\n8k ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é•·ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€ã‚ˆã‚Šé•·ã„å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨å¼·åŠ›ãªæ¨è«–æ©Ÿèƒ½ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚\\n\\nãƒ¦ãƒ¼ã‚¶ãŒç‹¬è‡ªã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æŸ”è»Ÿã«æ§‹ç¯‰ã§ãã‚‹ã‚ˆã†ã€æ±ç”¨æ€§ã®é«˜ã„ãƒ„ãƒ¼ãƒ«ã‚»ãƒƒãƒˆã‚’æä¾›ã—ã¾ã™ã€‚\\n\\nã•ã‚‰ã«ã€å¤§è¦æ¨¡ãªä¾å­˜é–¢ä¿‚ã‚’å¿…è¦ã¨ã›ãšã«ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹è»½é‡ãªå­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒæä¾›ã•ã‚Œã¾ã™ã€‚å˜ä¸€ã®ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§ã€æ•°åƒã® GPU ã‚’æŒã¤å¤§è¦æ¨¡ã‚¯ãƒ©ã‚¹ã‚¿ã§ã®äº‹å‰å­¦ç¿’ã¨ã€å˜ä¸€ã® GPU ã§ã®å¾®èª¿æ•´ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€é¡•è‘—ãªæ€§èƒ½æœ€é©åŒ–ã‚’é”æˆã—ã¾ã™ã€‚InternLM ã¯ã€1024GPU ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦ 90% è¿‘ã„ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŠ¹ç‡ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚\\n\\næ–°é—»', metadata={'source': './data/InternLM\\\\README-ja-JP.md'}),\n",
       " Document(page_content='æ–°é—»\\n\\nInternLM-7B-Chat v1.1 ã¯ã€ã‚³ãƒ¼ãƒ‰ ã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã¨é–¢æ•°å‘¼ã³å‡ºã—æ©Ÿèƒ½ã‚’å‚™ãˆã¦ãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¾ã—ãŸã€‚ Lagent ã§è©¦ã™ã“ã¨ãŒã§ãã¾ã™ã€‚\\n\\nInternLM-7B\\n\\nãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡\\n\\nã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®è©•ä¾¡ãƒ„ãƒ¼ãƒ« OpenCompass ã‚’ç”¨ã„ã¦ã€InternLM ã®ç·åˆçš„ãªè©•ä¾¡ã‚’è¡Œã£ãŸã€‚ã“ã®è©•ä¾¡ã§ã¯ã€åˆ†é‡åˆ¥èƒ½åŠ›ã€è¨€èªèƒ½åŠ›ã€çŸ¥è­˜èƒ½åŠ›ã€æ¨è«–èƒ½åŠ›ã€ç†è§£èƒ½åŠ›ã® 5 ã¤ã®æ¬¡å…ƒã‚’ã‚«ãƒãƒ¼ã—ã¾ã—ãŸã€‚ä»¥ä¸‹ã¯è©•ä¾¡çµæœã®ä¸€éƒ¨ã§ã‚ã‚Šã€ãã®ä»–ã®è©•ä¾¡çµæœã«ã¤ã„ã¦ã¯ OpenCompass leaderboard ã‚’ã”è¦§ãã ã•ã„ã€‚', metadata={'source': './data/InternLM\\\\README-ja-JP.md'}),\n",
       " Document(page_content='ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\\\\ãƒ¢ãƒ‡ãƒ« InternLM-Chat-7B InternLM-7B LLaMA-7B Baichuan-7B ChatGLM2-6B Alpaca-7B Vicuna-7B C-Eval(Val) 53.2 53.4 24.2 42.7 50.9 28.9 31.2 MMLU 50.8 51.0 35.2* 41.5 46.0 39.7 47.3 AGIEval 42.5 37.6 20.8 24.6 39.0 24.1 26.4 CommonSenseQA 75.2 59.5 65.0 58.8 60.0 68.7 66.7 BUSTM 74.3 50.6 48.5 51.3 55.0 48.8 62.5 CLUEWSC 78.6 59.1 50.3 52.8 59.8 50.3 52.2 MATH 6.4 7.1 2.8 3.0 6.6 2.2 2.8 GSM8K 34.5 31.2 10.1 9.7 29.2 6.0 15.3 HumanEval 14.0 10.4 14.0 9.2 9.2 9.2 11.0 RACE(High) 76.3 57.4 46.9*', metadata={'source': './data/InternLM\\\\README-ja-JP.md'}),\n",
       " Document(page_content='52.2 MATH 6.4 7.1 2.8 3.0 6.6 2.2 2.8 GSM8K 34.5 31.2 10.1 9.7 29.2 6.0 15.3 HumanEval 14.0 10.4 14.0 9.2 9.2 9.2 11.0 RACE(High) 76.3 57.4 46.9* 28.1 66.3 40.7 54.0', metadata={'source': './data/InternLM\\\\README-ja-JP.md'}),\n",
       " Document(page_content='è©•ä¾¡çµæœã¯ OpenCompass 20230706 (*å°ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯åŸè‘—è«–æ–‡ã‹ã‚‰ã®å¼•ç”¨ã‚’æ„å‘³ã™ã‚‹)ã‹ã‚‰å–å¾—ã—ãŸã‚‚ã®ã§ã€è©•ä¾¡è¨­å®šã¯ OpenCompass ãŒæä¾›ã™ã‚‹è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚\\n\\nè©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã¯ã€OpenCompass ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚¢ãƒƒãƒ—ã«ã‚ˆã‚Šæ•°å€¤çš„ãªå·®ç•°ãŒç”Ÿã˜ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã®ã§ã€OpenCompass ã®æœ€æ–°ã®è©•ä¾¡çµæœã‚’ã”å‚ç…§ãã ã•ã„ã€‚\\n\\nModel Zoo\\n\\nInternLM 7B ã¨ InternLM 7B ãƒãƒ£ãƒƒãƒˆã¯ã€InternLM ã‚’ä½¿ã£ã¦è¨“ç·´ã•ã‚Œã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¯ 2 ã¤ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚Transformers ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã ã‘ã§ãªãã€InternLM ã‚’ä½¿ã£ã¦ç›´æ¥é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ã•ã‚‰ã«äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„äººé–“ã®å¥½ã¿ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ã“ã¨ã‚‚ã§ãã¾ã™ã€‚', metadata={'source': './data/InternLM\\\\README-ja-JP.md'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs = text_splitter.split_documents(docs)\n",
    "split_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\mm\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½å¼€æºè¯å‘é‡æ¨¡å‹\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"./sentence-transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºå‘é‡æ•°æ®åº“\n",
    "# å®šä¹‰æŒä¹…åŒ–è·¯å¾„\n",
    "persist_directory = './vector_db/chroma'\n",
    "# åŠ è½½æ•°æ®åº“\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory  # å…è®¸æˆ‘ä»¬å°†persist_directoryç›®å½•ä¿å­˜åˆ°ç£ç›˜ä¸Š\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†åŠ è½½çš„å‘é‡æ•°æ®åº“æŒä¹…åŒ–åˆ°ç£ç›˜ä¸Š\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
