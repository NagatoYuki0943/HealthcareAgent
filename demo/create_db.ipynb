{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先导入所需第三方库\n",
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "# from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取文件路径函数\n",
    "def get_files(dir_path):\n",
    "    # args：dir_path，目标文件夹路径\n",
    "    file_list = []\n",
    "    for filepath, dirnames, filenames in os.walk(dir_path):\n",
    "        # os.walk 函数将递归遍历指定文件夹\n",
    "        for filename in filenames:\n",
    "            # 通过后缀名判断文件类型是否满足要求\n",
    "            if filename.endswith(\".md\"):\n",
    "                # 如果满足要求，将其绝对路径加入到结果列表\n",
    "                file_list.append(os.path.join(filepath, filename))\n",
    "            elif filename.endswith(\".txt\"):\n",
    "                file_list.append(os.path.join(filepath, filename))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载文件函数\n",
    "def get_text(dir_path):\n",
    "    # args：dir_path，目标文件夹路径\n",
    "    # 首先调用上文定义的函数得到目标文件路径列表\n",
    "    file_lst = get_files(dir_path)\n",
    "    # docs 存放加载之后的纯文本对象\n",
    "    docs = []\n",
    "    # 遍历所有目标文件\n",
    "    for one_file in tqdm(file_lst):\n",
    "        file_type = one_file.split('.')[-1]\n",
    "        if file_type == 'md':\n",
    "            loader = UnstructuredMarkdownLoader(one_file)\n",
    "        elif file_type == 'txt':\n",
    "            loader = UnstructuredFileLoader(one_file)\n",
    "        else:\n",
    "            # 如果是不符合条件的文件，直接跳过\n",
    "            continue\n",
    "        docs.extend(loader.load())\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标文件夹\n",
    "tar_dir = [\n",
    "    \"./data/InternLM\",\n",
    "    \"./data/InternLM-XComposer\",\n",
    "    \"./data/lagent\",\n",
    "    \"./data/lmdeploy\",\n",
    "    \"./data/opencompass\",\n",
    "    \"./data/xtuner\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 21.54it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 18.94it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 31.03it/s]\n",
      "100%|██████████| 72/72 [00:03<00:00, 21.90it/s]\n",
      "100%|██████████| 130/130 [00:07<00:00, 16.56it/s]\n",
      "100%|██████████| 38/38 [00:02<00:00, 15.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='', metadata={'source': './data/InternLM\\\\CHANGE_LOG.md'}),\n",
       " Document(page_content='InternLM\\n\\nInternLM\\n\\nHOT\\n\\nはじめに\\n\\nInternLM は、70 億のパラメータを持つベースモデルと、実用的なシナリオに合わせたチャットモデルをオープンソース化しています。このモデルには以下の特徴があります:\\n\\n何兆もの高品質なトークンをトレーニングに活用し、強力な知識ベースを確立します。\\n\\n8k のコンテキストウィンドウ長をサポートし、より長い入力シーケンスと強力な推論機能を可能にする。\\n\\nユーザが独自のワークフローを柔軟に構築できるよう、汎用性の高いツールセットを提供します。\\n\\nさらに、大規模な依存関係を必要とせずにモデルの事前学習をサポートする軽量な学習フレームワークが提供されます。単一のコードベースで、数千の GPU を持つ大規模クラスタでの事前学習と、単一の GPU での微調整をサポートし、顕著な性能最適化を達成します。InternLM は、1024GPU でのトレーニングにおいて 90% 近いアクセラレーション効率を達成しています。\\n\\n新闻\\n\\nInternLM-7B-Chat v1.1 は、コード インタプリタと関数呼び出し機能を備えてリリースされました。 Lagent で試すことができます。\\n\\nInternLM-7B\\n\\nパフォーマンス評価\\n\\nオープンソースの評価ツール OpenCompass を用いて、InternLM の総合的な評価を行った。この評価では、分野別能力、言語能力、知識能力、推論能力、理解能力の 5 つの次元をカバーしました。以下は評価結果の一部であり、その他の評価結果については OpenCompass leaderboard をご覧ください。\\n\\nデータセット\\\\モデル InternLM-Chat-7B InternLM-7B LLaMA-7B Baichuan-7B ChatGLM2-6B Alpaca-7B Vicuna-7B C-Eval(Val) 53.2 53.4 24.2 42.7 50.9 28.9 31.2 MMLU 50.8 51.0 35.2* 41.5 46.0 39.7 47.3 AGIEval 42.5 37.6 20.8 24.6 39.0 24.1 26.4 CommonSenseQA 75.2 59.5 65.0 58.8 60.0 68.7 66.7 BUSTM 74.3 50.6 48.5 51.3 55.0 48.8 62.5 CLUEWSC 78.6 59.1 50.3 52.8 59.8 50.3 52.2 MATH 6.4 7.1 2.8 3.0 6.6 2.2 2.8 GSM8K 34.5 31.2 10.1 9.7 29.2 6.0 15.3 HumanEval 14.0 10.4 14.0 9.2 9.2 9.2 11.0 RACE(High) 76.3 57.4 46.9* 28.1 66.3 40.7 54.0\\n\\n評価結果は OpenCompass 20230706 (*印のあるデータは原著論文からの引用を意味する)から取得したもので、評価設定は OpenCompass が提供する設定ファイルに記載されています。\\n\\n評価データは、OpenCompass のバージョンアップにより数値的な差異が生じる可能性がありますので、OpenCompass の最新の評価結果をご参照ください。\\n\\nModel Zoo\\n\\nInternLM 7B と InternLM 7B チャットは、InternLM を使って訓練され、オープンソース化されています。モデルの重みは 2 つのフォーマットで提供されています。Transformers フォーマットを使ってモデルをロードするだけでなく、InternLM を使って直接重みをロードして、さらに事前トレーニングや人間の好みアライメントトレーニングを行うこともできます。\\n\\nモデル InternLM フォーマット Weight ダウンロードリンク Transformers フォーマット Weight ダウンロードリンク InternLM 7B 🤗internlm/intern-7b InternLM Chat 7B 🤗internlm/intern-chat-7b InternLM Chat 7B 8k 🤗internlm/intern-chat-7b-8k\\n\\n制限事項: 学習過程におけるモデルの安全性を確保し、倫理的・法的要件に準拠したテキストを生成するようモデルに促す努力を行ってきたが、モデルのサイズと確率的生成パラダイムのため、モデルは依然として予期せぬ出力を生成する可能性がある。例えば、生成された回答には偏見や差別、その他の有害な内容が含まれている可能性があります。そのような内容を伝播しないでください。有害な情報の伝播によって生じるいかなる結果に対しても、私たちは責任を負いません。\\n\\nTransformers からのインポート\\n\\nTransformers を使用して InternLM 7B チャットモデルをロードするには、以下のコードを使用します:\\n\\n```python\\n\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\ntokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm-chat-7b-v1_1\", trust_remote_code=True)\\nmodel = AutoModelForCausalLM.from_pretrained(\"internlm/internlm-chat-7b-v1_1\", trust_remote_code=True).cuda()\\nmodel = model.eval()\\nresponse, history = model.chat(tokenizer, \"こんにちは\", history=[])\\nprint(response)\\nこんにちは！どのようにお手伝いできますか？\\nresponse, history = model.chat(tokenizer, \"時間管理について3つの提案をお願いします\", history=history)\\nprint(response)\\nもちろんです！以下に簡潔な形で時間管理に関する3つの提案を示します。\\n\\nTo-Doリストを作成し、優先順位を付ける: タスクを明確にリストアップし、それぞれの優先度を判断しましょう。重要で緊急なタスクから順に取り組むことで、効率的に作業を進めることができます。\\n\\n時間のブロック化を実践する: 作業を特定の時間枠に集中させるため、時間をブロック化しましょう。例えば、朝の2時間をメール対応に割り当て、午後の3時間をプロジェクトに集中するなど、タスクごとに時間を確保することが効果的です。\\n\\nディストラクションを排除する: 集中力を保つために、ディストラクションを最小限に抑えましょう。通知をオフにし、SNSやメールに気を取られないようにすることで、作業効率を向上させることができます。\\n\\nこれらの提案を実践することで、時間管理のスキルを向上させ、効果的に日々のタスクをこなしていくことができます。\\n```\\n\\n対話\\n\\n以下のコードを実行することで、フロントエンドインターフェースを通して InternLM Chat 7B モデルと対話することができます:\\n\\nbash\\npip install streamlit==1.24.0\\npip install transformers==4.30.2\\nstreamlit run web_demo.py\\n\\nその効果は以下の通り\\n\\nデプロイ\\n\\nLMDeploy を使って、InternLM をワンクリックでデプロイする。\\n\\nまず、LMDeploy をインストールする:\\n\\npython3 -m pip install lmdeploy\\n\\nクイックデプロイには以下のコマンドを使用します:\\n\\npython3 -m lmdeploy.serve.turbomind.deploy InternLM-7B /path/to/internlm-7b/model hf\\n\\nモデルをエクスポートした後、以下のコマンドを使ってサーバーを起動し、デプロイされたモデルと会話することができます:\\n\\npython3 -m lmdeploy.serve.client {server_ip_addresss}:33337\\n\\nLMDeploy は、InternLM をデプロイするための完全なワークフローを提供します。InternLM のデプロイの詳細については、デプロイチュートリアルを参照してください。\\n\\nファインチューニングとトレーニング\\n\\nプリトレーニングとファインチューニングのチュートリアル\\n\\nInternLMのインストール、データ処理、プレトレーニング、ファインチューニングを始めるには、使用法チュートリアルを参照してください。\\n\\nTransformers フォーマットへの変換\\n\\nInternLM によって学習されたモデルは、コミュニティの様々なオープンソースプロジェクトとシームレスにドッキングするのに便利な Hugging Face Transformers 形式に簡単に変換することができます。tools/convert2hf.py の助けを借りて、トレーニング中に保存された weights は 1 つのコマンドで transformers 形式に変換することができます\\n\\nbash\\npython convert2hf.py --src_folder origin_ckpt/ --tgt_folder hf_ckpt/ --tokenizer tokenizes/tokenizer.model\\n\\n変換後、以下のコードで transformers として読み込むことができます\\n\\n```python\\n\\nfrom transformers import AutoTokenizer, AutoModel\\nmodel = AutoModel.from_pretrained(\"hf_ckpt/\", trust_remote_code=True).cuda()\\n```\\n\\nトレーニングシステム\\n\\nシステムアーキテクチャ\\n\\n詳細については、システムアーキテクチャドキュメント を参照してください。\\n\\nトレーニングパフォーマンス\\n\\nInternLM は、Flash-Attention、Apex その他の高性能モデルオペレータを深く統合し、トレーニング効率を向上させます。Hybrid Zero 技術を構築することで、計算と通信の効率的なオーバーラップを実現し、トレーニング中のノード間の通信トラフィックを大幅に削減します。InternLM は 7B モデルを 8GPU から 1024GPU まで拡張することをサポートし、1000GPU スケールで最大 90% のアクセラレーション効率、180TFLOPS 以上のトレーニングスループット、GPU あたり平均 3600 トークン/秒以上を実現します。次の表は、異なる構成における InternLM のスケーラビリティテストデータです:\\n\\nGPU Number 8 16 32 64 128 256 512 1024 TGS 4078 3939 3919 3944 3928 3920 3835 3625 TFLOPS 193 191 188 188 187 185 186 184\\n\\nTGSは、GPUあたり1秒間に処理されるトークンの平均数を表します。パフォーマンステストデータの詳細については、トレーニングパフォーマンスドキュメントを参照してください。\\n\\nコントリビュート\\n\\n我々は、InternLM を改善し、向上させるために尽力してくれたすべての貢献者に感謝している。コミュニティ・ユーザーのプロジェクトへの参加が強く推奨されます。プロジェクトへの貢献方法については、貢献ガイドラインを参照してください。\\n\\n謝辞\\n\\nInternLM コードベースは、上海 AI 研究所と様々な大学や企業の研究者によって貢献されたオープンソースプロジェクトです。プロジェクトに新機能を追加してくれたすべての貢献者と、貴重なフィードバックを提供してくれたユーザーに感謝したい。私たちは、このツールキットとベンチマークが、InternLM をファインチューニングし、独自のモデルを開発するための柔軟で効率的なコードツールをコミュニティに提供し、オープンソースコミュニティに継続的に貢献できることを願っています。2 つのオープンソースプロジェクト、flash-attention と ColossalAI に感謝します。\\n\\nライセンス\\n\\nコードは Apache-2.0 でライセンスされており、モデルの重さは学術研究のために完全にオープンで、無料 の商用利用も許可されています。商用ライセンスの申請は、申請フォーム（英語）/申请表（中文）にご記入ください。その他のご質問やコラボレーションについては、internlm@pjlab.org.cn までご連絡ください。\\n\\n引用\\n\\n@misc{2023internlm,\\n    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},\\n    author={InternLM Team},\\n    howpublished = {\\\\url{https://github.com/InternLM/InternLM}},\\n    year={2023}\\n}', metadata={'source': './data/InternLM\\\\README-ja-JP.md'}),\n",
       " Document(page_content='InternLM\\n\\n书生·浦语 官网\\n\\nHOT\\n\\n👋 加入我们的 Discord 和 微信社区\\n\\n简介\\n\\nInternLM 是一个开源的轻量级训练框架，旨在支持大模型训练而无需大量的依赖。通过单一的代码库，它支持在拥有数千个 GPU 的大型集群上进行预训练，并在单个 GPU 上进行微调，同时实现了卓越的性能优化。在1024个 GPU 上训练时，InternLM 可以实现近90%的加速效率。\\n\\n基于InternLM训练框架，我们已经发布了两个开源的预训练模型：InternLM-7B 和 InternLM-20B。\\n\\n更新\\n\\n[20230920] InternLM-20B 已发布，包括基础版和对话版。\\n\\n[20230822] InternLM-7B-Chat v1.1 已发布，增加了代码解释器和函数调用能力。您可以使用\\n\\nLagent 进行尝试。\\n\\nModel Zoo\\n\\n我们的模型在三个平台上发布：Transformers、ModelScope 和 OpenXLab。\\n\\nModel Transformers ModelScope OpenXLab 发布日期 InternLM Chat 20B 🤗internlm/internlm-chat-20b Shanghai_AI_Laboratory/internlm-chat-20b 2023-09-20 InternLM 20B 🤗internlm/internlm-20b Shanghai_AI_Laboratory/internlm-20b 2023-09-20 InternLM Chat 7B v1.1 🤗internlm/internlm-chat-7b-v1.1 Shanghai_AI_Laboratory/internlm-chat-7b-v1_1 2023-08-22 InternLM 7B 🤗internlm/internlm-7b Shanghai_AI_Laboratory/internlm-7b 2023-07-06 InternLM Chat 7B 🤗internlm/internlm-chat-7b Shanghai_AI_Laboratory/internlm-chat-7b 2023-07-06 InternLM Chat 7B 8k 🤗internlm/internlm-chat-7b-8k Shanghai_AI_Laboratory/internlm-chat-7b-8k 2023-07-06\\n\\n使用案例\\n\\n通过 Transformers 加载\\n\\n通过以下的代码从 Transformers 加载 InternLM 模型 （可修改模型名称替换不同的模型）\\n\\n```python\\n\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\ntokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm-chat-7b\", trust_remote_code=True)\\nmodel = AutoModelForCausalLM.from_pretrained(\"internlm/internlm-chat-7b\", trust_remote_code=True).cuda()\\nmodel = model.eval()\\nresponse, history = model.chat(tokenizer, \"你好\", history=[])\\nprint(response)\\n你好！有什么我可以帮助你的吗？\\nresponse, history = model.chat(tokenizer, \"请提供三个管理时间的建议。\", history=history)\\nprint(response)\\n当然可以！以下是三个管理时间的建议：\\n1. 制定计划：制定一个详细的计划，包括每天要完成的任务和活动。这将有助于您更好地组织时间，并确保您能够按时完成任务。\\n2. 优先级：将任务按照优先级排序，先完成最重要的任务。这将确保您能够在最短的时间内完成最重要的任务，从而节省时间。\\n3. 集中注意力：避免分心，集中注意力完成任务。关闭社交媒体和电子邮件通知，专注于任务，这将帮助您更快地完成任务，并减少错误的可能性。\\n```\\n\\n通过 ModelScope 加载\\n\\n通过以下的代码从 ModelScope 加载 InternLM 模型 （可修改模型名称替换不同的模型）\\n\\npython\\nfrom modelscope import snapshot_download, AutoTokenizer, AutoModelForCausalLM\\nimport torch\\nmodel_dir = snapshot_download(\\'Shanghai_AI_Laboratory/internlm-chat-7b-v1_1\\', revision=\\'v1.0.0\\')\\ntokenizer = AutoTokenizer.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True,torch_dtype=torch.float16)\\nmodel = AutoModelForCausalLM.from_pretrained(model_dir,device_map=\"auto\",  trust_remote_code=True,torch_dtype=torch.float16)\\nmodel = model.eval()\\nresponse, history = model.chat(tokenizer, \"hello\", history=[])\\nprint(response)\\nresponse, history = model.chat(tokenizer, \"please provide three suggestions about time management\", history=history)\\nprint(response)\\n\\n通过前端网页对话\\n\\n可以通过以下代码启动一个前端的界面来与 InternLM Chat 7B 模型进行交互\\n\\nbash\\npip install streamlit==1.24.0\\npip install transformers==4.30.2\\nstreamlit run web_demo.py\\n\\n效果如下\\n\\n基于InternLM高性能部署\\n\\n我们使用 LMDeploy 完成 InternLM 的一键部署。\\n\\n首先安装 LMDeploy:\\n\\npython3 -m pip install lmdeploy\\n\\n快速的部署命令如下：\\n\\npython3 -m lmdeploy.serve.turbomind.deploy InternLM-7B /path/to/internlm-7b/model hf\\n\\n在导出模型后，你可以直接通过如下命令启动服务一个服务并和部署后的模型对话\\n\\npython3 -m lmdeploy.serve.client {server_ip_addresss}:33337\\n\\nLMDeploy 支持了 InternLM 部署的完整流程，请参考 部署教程 了解 InternLM 的更多部署细节。\\n\\n微调&训练\\n\\n预训练与微调使用教程\\n\\n请参考使用教程开始InternLM的安装、数据处理、预训练与微调。\\n\\n转换为 Transformers 格式使用\\n\\n通过 InternLM 进行训练的模型可以很轻松地转换为 HuggingFace Transformers 格式，方便与社区各种开源项目无缝对接。借助 tools/transformers/convert2hf.py 可以将训练保存的权重一键转换为 transformers 格式\\n\\nbash\\npython tools/transformers/convert2hf.py --src_folder origin_ckpt/ --tgt_folder hf_ckpt/ --tokenizer ./tools/V7_sft.model\\n\\n转换之后可以通过以下的代码加载为 transformers\\n\\n```python\\n\\nfrom transformers import AutoTokenizer, AutoModel\\nmodel = AutoModel.from_pretrained(\"hf_ckpt/\", trust_remote_code=True).cuda()\\n```\\n\\n训练系统\\n\\n系统结构\\n\\n请参考系统结构文档进一步了解。\\n\\n训练性能\\n\\nInternLM 深度整合了 Flash-Attention, Apex 等高性能模型算子，提高了训练效率。通过构建 Hybrid Zero 技术，实现计算和通信的高效重叠，大幅降低了训练过程中的跨节点通信流量。InternLM 支持 7B 模型从 8 卡扩展到 1024 卡，千卡规模下加速效率可高达 90%，训练吞吐超过 180TFLOPS，平均单卡每秒处理的 token 数量超过3600。下表为 InternLM 在不同配置下的扩展性测试数据：\\n\\nGPU Number 8 16 32 64 128 256 512 1024 TGS 4078 3939 3919 3944 3928 3920 3835 3625 TFLOPS 193 191 188 188 187 185 186 184\\n\\nTGS 代表平均每GPU每秒可以处理的 Token 数量。更多的性能测试数据可参考训练性能文档进一步了解。\\n\\n贡献\\n\\n我们感谢所有的贡献者为改进和提升 InternLM 所作出的努力。非常欢迎社区用户能参与进项目中来。请参考贡献指南来了解参与项目贡献的相关指引。\\n\\n致谢\\n\\nInternLM 代码库是一款由上海人工智能实验室和来自不同高校、企业的研发人员共同参与贡献的开源项目。我们感谢所有为项目提供新功能支持的贡献者，以及提供宝贵反馈的用户。 我们希望这个工具箱和基准测试可以为社区提供灵活高效的代码工具，供用户微调 InternLM 并开发自己的新模型，从而不断为开源社区提供贡献。特别鸣谢flash-attention 与 ColossalAI 两项开源项目。\\n\\n开源许可证\\n\\n本仓库的代码依照 Apache-2.0 协议开源。模型权重对学术研究完全开放，也可申请免费的商业使用授权（申请表）。其他问题与合作请联系 internlm@pjlab.org.cn。\\n\\n引用\\n\\n@misc{2023internlm,\\n    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},\\n    author={InternLM Team},\\n    howpublished = {\\\\url{https://github.com/InternLM/InternLM}},\\n    year={2023}\\n}', metadata={'source': './data/InternLM\\\\README-zh-Hans.md'}),\n",
       " Document(page_content='InternLM\\n\\nInternLM\\n\\nHOT\\n\\n👋 join us on Discord and WeChat\\n\\nIntroduction\\n\\nInternLM is an open-sourced lightweight training framework aims to  support model pre-training without the need for extensive dependencies. With a single codebase, it supports pre-training on large-scale clusters with thousands of GPUs, and fine-tuning on a single GPU while achieving remarkable performance optimizations. InternLM achieves nearly 90% acceleration efficiency during training on 1024 GPUs.\\n\\nBased on the InternLM training framework, we have released two open-sourced pretrained model InternLM-7B and InternLM-20B.\\n\\nNews\\n\\n[20230920] InternLM-20B is released with base and chat versions.\\n\\n[20230822] InternLM-7B-Chat v1.1 is released with code interpreter and function calling capability. You can try it with\\n\\nLagent.\\n\\nModel Zoo\\n\\nOur models are released in three platforms: Transformers, ModelScope and OpenXLab.\\n\\nThere are two kinds of model weights: \\n  1. huggingface type(marked as HF)\\n  2. original model weight(marked as Original), providing in OpenXLab, which can be loaded by InternLM and finetuned directly.\\n\\nModel Transformers(HF) ModelScope(HF) OpenXLab(HF) OpenXLab(Original) Release Date InternLM Chat 20B 🤗internlm/internlm-chat-20b Shanghai_AI_Laboratory/internlm-chat-20b 2023-09-20 InternLM 20B 🤗internlm/internlm-20b Shanghai_AI_Laboratory/internlm-20b 2023-09-20 InternLM Chat 7B v1.1 🤗internlm/internlm-chat-7b-v1.1 Shanghai_AI_Laboratory/internlm-chat-7b-v1_1 2023-08-22 InternLM 7B 🤗internlm/internlm-7b Shanghai_AI_Laboratory/internlm-7b 2023-07-06 InternLM Chat 7B 🤗internlm/internlm-chat-7b Shanghai_AI_Laboratory/internlm-chat-7b 2023-07-06 InternLM Chat 7B 8k 🤗internlm/internlm-chat-7b-8k Shanghai_AI_Laboratory/internlm-chat-7b-8k 2023-07-06\\n\\nIntroduction\\n\\nInternLM-20B was pre-trained on over 2.3T Tokens containing high-quality English, Chinese, and code data. Additionally, the Chat version has undergone SFT and RLHF training, enabling it to better and more securely meet users\\' needs.\\n\\nIn terms of model structure, InternLM-20B opted for a deeper architecture, with a depth set at 60 layers. This surpasses the conventional 7B and 13B models that utilize 32 or 40 layers. When parameters are limited, increasing the number of layers can enhance the model\\'s overall capability. Furthermore, compared to InternLM-7B, the pre-training data used for InternLM-20B underwent higher quality cleansing and was supplemented with data rich in knowledge and designed for reinforcing understanding and reasoning capabilities. As a result, it exhibits significant improvements in understanding, reasoning, mathematical, and programming abilities—all of which test the technical proficiency of language models. Overall, InternLM-20B features the following characteristics:\\n- Outstanding overall performance\\n- Strong utility invocation capability\\n- Supports a 16k context length (Through inference extrapolation)\\n- Better value alignment.\\n\\nPerformance Evaluation\\n\\nOn the 5 capability dimensions proposed by OpenCompass, InternLM-20B has achieved excellent results (the bolded scores represent the best performances within the 13B-33B parameter range).\\n\\nCapability Llama-13B Llama2-13B Baichuan2-13B InternLM-20B Llama-33B Llama-65B Llama2-70B Language 42.5 47 47.5 55 44.6 47.1 51.6 Knowledge 58.2 58.3 48.9 60.1 64 66 67.7 Understanding 45.5 50.9 58.1 67.3 50.6 54.2 60.8 Reasoning 42.7 43.6 44.2 54.9 46.4 49.8 55 Examination 37.3 45.2 51.8 62.5 47.4 49.7 57.3 Overall 43.8 47.3 49.4 59.2 48.9 51.9 57.4\\n\\nThe table below compares the performance of mainstream open-source models on some influential and typical datasets.\\n\\nBenchmarks Llama-13B Llama2-13B Baichuan2-13B InternLM-20B Llama-33B Llama-65B Llama2-70B Examination MMLU 47.73 54.99 59.55 62.05 58.73 63.71 69.75 C-Eval (val) 31.83 41.4 59.01 58.8 37.47 40.36 50.13 AGI-Eval 22.03 30.93 37.37 44.58 33.53 33.92 40.02 Knowledge BoolQ 78.75 82.42 67 87.46 84.43 86.61 87.74 TriviaQA 52.47 59.36 46.61 57.26 66.24 69.79 70.71 NaturalQuestions 20.17 24.85 16.32 25.15 30.89 33.41 34.16 Understanding CMRC 9.26 31.59 29.85 68.78 14.17 34.73 43.74 CSL 55 58.75 63.12 65.62 57.5 59.38 60 RACE (middle) 53.41 63.02 68.94 86.35 64.55 72.35 81.55 RACE (high) 47.63 58.86 67.18 83.28 62.61 68.01 79.93 XSum 20.37 23.37 25.23 35.54 20.55 19.91 25.38 Reasoning WinoGrande 64.64 64.01 67.32 69.38 66.85 69.38 69.77 BBH 37.93 45.62 48.98 52.51 49.98 58.38 64.91 GSM8K 20.32 29.57 52.62 52.62 42.3 54.44 63.31 PIQA 79.71 79.76 78.07 80.25 81.34 82.15 82.54 Programming HumanEval 14.02 18.9 17.07 25.61 17.68 18.9 26.22 MBPP 20.6 26.8 30.8 35.6 28.4 33.6 39.6\\n\\nOverall, InternLM-20B comprehensively outperforms open-source models in the 13B parameter range in terms of overall capabilities, and on inference evaluation sets, it approaches or even surpasses the performance of Llama-65B.\\n\\nThe evaluation results were obtained from OpenCompass 20230920.\\n\\nThe evaluation data may have numerical differences due to the version iteration of OpenCompass, so please refer to the latest evaluation results of OpenCompass.\\n\\nLimitations: Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information.\\n\\nUsage Examples\\n\\nImport from Transformers\\n\\nTo load the InternLM 7B Chat model using Transformers, use the following code:\\n\\n```python\\n\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\ntokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm-chat-7b-v1_1\", trust_remote_code=True)\\nmodel = AutoModelForCausalLM.from_pretrained(\"internlm/internlm-chat-7b-v1_1\", trust_remote_code=True).cuda()\\nmodel = model.eval()\\nresponse, history = model.chat(tokenizer, \"hello\", history=[])\\nprint(response)\\nHello! How can I help you today?\\nresponse, history = model.chat(tokenizer, \"please provide three suggestions about time management\", history=history)\\nprint(response)\\nSure, here are three tips for effective time management:\\n\\nPrioritize tasks based on importance and urgency: Make a list of all your tasks and categorize them into \"important and urgent,\" \"important but not urgent,\" and \"not important but urgent.\" Focus on completing the tasks in the first category before moving on to the others.\\n\\nUse a calendar or planner: Write down deadlines and appointments in a calendar or planner so you don\\'t forget them. This will also help you schedule your time more effectively and avoid overbooking yourself.\\n\\nMinimize distractions: Try to eliminate any potential distractions when working on important tasks. Turn off notifications on your phone, close unnecessary tabs on your computer, and find a quiet place to work if possible.\\n\\nRemember, good time management skills take practice and patience. Start with small steps and gradually incorporate these habits into your daily routine.\\n```\\n\\nImport from ModelScope\\n\\nTo load the InternLM model using ModelScope, use the following code:\\n\\npython\\nfrom modelscope import snapshot_download, AutoTokenizer, AutoModelForCausalLM\\nimport torch\\nmodel_dir = snapshot_download(\\'Shanghai_AI_Laboratory/internlm-chat-7b-v1_1\\', revision=\\'v1.0.0\\')\\ntokenizer = AutoTokenizer.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True,torch_dtype=torch.float16)\\nmodel = AutoModelForCausalLM.from_pretrained(model_dir,device_map=\"auto\",  trust_remote_code=True,torch_dtype=torch.float16)\\nmodel = model.eval()\\nresponse, history = model.chat(tokenizer, \"hello\", history=[])\\nprint(response)\\nresponse, history = model.chat(tokenizer, \"please provide three suggestions about time management\", history=history)\\nprint(response)\\n\\nDialogue\\n\\nYou can interact with the InternLM Chat 7B model through a frontend interface by running the following code:\\n\\nbash\\npip install streamlit==1.24.0\\npip install transformers==4.30.2\\nstreamlit run web_demo.py\\n\\nThe effect is as follows\\n\\nDeployment\\n\\nWe use LMDeploy to complete the one-click deployment of InternLM.\\n\\nFirst, install LMDeploy:\\n\\npython3 -m pip install lmdeploy\\n\\nUse the following command for quick deployment:\\n\\npython3 -m lmdeploy.serve.turbomind.deploy InternLM-7B /path/to/internlm-7b/model hf\\n\\nAfter exporting the model, you can start a server and have a conversation with the deployed model using the following command:\\n\\npython3 -m lmdeploy.serve.client {server_ip_addresss}:33337\\n\\nLMDeploy provides a complete workflow for deploying InternLM. Please refer to the deployment tutorial for more details on deploying InternLM.\\n\\nFine-tuning & Training\\n\\nPre-training and Fine-tuning Tutorial\\n\\nPlease refer to Usage Tutorial to start InternLM installation, data processing, pre-training and fine-tuning.\\n\\nConvert to Transformers Format\\n\\nThe model trained by InternLM can be easily converted to HuggingFace Transformers format, which is convenient for seamless docking with various open source projects in the community. With the help of tools/transformers/convert2hf.py, the weights saved during training can be converted into transformers format with one command\\n\\nbash\\npython tools/transformers/convert2hf.py --src_folder origin_ckpt/ --tgt_folder hf_ckpt/ --tokenizer ./tools/V7_sft.model\\n\\nAfter conversion, it can be loaded as transformers by the following code\\n\\n```python\\n\\nfrom transformers import AutoTokenizer, AutoModel\\nmodel = AutoModel.from_pretrained(\"hf_ckpt/\", trust_remote_code=True).cuda()\\n```\\n\\nTraining System\\n\\nSystem Architecture\\n\\nPlease refer to the System Architecture document for further details.\\n\\nTraining Performance\\n\\nInternLM deeply integrates Flash-Attention, Apex and other high-performance model operators to improve training efficiency. By building the Hybrid Zero technique, it achieves efficient overlap of computation and communication, significantly reducing cross-node communication traffic during training. InternLM supports expanding the 7B model from 8 GPUs to 1024 GPUs, with an acceleration efficiency of up to 90% at the thousand-GPU scale, a training throughput of over 180 TFLOPS, and an average of over 3600 tokens per GPU per second. The following table shows InternLM\\'s scalability test data at different configurations:\\n\\nGPU Number 8 16 32 64 128 256 512 1024 TGS 4078 3939 3919 3944 3928 3920 3835 3625 TFLOPS 193 191 188 188 187 185 186 184\\n\\nTGS represents the average number of tokens processed per GPU per second. For more performance test data, please refer to the Training Performance document for further details.\\n\\nContribution\\n\\nWe appreciate all the contributors for their efforts to improve and enhance InternLM. Community users are highly encouraged to participate in the project. Please refer to the contribution guidelines for instructions on how to contribute to the project.\\n\\nAcknowledgements\\n\\nInternLM codebase is an open-source project contributed by Shanghai AI Laboratory and researchers from different universities and companies. We would like to thank all the contributors for their support in adding new features to the project and the users for providing valuable feedback. We hope that this toolkit and benchmark can provide the community with flexible and efficient code tools for fine-tuning InternLM and developing their own models, thus continuously contributing to the open-source community. Special thanks to the two open-source projects, flash-attention and ColossalAI.\\n\\nLicense\\n\\nThe code is licensed under Apache-2.0, while model weights are fully open for academic research and also allow free commercial usage. To apply for a commercial license, please fill in the application form (English)/申请表（中文）. For other questions or collaborations, please contact internlm@pjlab.org.cn.\\n\\nCitation\\n\\n@misc{2023internlm,\\n    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},\\n    author={InternLM Team},\\n    howpublished = {\\\\url{https://github.com/InternLM/InternLM}},\\n    year={2023}\\n}', metadata={'source': './data/InternLM\\\\README.md'}),\n",
       " Document(page_content='0.2.0', metadata={'source': './data/InternLM\\\\version.txt'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载目标文件\n",
    "docs = []\n",
    "for dir_path in tar_dir:\n",
    "    docs.extend(get_text(dir_path))\n",
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.character.RecursiveCharacterTextSplitter at 0x185e293e7d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对文本进行分块\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=150)\n",
    "text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='InternLM\\n\\nInternLM\\n\\nHOT\\n\\nはじめに\\n\\nInternLM は、70 億のパラメータを持つベースモデルと、実用的なシナリオに合わせたチャットモデルをオープンソース化しています。このモデルには以下の特徴があります:\\n\\n何兆もの高品質なトークンをトレーニングに活用し、強力な知識ベースを確立します。\\n\\n8k のコンテキストウィンドウ長をサポートし、より長い入力シーケンスと強力な推論機能を可能にする。\\n\\nユーザが独自のワークフローを柔軟に構築できるよう、汎用性の高いツールセットを提供します。\\n\\nさらに、大規模な依存関係を必要とせずにモデルの事前学習をサポートする軽量な学習フレームワークが提供されます。単一のコードベースで、数千の GPU を持つ大規模クラスタでの事前学習と、単一の GPU での微調整をサポートし、顕著な性能最適化を達成します。InternLM は、1024GPU でのトレーニングにおいて 90% 近いアクセラレーション効率を達成しています。\\n\\n新闻', metadata={'source': './data/InternLM\\\\README-ja-JP.md'}),\n",
       " Document(page_content='新闻\\n\\nInternLM-7B-Chat v1.1 は、コード インタプリタと関数呼び出し機能を備えてリリースされました。 Lagent で試すことができます。\\n\\nInternLM-7B\\n\\nパフォーマンス評価\\n\\nオープンソースの評価ツール OpenCompass を用いて、InternLM の総合的な評価を行った。この評価では、分野別能力、言語能力、知識能力、推論能力、理解能力の 5 つの次元をカバーしました。以下は評価結果の一部であり、その他の評価結果については OpenCompass leaderboard をご覧ください。', metadata={'source': './data/InternLM\\\\README-ja-JP.md'}),\n",
       " Document(page_content='データセット\\\\モデル InternLM-Chat-7B InternLM-7B LLaMA-7B Baichuan-7B ChatGLM2-6B Alpaca-7B Vicuna-7B C-Eval(Val) 53.2 53.4 24.2 42.7 50.9 28.9 31.2 MMLU 50.8 51.0 35.2* 41.5 46.0 39.7 47.3 AGIEval 42.5 37.6 20.8 24.6 39.0 24.1 26.4 CommonSenseQA 75.2 59.5 65.0 58.8 60.0 68.7 66.7 BUSTM 74.3 50.6 48.5 51.3 55.0 48.8 62.5 CLUEWSC 78.6 59.1 50.3 52.8 59.8 50.3 52.2 MATH 6.4 7.1 2.8 3.0 6.6 2.2 2.8 GSM8K 34.5 31.2 10.1 9.7 29.2 6.0 15.3 HumanEval 14.0 10.4 14.0 9.2 9.2 9.2 11.0 RACE(High) 76.3 57.4 46.9*', metadata={'source': './data/InternLM\\\\README-ja-JP.md'}),\n",
       " Document(page_content='52.2 MATH 6.4 7.1 2.8 3.0 6.6 2.2 2.8 GSM8K 34.5 31.2 10.1 9.7 29.2 6.0 15.3 HumanEval 14.0 10.4 14.0 9.2 9.2 9.2 11.0 RACE(High) 76.3 57.4 46.9* 28.1 66.3 40.7 54.0', metadata={'source': './data/InternLM\\\\README-ja-JP.md'}),\n",
       " Document(page_content='評価結果は OpenCompass 20230706 (*印のあるデータは原著論文からの引用を意味する)から取得したもので、評価設定は OpenCompass が提供する設定ファイルに記載されています。\\n\\n評価データは、OpenCompass のバージョンアップにより数値的な差異が生じる可能性がありますので、OpenCompass の最新の評価結果をご参照ください。\\n\\nModel Zoo\\n\\nInternLM 7B と InternLM 7B チャットは、InternLM を使って訓練され、オープンソース化されています。モデルの重みは 2 つのフォーマットで提供されています。Transformers フォーマットを使ってモデルをロードするだけでなく、InternLM を使って直接重みをロードして、さらに事前トレーニングや人間の好みアライメントトレーニングを行うこともできます。', metadata={'source': './data/InternLM\\\\README-ja-JP.md'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs = text_splitter.split_documents(docs)\n",
    "split_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\mm\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# 加载开源词向量模型\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"./sentence-transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建向量数据库\n",
    "# 定义持久化路径\n",
    "persist_directory = './vector_db/chroma'\n",
    "# 加载数据库\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将加载的向量数据库持久化到磁盘上\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
