# fix chroma sqlite3 error
# refer: https://github.com/chroma-core/chroma/issues/1985#issuecomment-2055963683
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import os
from infer_engine import InferEngine, LmdeployConfig
from create_db import create_vectordb, load_vectordb, similarity_search
import gradio as gr
from typing import Generator, Any
from utils import download_dataset
from huggingface_hub import hf_hub_download, snapshot_download

print("*" * 100)
os.system("pip list")
print("*" * 100)


print("gradio version: ", gr.__version__)


DATA_PATH = "./data"
EMBEDDING_DIR = "./models/sentence-transformer"
PERSIST_DIRECTORY = "./vector_db/chroma"

# ä¸‹è½½embeddingæ¨¡å‹,ä¸ä¼šé‡å¤ä¸‹è½½
snapshot_download(
    repo_id = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    local_dir = EMBEDDING_DIR,
    resume_download = True,
    max_workers = 8,
    endpoint = "https://hf-mirror.com",
)

# ä¸‹è½½æ•°æ®é›†,ä¸ä¼šé‡å¤ä¸‹è½½
download_dataset(target_path = DATA_PATH)

# ä¸å­˜åœ¨æ‰åˆ›å»º,åŸå› æ˜¯åº”ç”¨è‡ªå¯åŠ¨å¯èƒ½ä¼šé‡æ–°å†™å…¥æ•°æ®åº“
if not os.path.exists(PERSIST_DIRECTORY):
    # åˆ›å»ºå‘é‡æ•°æ®åº“
    create_vectordb(
        tar_dirs = DATA_PATH,
        embedding_dir = EMBEDDING_DIR,
        persist_directory = PERSIST_DIRECTORY
    )

# è½½å…¥å‘é‡æ•°æ®åº“
vectordb = load_vectordb(
    embedding_dir = EMBEDDING_DIR,
    persist_directory = PERSIST_DIRECTORY
)

# clone æ¨¡å‹
MODEL_PATH = './models/internlm2-chat-7b'
os.system(f'git clone https://code.openxlab.org.cn/OpenLMLab/internlm2-chat-7b {MODEL_PATH}')
os.system(f'cd {MODEL_PATH} && git lfs pull')

SYSTEM_PROMPT = "ä½ ç°åœ¨æ˜¯ä¸€ååŒ»ç”Ÿï¼Œå…·å¤‡ä¸°å¯Œçš„åŒ»å­¦çŸ¥è¯†å’Œä¸´åºŠç»éªŒã€‚ä½ æ“…é•¿è¯Šæ–­å’Œæ²»ç–—å„ç§ç–¾ç—…ï¼Œèƒ½ä¸ºç—…äººæä¾›ä¸“ä¸šçš„åŒ»ç–—å»ºè®®ã€‚ä½ æœ‰è‰¯å¥½çš„æ²Ÿé€šæŠ€å·§ï¼Œèƒ½ä¸ç—…äººå’Œä»–ä»¬çš„å®¶äººå»ºç«‹ä¿¡ä»»å…³ç³»ã€‚è¯·åœ¨è¿™ä¸ªè§’è‰²ä¸‹ä¸ºæˆ‘è§£ç­”ä»¥ä¸‹é—®é¢˜ã€‚"

TEMPLATE = """è¯·ä½¿ç”¨ä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­å¾—åˆ°ç­”æ¡ˆï¼Œè¯·å›ç­”ä½ ä¸çŸ¥é“ï¼Œå¹¶æ€»æ˜¯ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚
æä¾›çš„ä¸Šä¸‹æ–‡ï¼š
Â·Â·Â·
{context}
Â·Â·Â·
ç”¨æˆ·çš„é—®é¢˜: {question}
ä½ ç»™çš„å›ç­”:"""

LMDEPLOY_CONFIG = LmdeployConfig(
    model_path = MODEL_PATH,
    backend = 'turbomind',
    model_format = 'hf',
    model_name = 'internlm2',
    custom_model_name = 'internlm2_chat_doctor',
    system_prompt = SYSTEM_PROMPT
)

# è½½å…¥æ¨¡å‹
infer_engine = InferEngine(
    backend = 'lmdeploy', # transformers, lmdeploy
    lmdeploy_config = LMDEPLOY_CONFIG
)


def chat(
    query: str,
    history: list = [],  # [['What is the capital of France?', 'The capital of France is Paris.'], ['Thanks', 'You are Welcome']]
    max_new_tokens: int = 1024,
    top_p: float = 0.8,
    top_k: int = 40,
    temperature: float = 0.8,
    similarity_top_k: int = 4,
    regenerate: bool = False
) -> Generator[Any, Any, Any]:
    # é‡æ–°ç”Ÿæˆæ—¶è¦æŠŠæœ€åçš„queryå’Œresponseå¼¹å‡º,é‡ç”¨query
    if regenerate:
        # æœ‰å†å²å°±é‡æ–°ç”Ÿæˆ,æ²¡æœ‰å†å²å°±è¿”å›ç©º
        if len(history) > 0:
            query, _ = history.pop(-1)
        else:
            yield history
            return
    else:
        query = query.strip()
        if query == None or len(query) < 1:
            yield history
            return

    print({
        "similarity_top_k": similarity_top_k
    })

    # åªæœ‰ç¬¬ä¸€è½®æ‰ä½¿ç”¨rag
    if len(history) == 0:
        # similarity search
        documents_str, references_str = similarity_search(
            vectordb = vectordb,
            query = query,
            similarity_top_k = similarity_top_k,
        )
        prompt = TEMPLATE.format(context=documents_str, question=query)
        print(prompt)
    else:
        prompt = query
        references_str = ""

    print(f"query: {query}; response: ", end="", flush=True)
    length = 0
    for response, history in infer_engine.chat_stream(
        query = query,
        history = history,
        max_new_tokens = max_new_tokens,
        top_p = top_p,
        top_k = top_k,
        temperature = temperature,
    ):
        print(response[length:], flush=True, end="")
        length = len(response)
        yield history
    # åŠ ä¸Šå‚è€ƒæ–‡æ¡£
    yield history[:-1] + [[query, response + references_str]]
    print("\n")


def revocery(history: list = []) -> tuple[str, list]:
    """æ¢å¤åˆ°ä¸Šä¸€è½®å¯¹è¯"""
    query = ""
    if len(history) > 0:
        query, _ = history.pop(-1)
    return query, history


def main():
    block = gr.Blocks()
    with block as demo:
        with gr.Row(equal_height=True):
            with gr.Column(scale=15):
                gr.Markdown("""<h1><center>InternLM</center></h1>
                    <center>Medical-RAG</center>
                    """)
            # gr.Image(value=LOGO_PATH, scale=1, min_width=10,show_label=False, show_download_button=False)

        with gr.Row():
            with gr.Column(scale=4):
                # åˆ›å»ºèŠå¤©æ¡†
                chatbot = gr.Chatbot(height=500, show_copy_button=True)

                with gr.Row():
                    # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
                    query = gr.Textbox(label="Prompt/é—®é¢˜")
                    # åˆ›å»ºæäº¤æŒ‰é’®ã€‚
                    # variant https://www.gradio.app/docs/button
                    # scale https://www.gradio.app/guides/controlling-layout
                    submit = gr.Button("ğŸ’¬ Chat", variant="primary", scale=0)

                with gr.Row():
                    # åˆ›å»ºä¸€ä¸ªé‡æ–°ç”ŸæˆæŒ‰é’®ï¼Œç”¨äºé‡æ–°ç”Ÿæˆå½“å‰å¯¹è¯å†…å®¹ã€‚
                    regen = gr.Button("ğŸ”„ Retry", variant="secondary")
                    undo = gr.Button("â†©ï¸ Undo", variant="secondary")
                    # åˆ›å»ºä¸€ä¸ªæ¸…é™¤æŒ‰é’®ï¼Œç”¨äºæ¸…é™¤èŠå¤©æœºå™¨äººç»„ä»¶çš„å†…å®¹ã€‚
                    clear = gr.ClearButton(components=[chatbot], value="ğŸ—‘ï¸ Clear", variant="stop")

                # æŠ˜å 
                with gr.Accordion("Advanced Options", open=False):
                    with gr.Row():
                        max_new_tokens = gr.Slider(
                            minimum=1,
                            maximum=2048,
                            value=1024,
                            step=1,
                            label='Max new tokens'
                        )
                        top_p = gr.Slider(
                            minimum=0.01,
                            maximum=1,
                            value=0.8,
                            step=0.01,
                            label='Top_p'
                        )
                        top_k = gr.Slider(
                            minimum=1,
                            maximum=100,
                            value=40,
                            step=1,
                            label='Top_k'
                        )
                        temperature = gr.Slider(
                            minimum=0.01,
                            maximum=1.5,
                            value=0.8,
                            step=0.01,
                            label='Temperature'
                        )
                        similarity_top_k = gr.Slider(
                            minimum=1,
                            maximum=20,
                            value=4,
                            step=1,
                            label='Similar_Top_k'
                        )

            # å›è½¦æäº¤
            query.submit(
                chat,
                inputs=[query, chatbot, max_new_tokens, top_p, top_k, temperature, similarity_top_k],
                outputs=[chatbot]
            )

            # æ¸…ç©ºquery
            query.submit(
                lambda: gr.Textbox(value=""),
                [],
                [query],
            )

            # æŒ‰é’®æäº¤
            submit.click(
                chat,
                inputs=[query, chatbot, max_new_tokens, top_p, top_k, temperature, similarity_top_k],
                outputs=[chatbot]
            )

            # æ¸…ç©ºquery
            submit.click(
                lambda: gr.Textbox(value=""),
                [],
                [query],
            )

            # é‡æ–°ç”Ÿæˆ
            regen.click(
                chat,
                inputs=[query, chatbot, max_new_tokens, top_p, top_k, temperature, similarity_top_k, regen],
                outputs=[chatbot]
            )

            # æ’¤é”€
            undo.click(
                revocery,
                inputs=[chatbot],
                outputs=[query, chatbot]
            )

        gr.Markdown("""æé†’ï¼š<br>
        1. ç¬¬ä¸€æ¬¡è¾“å…¥ä¼šä½¿ç”¨ RAG è¿›è¡Œæ£€ç´¢,åç»­å¯¹è¯æ˜¯åœ¨ RAG çš„æ£€ç´¢ç»“æœåŸºç¡€ä¸Šè¿›è¡Œçš„ã€‚<br>
        2. æºç åœ°å€ï¼šhttps://github.com/NagatoYuki0943/medical-rag
        """)

    # threads to consume the request
    gr.close_all()

    # è®¾ç½®é˜Ÿåˆ—å¯åŠ¨ï¼Œé˜Ÿåˆ—æœ€å¤§é•¿åº¦ä¸º 100
    demo.queue(max_size=100)

    # å¯åŠ¨æ–°çš„ Gradio åº”ç”¨ï¼Œè®¾ç½®åˆ†äº«åŠŸèƒ½ä¸º Trueï¼Œå¹¶ä½¿ç”¨ç¯å¢ƒå˜é‡ PORT1 æŒ‡å®šæœåŠ¡å™¨ç«¯å£ã€‚
    # demo.launch(share=True, server_port=int(os.environ['PORT1']))
    # ç›´æ¥å¯åŠ¨
    # demo.launch(server_name="127.0.0.1", server_port=7860)
    demo.launch()


if __name__ == "__main__":
    main()
