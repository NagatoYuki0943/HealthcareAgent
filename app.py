# fix chroma sqlite3 error
# refer: https://github.com/chroma-core/chroma/issues/1985#issuecomment-2055963683
# __import__('pysqlite3')
# import sys
# sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import os
from infer_engine import InferEngine, LmdeployConfig
from database import VectorDatabase
import gradio as gr
from typing import Generator, Any
from utils import download_dataset
from huggingface_hub import hf_hub_download, snapshot_download


print("*" * 100)
os.system("pip list")
print("*" * 100)


print("gradio version: ", gr.__version__)


DATA_PATH = "./data"
EMBEDDING_MODEL_PATH = "./models/bce-embedding-base_v1"
RERANKER_MODEL_PATH : str = "./models/bce-reranker-base_v1"
PERSIST_DIRECTORY = "./vector_db/faiss"
SIMILARITY_TOP_K = 7
SCORE_THRESHOLD = 0.15
ALLOW_SUFFIX = (".pdf")

# ä¸‹è½½ embedding å’Œ reranker æ¨¡å‹,ä¸ä¼šé‡å¤ä¸‹è½½
hf_token = os.getenv("HF_TOKEN", "")
snapshot_download(
    repo_id = "maidalun1020/bce-embedding-base_v1",
    local_dir = EMBEDDING_MODEL_PATH,
    resume_download = True,
    max_workers = 8,
    token = hf_token
)
snapshot_download(
    repo_id = "maidalun1020/bce-reranker-base_v1",
    local_dir = RERANKER_MODEL_PATH,
    resume_download = True,
    max_workers = 8,
    token = hf_token
)

# ä¸‹è½½æ•°æ®é›†,ä¸ä¼šé‡å¤ä¸‹è½½
download_dataset(target_path = DATA_PATH)

vector_database = VectorDatabase(
    data_path = DATA_PATH,
    embedding_model_path = EMBEDDING_MODEL_PATH,
    reranker_model_path = RERANKER_MODEL_PATH,
    persist_directory = PERSIST_DIRECTORY,
    similarity_top_k = SIMILARITY_TOP_K,
    score_threshold = SCORE_THRESHOLD,
    allow_suffix = ALLOW_SUFFIX
)
# åˆ›å»ºæ•°æ®åº“
vector_database.create_faiss_vectordb()
# è½½å…¥æ•°æ®åº“(åˆ›å»ºæ•°æ®åº“åä¸éœ€è¦è½½å…¥ä¹Ÿå¯ä»¥)
vector_database.load_faiss_vectordb()
# åˆ›å»ºé‡æ’åº retriever
vector_database.create_faiss_reranker_retriever()

# clone æ¨¡å‹
MODEL_PATH = './models/internlm2-chat-7b'
os.system(f'git clone https://code.openxlab.org.cn/OpenLMLab/internlm2-chat-7b {MODEL_PATH}')
os.system(f'cd {MODEL_PATH} && git lfs pull')

SYSTEM_PROMPT = """ä½ ç°åœ¨æ˜¯ä¸€ååŒ»ç”Ÿï¼Œå…·å¤‡ä¸°å¯Œçš„åŒ»å­¦çŸ¥è¯†å’Œä¸´åºŠç»éªŒã€‚ä½ æ“…é•¿è¯Šæ–­å’Œæ²»ç–—å„ç§ç–¾ç—…ï¼Œèƒ½ä¸ºç—…äººæä¾›ä¸“ä¸šçš„åŒ»ç–—å»ºè®®ã€‚ä½ æœ‰è‰¯å¥½çš„æ²Ÿé€šæŠ€å·§ï¼Œèƒ½ä¸ç—…äººå’Œä»–ä»¬çš„å®¶äººå»ºç«‹ä¿¡ä»»å…³ç³»ã€‚è¯·åœ¨è¿™ä¸ªè§’è‰²ä¸‹ä¸ºæˆ‘è§£ç­”ä»¥ä¸‹é—®é¢˜ã€‚
You are now a doctor with extensive medical knowledge and clinical experience. You are adept at diagnosing and treating various diseases and can provide professional medical advice to patients. You have good communication skills and can establish a trust relationship with patients and their families. Please answer the following questions for me in this role.
"""

REJECT_ANSWER_ZH = "å¯¹ä¸èµ·ï¼Œæˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚å¦‚æœæ‚¨æœ‰å…¶ä»–é—®é¢˜ï¼Œæ¬¢è¿éšæ—¶å‘æˆ‘æé—®ï¼Œæˆ‘ä¼šåœ¨æˆ‘èƒ½åŠ›èŒƒå›´å†…å°½åŠ›ä¸ºæ‚¨è§£ç­”ã€‚"
REJECT_ANSWER_EN = "Sorry, I can't answer your question. If you have any other questions, please feel free to ask me questions and I will try my best to answer them for you."

TEMPLATE_ZH = """è¯·ä½¿ç”¨ä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­å¾—åˆ°ç­”æ¡ˆï¼Œè¯·å›ç­”ä½ ä¸çŸ¥é“ï¼Œå¹¶æ€»æ˜¯ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚
æä¾›çš„ä¸Šä¸‹æ–‡:
Â·Â·Â·
{context}
Â·Â·Â·
ç”¨æˆ·çš„é—®é¢˜: {question}
ä½ ç»™çš„å›ç­”:"""

TEMPLATE_EN = """Please use the context provided below to answer the user's question. If you can't get the answer from the context, answer you don't know, and always answer in English.
context provided:
Â·Â·Â·
{context}
Â·Â·Â·
user's question: {question}
your answer:"""

LMDEPLOY_CONFIG = LmdeployConfig(
    model_path = MODEL_PATH,
    backend = 'turbomind',
    model_format = 'hf',
    model_name = 'internlm2',
    custom_model_name = 'internlm2_chat_doctor',
    system_prompt = SYSTEM_PROMPT
)

# è½½å…¥æ¨¡å‹
infer_engine = InferEngine(
    backend = 'lmdeploy', # transformers, lmdeploy
    lmdeploy_config = LMDEPLOY_CONFIG
)


def chat(
    query: str,
    history: list = [],  # [['What is the capital of France?', 'The capital of France is Paris.'], ['Thanks', 'You are Welcome']]
    max_new_tokens: int = 1024,
    top_p: float = 0.8,
    top_k: int = 40,
    temperature: float = 0.8,
    language: str = "ZH",
    regenerate: bool = False
) -> Generator[Any, Any, Any]:
    # é‡æ–°ç”Ÿæˆæ—¶è¦æŠŠæœ€åçš„queryå’Œresponseå¼¹å‡º,é‡ç”¨query
    if regenerate:
        # æœ‰å†å²å°±é‡æ–°ç”Ÿæˆ,æ²¡æœ‰å†å²å°±è¿”å›ç©º
        if len(history) > 0:
            query, _ = history.pop(-1)
        else:
            yield history
            return
    else:
        query = query.strip()
        if query == None or len(query) < 1:
            yield history
            return

    # é€‰æ‹©è¯­è¨€
    reject_answer = REJECT_ANSWER_ZH if language == "ZH" else REJECT_ANSWER_EN
    template = TEMPLATE_ZH if language == "ZH" else TEMPLATE_EN

    # similarity search
    documents_str, references_str = vector_database.similarity_search(
        query = query,
    )
    # æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£,è¿”å›æ‹’ç»é—®é¢˜
    if documents_str == "":
        yield history + [[query, reject_answer]]
        print(f"\033[0;32;40mhistory: {history + [[query, reject_answer]]}\033[0m")
        return
    prompt = template.format(context = documents_str, question = query)
    print(f"\033[0;34;40mprompt:\n{prompt}\033[0m")

    print(f"\033[0;33;40mquery: {query}; \nresponse: \033[0m", end="", flush=True)
    length = 0
    for response, _history in infer_engine.chat_stream(
        query = prompt,
        history = history,
        max_new_tokens = max_new_tokens,
        top_p = top_p,
        top_k = top_k,
        temperature = temperature,
    ):
        print(f"\033[0;33;40m{response[length:]}\033[0m", flush=True, end="")
        length = len(response)
        yield history + [[query, response]]
    # åŠ ä¸Šå‚è€ƒæ–‡æ¡£
    yield history + [[query, response + references_str]]
    print(f"\033[0;36;40m{references_str}\033[0m")
    print(f"\033[0;32;40mhistory: {history + [[query, response + references_str]]}\033[0m")


def revocery(history: list = []) -> tuple[str, list]:
    """æ¢å¤åˆ°ä¸Šä¸€è½®å¯¹è¯"""
    query = ""
    if len(history) > 0:
        query, _ = history.pop(-1)
    return query, history


def main():
    block = gr.Blocks()
    with block as demo:
        with gr.Row(equal_height=True):
            with gr.Column(scale=15):
                gr.Markdown("""<h1><center>Healthcare Agent</center></h1>
                    <center>Healthcare Agent</center>
                    """)
            # gr.Image(value=LOGO_PATH, scale=1, min_width=10,show_label=False, show_download_button=False)

        with gr.Row():
            with gr.Column(scale=4):
                # åˆ›å»ºèŠå¤©æ¡†
                chatbot = gr.Chatbot(height=500, show_copy_button=True)

                with gr.Row():
                    # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
                    query = gr.Textbox(label="Prompt/é—®é¢˜", placeholder="Enter å‘é€; Shift + Enter æ¢è¡Œ / Enter to send; Shift + Enter to wrap")
                    # åˆ›å»ºæäº¤æŒ‰é’®ã€‚
                    # variant https://www.gradio.app/docs/button
                    # scale https://www.gradio.app/guides/controlling-layout
                    submit = gr.Button("ğŸ’¬ Chat", variant="primary", scale=0)

                with gr.Row():
                    # ä¸‹æ‹‰æ¡†
                    language = gr.Dropdown(choices=[("ä¸­æ–‡", "ZH"), ("English", "EN")], value="ZH", label="Language", type="value", interactive=True)
                    # åˆ›å»ºä¸€ä¸ªé‡æ–°ç”ŸæˆæŒ‰é’®ï¼Œç”¨äºé‡æ–°ç”Ÿæˆå½“å‰å¯¹è¯å†…å®¹ã€‚
                    regen = gr.Button("ğŸ”„ Retry", variant="secondary")
                    undo = gr.Button("â†©ï¸ Undo", variant="secondary")
                    # åˆ›å»ºä¸€ä¸ªæ¸…é™¤æŒ‰é’®ï¼Œç”¨äºæ¸…é™¤èŠå¤©æœºå™¨äººç»„ä»¶çš„å†…å®¹ã€‚
                    clear = gr.ClearButton(components=[chatbot], value="ğŸ—‘ï¸ Clear", variant="stop")

                # æŠ˜å 
                with gr.Accordion("Advanced Options", open=False):
                    with gr.Row():
                        max_new_tokens = gr.Slider(
                            minimum=1,
                            maximum=2048,
                            value=1024,
                            step=1,
                            label='Max new tokens'
                        )
                        top_p = gr.Slider(
                            minimum=0.01,
                            maximum=1,
                            value=0.8,
                            step=0.01,
                            label='Top_p'
                        )
                        top_k = gr.Slider(
                            minimum=1,
                            maximum=100,
                            value=40,
                            step=1,
                            label='Top_k'
                        )
                        temperature = gr.Slider(
                            minimum=0.01,
                            maximum=1.5,
                            value=0.8,
                            step=0.01,
                            label='Temperature'
                        )

            # å›è½¦æäº¤
            query.submit(
                chat,
                inputs=[query, chatbot, max_new_tokens, top_p, top_k, temperature, language],
                outputs=[chatbot]
            )

            # æ¸…ç©ºquery
            query.submit(
                lambda: gr.Textbox(value=""),
                [],
                [query],
            )

            # æŒ‰é’®æäº¤
            submit.click(
                chat,
                inputs=[query, chatbot, max_new_tokens, top_p, top_k, temperature, language],
                outputs=[chatbot]
            )

            # æ¸…ç©ºquery
            submit.click(
                lambda: gr.Textbox(value=""),
                [],
                [query],
            )

            # é‡æ–°ç”Ÿæˆ
            regen.click(
                chat,
                inputs=[query, chatbot, max_new_tokens, top_p, top_k, temperature, language, regen],
                outputs=[chatbot]
            )

            # æ’¤é”€
            undo.click(
                revocery,
                inputs=[chatbot],
                outputs=[query, chatbot]
            )

        gr.Markdown("""æé†’ï¼š<br>
        1. æ¯æ¬¡å¯¹è¯éƒ½ä¼šä½¿ç”¨ RAG è¿›è¡Œæ£€ç´¢ã€‚<br>
        2. æºç åœ°å€ï¼šhttps://github.com/NagatoYuki0943/HealthcareAgent
        """)

    # threads to consume the request
    gr.close_all()

    # è®¾ç½®é˜Ÿåˆ—å¯åŠ¨ï¼Œé˜Ÿåˆ—æœ€å¤§é•¿åº¦ä¸º 100
    demo.queue(max_size=100)

    # å¯åŠ¨æ–°çš„ Gradio åº”ç”¨ï¼Œè®¾ç½®åˆ†äº«åŠŸèƒ½ä¸º Trueï¼Œå¹¶ä½¿ç”¨ç¯å¢ƒå˜é‡ PORT1 æŒ‡å®šæœåŠ¡å™¨ç«¯å£ã€‚
    # demo.launch(share=True, server_port=int(os.environ['PORT1']))
    # ç›´æ¥å¯åŠ¨
    # demo.launch(server_name="127.0.0.1", server_port=7860)
    demo.launch()


if __name__ == "__main__":
    main()
