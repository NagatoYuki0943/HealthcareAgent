import gradio as gr
from typing import Generator, Sequence, Any
import threading
from loguru import logger
from infer_engine import InferEngine, TransformersConfig, LmdeployConfig
from vector_database import VectorDatabase
from utils import remove_history_references


log_file = logger.add("log/runtime_{time}.log", rotation="00:00")
logger.info(f"gradio version: {gr.__version__}")


DATA_PATH: str = "./data"
EMBEDDING_MODEL_PATH: str = "./models/bce-embedding-base_v1"
RERANKER_MODEL_PATH: str = "./models/bce-reranker-base_v1"
PERSIST_DIRECTORY: str = "./vector_db/faiss"
SIMILARITY_TOP_K: int = 4
SIMILARITY_FETCH_K: int = 10
SCORE_THRESHOLD: float = 0.15
ALLOW_SUFFIX: tuple[str] = (".txt", ".md", ".docx", ".doc", ".pdf")
VECTOR_DEVICE = "cuda"
TEXT_SPLITTER_TYPE = "RecursiveCharacterTextSplitter"

vector_database = VectorDatabase(
    data_path=DATA_PATH,
    embedding_model_path=EMBEDDING_MODEL_PATH,
    reranker_model_path=RERANKER_MODEL_PATH,
    persist_directory=PERSIST_DIRECTORY,
    similarity_top_k=SIMILARITY_TOP_K,
    similarity_fetch_k=SIMILARITY_FETCH_K,
    score_threshold=SCORE_THRESHOLD,
    allow_suffix=ALLOW_SUFFIX,
    device=VECTOR_DEVICE,
    text_splitter_type=TEXT_SPLITTER_TYPE,
)
# åˆ›å»ºæ•°æ®åº“
vector_database.create_faiss_vectordb(force=False)
# è½½å…¥æ•°æ®åº“(åˆ›å»ºæ•°æ®åº“åä¸éœ€è¦è½½å…¥ä¹Ÿå¯ä»¥)
vector_database.load_faiss_vectordb()
# åˆ›å»ºç›¸ä¼¼åº¦ retriever
# vector_database.create_faiss_retriever()
# åˆ›å»ºé‡æ’åº retriever
vector_database.create_faiss_reranker_retriever()

# clone æ¨¡å‹
PRETRAINED_MODEL_NAME_OR_PATH = "./models/internlm2_5-1_8b-chat"
# os.system(f'git clone https://code.openxlab.org.cn/OpenLMLab/internlm2_5-1_8b-chat.git {PRETRAINED_MODEL_NAME_OR_PATH}')
# os.system(f'cd {PRETRAINED_MODEL_NAME_OR_PATH} && git lfs pull')
ADAPTER_PATH = None
# é‡åŒ–
LOAD_IN_8BIT = False
LOAD_IN_4BIT = False

SYSTEM_PROMPT = """
ä½ æ˜¯åŒ»ç–—ä¿å¥æ™ºèƒ½ä½“ï¼Œåå­—å«åš "HeathcareAgent"ã€‚
    - "HeathcareAgent" å¯ä»¥æ ¹æ®è‡ªå·±ä¸°å¯Œçš„åŒ»ç–—çŸ¥è¯†æ¥å›ç­”é—®é¢˜ã€‚
    - "HeathcareAgent" çš„å›ç­”åº”è¯¥æ˜¯æœ‰ç›Šçš„ã€è¯šå®çš„å’Œæ— å®³çš„ã€‚
    - "HeathcareAgent" å¯ä»¥ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„è¯­è¨€ï¼ˆå¦‚è‹±è¯­å’Œä¸­æ–‡ï¼‰è¿›è¡Œç†è§£å’Œäº¤æµã€‚
"""

TEMPLATE = """ä¸Šä¸‹æ–‡:
<context>
{context}
</context>
é—®é¢˜:
<question>{question}</question>
è¯·ä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ï¼Œå¦‚æœä¸Šä¸‹æ–‡ä¸è¶³è¯·æ ¹æ®è‡ªå·±çš„çŸ¥è¯†ç»™å‡ºåˆé€‚çš„å›ç­”ï¼Œå›ç­”åº”è¯¥æœ‰æ¡ç†(é™¤éç”¨æˆ·æŒ‡å®šäº†å›ç­”çš„è¯­è¨€ï¼Œå¦åˆ™ç”¨æˆ·ä½¿ç”¨ä»€ä¹ˆè¯­è¨€å°±ç”¨ä»€ä¹ˆè¯­è¨€å›ç­”):"""
# è¯·ä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ï¼Œå¦‚æœä¸Šä¸‹æ–‡ä¸è¶³è¯·æ ¹æ®è‡ªå·±çš„çŸ¥è¯†ç»™å‡ºåˆé€‚çš„å›ç­”ï¼Œå›ç­”åº”è¯¥æœ‰æ¡ç†:"""

TRANSFORMERS_CONFIG = TransformersConfig(
    pretrained_model_name_or_path=PRETRAINED_MODEL_NAME_OR_PATH,
    adapter_path=ADAPTER_PATH,
    load_in_8bit=LOAD_IN_8BIT,
    load_in_4bit=LOAD_IN_4BIT,
    model_name="internlm2",
    system_prompt=SYSTEM_PROMPT,
)

LMDEPLOY_CONFIG = LmdeployConfig(
    model_path=PRETRAINED_MODEL_NAME_OR_PATH,
    backend="turbomind",
    model_name="internlm2",
    model_format="hf",
    cache_max_entry_count=0.5,  # è°ƒæ•´ KV Cache çš„å ç”¨æ¯”ä¾‹ä¸º0.5
    quant_policy=0,  # KV Cache é‡åŒ–, 0 ä»£è¡¨ç¦ç”¨, 4 ä»£è¡¨ 4bit é‡åŒ–, 8 ä»£è¡¨ 8bit é‡åŒ–
    system_prompt=SYSTEM_PROMPT,
    deploy_method="local",
    log_level="ERROR",
)

# è½½å…¥æ¨¡å‹
infer_engine = InferEngine(
    backend="transformers",  # transformers, lmdeploy, api
    transformers_config=TRANSFORMERS_CONFIG,
    lmdeploy_config=LMDEPLOY_CONFIG,
)


class InterFace:
    global_session_id: int = 0
    lock = threading.Lock()


enable_btn = gr.update(interactive=True)
disable_btn = gr.update(interactive=False)
btn = dict[str, Any]


def chat(
    query: str,
    history: Sequence
    | None = None,  # [['What is the capital of France?', 'The capital of France is Paris.'], ['Thanks', 'You are Welcome']]
    max_new_tokens: int = 1024,
    temperature: float = 0.8,
    top_p: float = 0.8,
    top_k: int = 40,
    session_id: int | None = None,
) -> Generator[tuple[Sequence, btn, btn, btn, btn], None, None]:
    history = [] if history is None else list(history)

    # æ˜¯å¦æ˜¯æœ‰æ•ˆçš„é—®é¢˜
    query = query.strip()
    if query is None or len(query) < 1:
        logger.warning("query is None, return history")
        yield history, enable_btn, enable_btn, enable_btn, enable_btn
        return
    logger.info(f"query: {query}")

    # æ•°æ®åº“æ£€ç´¢
    documents_str, references_str = vector_database.similarity_search(
        query=query,
    )

    # æ ¼å¼åŒ–ragæ–‡ä»¶
    prompt = (
        TEMPLATE.format(context=documents_str, question=query)
        if documents_str
        else query
    )
    logger.info(f"prompt: {prompt}")

    # ç»™æ¨¡å‹çš„å†å²è®°å½•å»é™¤å‚è€ƒæ–‡æ¡£
    history_without_reference = remove_history_references(history=history)
    logger.info(f"history_without_reference: {history_without_reference}")

    yield history + [[query, None]], disable_btn, disable_btn, disable_btn, disable_btn

    for response in infer_engine.chat_stream(
        query=prompt,
        history=history_without_reference,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        top_p=top_p,
        top_k=top_k,
        session_id=session_id,
    ):
        yield history + [[query, response]], disable_btn, disable_btn, disable_btn, disable_btn

    # åŠ ä¸Šå‚è€ƒæ–‡æ¡£
    yield history + [[query, response + references_str]], enable_btn, enable_btn, enable_btn, enable_btn
    logger.info(f"references_str: {references_str}")
    logger.info(
        f"history_without_rag: {history + [[query, response + references_str]]}"
    )


def regenerate(
    history: Sequence
    | None = None,  # [['What is the capital of France?', 'The capital of France is Paris.'], ['Thanks', 'You are Welcome']]
    max_new_tokens: int = 1024,
    temperature: float = 0.8,
    top_p: float = 0.8,
    top_k: int = 40,
    session_id: int | None = None,
) -> Generator[tuple[Sequence, btn, btn, btn, btn], None, None]:
    history = [] if history is None else list(history)

    # é‡æ–°ç”Ÿæˆæ—¶è¦æŠŠæœ€åçš„queryå’Œresponseå¼¹å‡º,é‡ç”¨query
    if len(history) > 0:
        query, _ = history.pop(-1)
        yield from chat(
            query=query,
            history=history,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            session_id=session_id,
        )
    else:
        logger.warning("no history, can't regenerate")
        yield history, enable_btn, enable_btn, enable_btn, enable_btn


def revocery(history: Sequence | None = None) -> tuple[str, Sequence]:
    """æ¢å¤åˆ°ä¸Šä¸€è½®å¯¹è¯"""
    history = [] if history is None else list(history)
    query = ""
    if len(history) > 0:
        query, _ = history.pop(-1)
    return query, history


def main() -> None:
    block = gr.Blocks()
    with block as demo:
        state_session_id = gr.State(0)

        with gr.Row(equal_height=True):
            with gr.Column(scale=15):
                gr.Markdown("""<h1><center>Healthcare Agent</center></h1>""")
            # gr.Image(value=LOGO_PATH, scale=1, min_width=10,show_label=False, show_download_button=False)

        # æ™ºèƒ½é—®ç­”é¡µé¢
        with gr.Tab("åŒ»ç–—æ™ºèƒ½é—®ç­”"):
            with gr.Row():
                with gr.Column(scale=4):
                    # åˆ›å»ºèŠå¤©æ¡†
                    chatbot = gr.Chatbot(
                        height=500,
                        show_copy_button=True,
                        placeholder="å†…å®¹ç”± AI å¤§æ¨¡å‹ç”Ÿæˆï¼Œä¸æ„æˆä¸“ä¸šåŒ»ç–—æ„è§æˆ–è¯Šæ–­ã€‚",
                    )

                    # ç»„å†…çš„ç»„ä»¶æ²¡æœ‰é—´è·
                    with gr.Group():
                        with gr.Row():
                            # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
                            query = gr.Textbox(
                                lines=1,
                                label="Prompt / é—®é¢˜",
                                placeholder="Enter å‘é€; Shift + Enter æ¢è¡Œ / Enter to send; Shift + Enter to wrap",
                            )
                            # åˆ›å»ºæäº¤æŒ‰é’®ã€‚
                            # variant https://www.gradio.app/docs/button
                            # scale https://www.gradio.app/guides/controlling-layout
                            submit = gr.Button("ğŸ’¬ Chat", variant="primary", scale=0)

                    gr.Examples(
                        examples=[
                            ["ç»´ç”Ÿç´ Eæœ‰ä»€ä¹ˆä½œç”¨ï¼Œè¯·è¯¦ç»†è¯´æ˜"],
                            ["ç»´ç”Ÿç´ Cå¯¹æ²»ç–—çœ¼ç›ç–¾ç—…æœ‰ä»€ä¹ˆä½œç”¨ï¼Œè¯·è¯¦ç»†è¯´æ˜"],
                            [
                                "Please explain the effect of vitamin C on the treatment of eye diseases"
                            ],
                        ],
                        inputs=[query],
                        label="ç¤ºä¾‹é—®é¢˜ / Example questions",
                    )

                    with gr.Row():
                        # åˆ›å»ºä¸€ä¸ªé‡æ–°ç”ŸæˆæŒ‰é’®ï¼Œç”¨äºé‡æ–°ç”Ÿæˆå½“å‰å¯¹è¯å†…å®¹ã€‚
                        regen = gr.Button("ğŸ”„ Retry", variant="secondary")
                        undo = gr.Button("â†©ï¸ Undo", variant="secondary")
                        # åˆ›å»ºä¸€ä¸ªæ¸…é™¤æŒ‰é’®ï¼Œç”¨äºæ¸…é™¤èŠå¤©æœºå™¨äººç»„ä»¶çš„å†…å®¹ã€‚
                        clear = gr.ClearButton(
                            components=[chatbot], value="ğŸ—‘ï¸ Clear", variant="stop"
                        )

                    # æŠ˜å 
                    with gr.Accordion("Advanced Options", open=False):
                        with gr.Row():
                            max_new_tokens = gr.Slider(
                                minimum=1,
                                maximum=2048,
                                value=1024,
                                step=1,
                                label="Max new tokens",
                            )
                            temperature = gr.Slider(
                                minimum=0.01,
                                maximum=2,
                                value=0.8,
                                step=0.01,
                                label="Temperature",
                            )
                            top_p = gr.Slider(
                                minimum=0.01,
                                maximum=1,
                                value=0.8,
                                step=0.01,
                                label="Top_p",
                            )
                            top_k = gr.Slider(
                                minimum=1, maximum=100, value=40, step=1, label="Top_k"
                            )

                # å›è½¦æäº¤(æ— æ³•ç¦æ­¢æŒ‰é’®)
                query.submit(
                    chat,
                    inputs=[
                        query,
                        chatbot,
                        max_new_tokens,
                        temperature,
                        top_p,
                        top_k,
                        state_session_id,
                    ],
                    outputs=[chatbot, submit, regen, undo, clear],
                )

                # æ¸…ç©ºquery
                query.submit(
                    lambda: gr.Textbox(value=""),
                    inputs=[],
                    outputs=[query],
                )

                # æŒ‰é’®æäº¤
                submit.click(
                    chat,
                    inputs=[
                        query,
                        chatbot,
                        max_new_tokens,
                        temperature,
                        top_p,
                        top_k,
                        state_session_id,
                    ],
                    outputs=[chatbot, submit, regen, undo, clear],
                )

                # æ¸…ç©ºquery
                submit.click(
                    lambda: gr.Textbox(value=""),
                    inputs=[],
                    outputs=[query],
                )

                # é‡æ–°ç”Ÿæˆ
                regen.click(
                    regenerate,
                    inputs=[
                        chatbot,
                        max_new_tokens,
                        temperature,
                        top_p,
                        top_k,
                        state_session_id,
                    ],
                    outputs=[chatbot, submit, regen, undo, clear],
                )

                # æ’¤é”€
                undo.click(revocery, inputs=[chatbot], outputs=[query, chatbot])

        gr.Markdown("""
        ### å†…å®¹ç”± AI å¤§æ¨¡å‹ç”Ÿæˆï¼Œä¸æ„æˆä¸“ä¸šåŒ»ç–—æ„è§æˆ–è¯Šæ–­ã€‚
        """)

        # åˆå§‹åŒ–session_id
        def init():
            with InterFace.lock:
                InterFace.global_session_id += 1
            new_session_id = InterFace.global_session_id
            return new_session_id

        demo.load(init, inputs=None, outputs=[state_session_id])

    # threads to consume the request
    gr.close_all()

    # è®¾ç½®é˜Ÿåˆ—å¯åŠ¨
    demo.queue(
        max_size=None,  # If None, the queue size will be unlimited.
        default_concurrency_limit=40,  # æœ€å¤§å¹¶å‘é™åˆ¶
    )

    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
        max_threads=40,
    )


if __name__ == "__main__":
    main()
