# å¯¼å…¥å¿…è¦çš„åº“
import gradio as gr
from old_file.load_chain import load_chain


class ModelCenter:
    """
    å­˜å‚¨é—®ç­” Chain çš„å¯¹è±¡
    """

    def __init__(self):
        self.chain = load_chain(
            llm_path="./models/internlm2_5-1_8b-chat",
            embedding_model_name="./models/sentence-transformer",
            persist_directory="./vector_db/chroma",
            adapter_dir=None,
            load_in_8bit=False,
            load_in_4bit=False,
            system_prompt="ä½ ç°åœ¨æ˜¯ä¸€ååŒ»ç”Ÿï¼Œå…·å¤‡ä¸°å¯Œçš„åŒ»å­¦çŸ¥è¯†å’Œä¸´åºŠç»éªŒã€‚ä½ æ“…é•¿è¯Šæ–­å’Œæ²»ç–—å„ç§ç–¾ç—…ï¼Œèƒ½ä¸ºç—…äººæä¾›ä¸“ä¸šçš„åŒ»ç–—å»ºè®®ã€‚ä½ æœ‰è‰¯å¥½çš„æ²Ÿé€šæŠ€å·§ï¼Œèƒ½ä¸ç—…äººå’Œä»–ä»¬çš„å®¶äººå»ºç«‹ä¿¡ä»»å…³ç³»ã€‚è¯·åœ¨è¿™ä¸ªè§’è‰²ä¸‹ä¸ºæˆ‘è§£ç­”ä»¥ä¸‹é—®é¢˜ã€‚",
        )

    def qa_chain_self_answer(self, query: str, history: list = []):
        """
        è°ƒç”¨ä¸å¸¦å†å²è®°å½•çš„é—®ç­”é“¾è¿›è¡Œå›ç­”

        history: [['What is the capital of France?', 'The capital of France is Paris.'], ['Thanks', 'You are Welcome']]
        """
        if query is None or len(query) < 1:
            return history
        try:
            # invoke(input: Dict[str, Any], config: Optional[langchain_core.runnables.config.RunnableConfig] = None, **kwargs: Any) -> Dict[str, Any]
            # method of langchain.chains.retrieval_qa.base.RetrievalQA instance
            response = self.chain.invoke(input={"query": query})["result"]
            history.append([query, response])
            return history
        except Exception as e:
            return history


model_center = ModelCenter()


def revocery(history: list = []) -> tuple[str, list]:
    """æ¢å¤åˆ°ä¸Šä¸€è½®å¯¹è¯"""
    query = ""
    if len(history) > 0:
        query, _ = history.pop(-1)
    return query, history


def main():
    block = gr.Blocks()
    with block as demo:
        with gr.Row(equal_height=True):
            with gr.Column(scale=15):
                gr.Markdown("""<h1><center>InternLM</center></h1>
                    <center>InternLM2</center>
                    """)
            # gr.Image(value=LOGO_PATH, scale=1, min_width=10,show_label=False, show_download_button=False)

        with gr.Row():
            with gr.Column(scale=4):
                # åˆ›å»ºèŠå¤©æ¡†
                chatbot = gr.Chatbot(height=500, show_copy_button=True)

                with gr.Row():
                    max_new_tokens = gr.Slider(
                        minimum=1,
                        maximum=2048,
                        value=1024,
                        step=1,
                        label="Maximum new tokens",
                    )
                    top_p = gr.Slider(
                        minimum=0.01, maximum=1, value=0.8, step=0.01, label="Top_p"
                    )
                    top_k = gr.Slider(
                        minimum=1, maximum=100, value=40, step=1, label="Top_k"
                    )
                    temperature = gr.Slider(
                        minimum=0.01,
                        maximum=2,
                        value=0.8,
                        step=0.01,
                        label="Temperature",
                    )

                with gr.Row():
                    # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
                    query = gr.Textbox(label="Prompt/é—®é¢˜")
                    # åˆ›å»ºæäº¤æŒ‰é’®ã€‚
                    # variant https://www.gradio.app/docs/button
                    # scale https://www.gradio.app/guides/controlling-layout
                    submit = gr.Button("ğŸ’¬ Chat", variant="primary", scale=0)

                with gr.Row():
                    # åˆ›å»ºä¸€ä¸ªé‡æ–°ç”ŸæˆæŒ‰é’®ï¼Œç”¨äºé‡æ–°ç”Ÿæˆå½“å‰å¯¹è¯å†…å®¹ã€‚
                    # regen = gr.Button("ğŸ”„ Retry", variant="secondary")
                    # undo = gr.Button("â†©ï¸ Undo", variant="secondary")
                    # åˆ›å»ºä¸€ä¸ªæ¸…é™¤æŒ‰é’®ï¼Œç”¨äºæ¸…é™¤èŠå¤©æœºå™¨äººç»„ä»¶çš„å†…å®¹ã€‚
                    clear = gr.ClearButton(
                        components=[chatbot], value="ğŸ—‘ï¸ Clear", variant="stop"
                    )

            # å›è½¦æäº¤
            query.submit(
                model_center.qa_chain_self_answer,
                inputs=[query, chatbot],
                outputs=[chatbot],
            )

            # æ¸…ç©ºquery
            query.submit(
                lambda: gr.Textbox(value=""),
                [],
                [query],
            )

            # æŒ‰é’®æäº¤
            submit.click(
                model_center.qa_chain_self_answer,
                inputs=[query, chatbot],
                outputs=[chatbot],
            )

            # æ¸…ç©ºquery
            submit.click(
                lambda: gr.Textbox(value=""),
                [],
                [query],
            )

            # # é‡æ–°ç”Ÿæˆ
            # regen.click(
            #     model_center.qa_chain_self_answer,
            #     inputs=[query, chatbot],
            #     outputs=[chatbot]
            # )

            # # æ’¤é”€
            # undo.click(
            #     model_center.qa_chain_self_answer,
            #     inputs=[chatbot],
            #     outputs=[query, chatbot]
            # )

        gr.Markdown("""æé†’ï¼š<br>
        1. åˆå§‹åŒ–æ•°æ®åº“æ—¶é—´å¯èƒ½è¾ƒé•¿ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚
        2. ä½¿ç”¨ä¸­å¦‚æœå‡ºç°å¼‚å¸¸ï¼Œå°†ä¼šåœ¨æ–‡æœ¬è¾“å…¥æ¡†è¿›è¡Œå±•ç¤ºï¼Œè¯·ä¸è¦æƒŠæ…Œã€‚ <br>
        """)

    # threads to consume the request
    gr.close_all()

    # è®¾ç½®é˜Ÿåˆ—å¯åŠ¨ï¼Œé˜Ÿåˆ—æœ€å¤§é•¿åº¦ä¸º 100
    demo.queue(max_size=100)

    # å¯åŠ¨æ–°çš„ Gradio åº”ç”¨ï¼Œè®¾ç½®åˆ†äº«åŠŸèƒ½ä¸º Trueï¼Œå¹¶ä½¿ç”¨ç¯å¢ƒå˜é‡ PORT1 æŒ‡å®šæœåŠ¡å™¨ç«¯å£ã€‚
    # demo.launch(share=True, server_port=int(os.environ['PORT1']))
    # ç›´æ¥å¯åŠ¨
    # demo.launch(server_name="127.0.0.1", server_port=7860)
    demo.launch()


if __name__ == "__main__":
    main()
