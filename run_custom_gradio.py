import os
from infer_engine import InferEngine, TransformersConfig
from database import VectorDatabase
import gradio as gr
from typing import Generator, Any
from loguru import logger
from utils import remove_history_references


logger.info(f"gradio version: {gr.__version__}")


DATA_PATH = "./data"
EMBEDDING_MODEL_PATH = "./models/bce-embedding-base_v1"
RERANKER_MODEL_PATH : str = "./models/bce-reranker-base_v1"
PERSIST_DIRECTORY = "./vector_db/faiss"
SIMILARITY_TOP_K = 7
SCORE_THRESHOLD = 0.15
ALLOW_SUFFIX = (".pdf")

vector_database = VectorDatabase(
    data_path = DATA_PATH,
    embedding_model_path = EMBEDDING_MODEL_PATH,
    reranker_model_path = RERANKER_MODEL_PATH,
    persist_directory = PERSIST_DIRECTORY,
    similarity_top_k = SIMILARITY_TOP_K,
    score_threshold = SCORE_THRESHOLD,
    allow_suffix = ALLOW_SUFFIX
)
# åˆ›å»ºæ•°æ®åº“
vector_database.create_faiss_vectordb()
# è½½å…¥æ•°æ®åº“(åˆ›å»ºæ•°æ®åº“åä¸éœ€è¦è½½å…¥ä¹Ÿå¯ä»¥)
vector_database.load_faiss_vectordb()
# åˆ›å»ºç›¸ä¼¼åº¦ retriever
# vector_database.create_faiss_retriever()
# åˆ›å»ºé‡æ’åº retriever
vector_database.create_faiss_reranker_retriever()

# clone æ¨¡å‹
PRETRAINED_MODEL_NAME_OR_PATH = './models/internlm2-chat-1_8b'
# os.system(f'git clone https://code.openxlab.org.cn/OpenLMLab/internlm2-chat-1.8b {PRETRAINED_MODEL_NAME_OR_PATH}')
# os.system(f'cd {PRETRAINED_MODEL_NAME_OR_PATH} && git lfs pull')
ADAPTER_PATH = None
# é‡åŒ–
LOAD_IN_8BIT = False
LOAD_IN_4BIT = False

SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ååŒ»ç–—çŸ¥è¯†åŠ©æ‰‹ï¼Œåå­—å«åšâ€æ™ºç–—â€œã€‚
    - â€æ™ºç–—â€œå¯ä»¥æ ¹æ®è‡ªå·±ä¸°å¯Œçš„åŒ»ç–—çŸ¥è¯†æ¥å›ç­”é—®é¢˜ã€‚ã€‚
    - â€æ™ºç–—â€œçš„å›ç­”åº”è¯¥æ˜¯æœ‰ç›Šçš„ã€è¯šå®çš„å’Œæ— å®³çš„ã€‚
    - â€æ™ºç–—â€œå¯ä»¥ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„è¯­è¨€ï¼ˆå¦‚è‹±è¯­å’Œä¸­æ–‡ï¼‰è¿›è¡Œç†è§£å’Œäº¤æµã€‚
"""

TEMPLATE = """ä¸Šä¸‹æ–‡:
<context>
{context}
</context>
é—®é¢˜:
<question>{question}</question>
è¯·ä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ï¼Œå¦‚æœä¸Šä¸‹æ–‡ä¸è¶³è¯·æ ¹æ®è‡ªå·±çš„çŸ¥è¯†ç»™å‡ºåˆé€‚çš„å»ºè®®(é™¤éç”¨æˆ·æŒ‡å®šäº†å›ç­”çš„è¯­è¨€ï¼Œå¦åˆ™ç”¨æˆ·ä½¿ç”¨ä»€ä¹ˆè¯­è¨€å°±ä»€ä¹ˆè¯­è¨€å›ç­”):"""

TRANSFORMERS_CONFIG = TransformersConfig(
    pretrained_model_name_or_path = PRETRAINED_MODEL_NAME_OR_PATH,
    adapter_path = ADAPTER_PATH,
    load_in_8bit = LOAD_IN_8BIT,
    load_in_4bit = LOAD_IN_4BIT,
    system_prompt = SYSTEM_PROMPT
)

# è½½å…¥æ¨¡å‹
infer_engine = InferEngine(
    backend = 'transformers', # transformers, lmdeploy
    transformers_config = TRANSFORMERS_CONFIG,
)


def chat(
    query: str,
    history: list | None = None,  # [['What is the capital of France?', 'The capital of France is Paris.'], ['Thanks', 'You are Welcome']]
    max_new_tokens: int = 1024,
    temperature: float = 0.8,
    top_p: float = 0.8,
    top_k: int = 40,
) -> Generator[Any, Any, Any]:
    history = [] if history is None else history

    # æ˜¯å¦æ˜¯æœ‰æ•ˆçš„é—®é¢˜
    query = query.strip()
    if query == None or len(query) < 1:
        yield history
        return
    logger.info(f"query: {query}")

    # æ•°æ®åº“æ£€ç´¢
    documents_str, references_str = vector_database.similarity_search(
        query = query,
    )

    # æ ¼å¼åŒ–ragæ–‡ä»¶
    prompt = TEMPLATE.format(context = documents_str, question = query) if documents_str else query
    logger.info(f"prompt: {prompt}")

    # ç»™æ¨¡å‹çš„å†å²è®°å½•å»é™¤å‚è€ƒæ–‡æ¡£
    history_without_reference = remove_history_references(history = history)
    logger.info(f"history_without_reference: {history_without_reference}")

    for response, _history in infer_engine.chat_stream(
        query = prompt,
        history = history_without_reference,
        max_new_tokens = max_new_tokens,
        temperature = temperature,
        top_p = top_p,
        top_k = top_k,
    ):
        yield history + [[query, response]]

    # åŠ ä¸Šå‚è€ƒæ–‡æ¡£
    yield history + [[query, response + references_str]]
    logger.info(f"references_str: {references_str}")
    logger.info(f"history_without_rag: {history + [[query, response + references_str]]}")


def regenerate(
    query: str,
    history: list | None = None,  # [['What is the capital of France?', 'The capital of France is Paris.'], ['Thanks', 'You are Welcome']]
    max_new_tokens: int = 1024,
    temperature: float = 0.8,
    top_p: float = 0.8,
    top_k: int = 40,
) -> Generator[Any, Any, Any]:
    history = [] if history is None else history

    # é‡æ–°ç”Ÿæˆæ—¶è¦æŠŠæœ€åçš„queryå’Œresponseå¼¹å‡º,é‡ç”¨query
    if len(history) > 0:
        query, _ = history.pop(-1)
        yield from chat(
            query = query,
            history = history,
            max_new_tokens = max_new_tokens,
            temperature = temperature,
            top_p = top_p,
            top_k = top_k,
        )
    else:
        yield history


def revocery(history: list = []) -> tuple[str, list]:
    """æ¢å¤åˆ°ä¸Šä¸€è½®å¯¹è¯"""
    query = ""
    if len(history) > 0:
        query, _ = history.pop(-1)
    return query, history


def main():
    block = gr.Blocks()
    with block as demo:
        with gr.Row(equal_height=True):
            with gr.Column(scale=15):
                gr.Markdown("""<h1><center>Healthcare Agent</center></h1>
                    <center>Healthcare Agent</center>
                    """)
            # gr.Image(value=LOGO_PATH, scale=1, min_width=10,show_label=False, show_download_button=False)

        with gr.Row():
            with gr.Column(scale=4):
                # åˆ›å»ºèŠå¤©æ¡†
                chatbot = gr.Chatbot(height=500, show_copy_button=True, placeholder="å†…å®¹ç”± AI å¤§æ¨¡å‹ç”Ÿæˆï¼Œä¸æ„æˆä¸“ä¸šåŒ»ç–—æ„è§æˆ–è¯Šæ–­ã€‚")

                with gr.Row():
                    # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
                    query = gr.Textbox(label="Prompt/é—®é¢˜", placeholder="Enter å‘é€; Shift + Enter æ¢è¡Œ / Enter to send; Shift + Enter to wrap")
                    # åˆ›å»ºæäº¤æŒ‰é’®ã€‚
                    # variant https://www.gradio.app/docs/button
                    # scale https://www.gradio.app/guides/controlling-layout
                    submit = gr.Button("ğŸ’¬ Chat", variant="primary", scale=0)

                with gr.Row():
                    # åˆ›å»ºä¸€ä¸ªé‡æ–°ç”ŸæˆæŒ‰é’®ï¼Œç”¨äºé‡æ–°ç”Ÿæˆå½“å‰å¯¹è¯å†…å®¹ã€‚
                    regen = gr.Button("ğŸ”„ Retry", variant="secondary")
                    undo = gr.Button("â†©ï¸ Undo", variant="secondary")
                    # åˆ›å»ºä¸€ä¸ªæ¸…é™¤æŒ‰é’®ï¼Œç”¨äºæ¸…é™¤èŠå¤©æœºå™¨äººç»„ä»¶çš„å†…å®¹ã€‚
                    clear = gr.ClearButton(components=[chatbot], value="ğŸ—‘ï¸ Clear", variant="stop")

                # æŠ˜å 
                with gr.Accordion("Advanced Options", open=False):
                    with gr.Row():
                        max_new_tokens = gr.Slider(
                            minimum=1,
                            maximum=2048,
                            value=1024,
                            step=1,
                            label='Max new tokens'
                        )
                        temperature = gr.Slider(
                            minimum=0.01,
                            maximum=2,
                            value=0.8,
                            step=0.01,
                            label='Temperature'
                        )
                        top_p = gr.Slider(
                            minimum=0.01,
                            maximum=1,
                            value=0.8,
                            step=0.01,
                            label='Top_p'
                        )
                        top_k = gr.Slider(
                            minimum=1,
                            maximum=100,
                            value=40,
                            step=1,
                            label='Top_k'
                        )

            # å›è½¦æäº¤
            query.submit(
                chat,
                inputs=[query, chatbot, max_new_tokens, temperature, top_p, top_k],
                outputs=[chatbot]
            )

            # æ¸…ç©ºquery
            query.submit(
                lambda: gr.Textbox(value=""),
                [],
                [query],
            )

            # æŒ‰é’®æäº¤
            submit.click(
                chat,
                inputs=[query, chatbot, max_new_tokens, temperature, top_p, top_k],
                outputs=[chatbot]
            )

            # æ¸…ç©ºquery
            submit.click(
                lambda: gr.Textbox(value=""),
                [],
                [query],
            )

            # é‡æ–°ç”Ÿæˆ
            regen.click(
                regenerate,
                inputs=[query, chatbot, max_new_tokens, temperature, top_p, top_k],
                outputs=[chatbot]
            )

            # æ’¤é”€
            undo.click(
                revocery,
                inputs=[chatbot],
                outputs=[query, chatbot]
            )

        gr.Markdown("""
        ### å†…å®¹ç”± AI å¤§æ¨¡å‹ç”Ÿæˆï¼Œä¸æ„æˆä¸“ä¸šåŒ»ç–—æ„è§æˆ–è¯Šæ–­ã€‚
        """)

    # threads to consume the request
    gr.close_all()

    # è®¾ç½®é˜Ÿåˆ—å¯åŠ¨ï¼Œé˜Ÿåˆ—æœ€å¤§é•¿åº¦ä¸º 100
    demo.queue(max_size=100)

    # å¯åŠ¨æ–°çš„ Gradio åº”ç”¨ï¼Œè®¾ç½®åˆ†äº«åŠŸèƒ½ä¸º Trueï¼Œå¹¶ä½¿ç”¨ç¯å¢ƒå˜é‡ PORT1 æŒ‡å®šæœåŠ¡å™¨ç«¯å£ã€‚
    # demo.launch(share=True, server_port=int(os.environ['PORT1']))
    # ç›´æ¥å¯åŠ¨
    # demo.launch(server_name="127.0.0.1", server_port=7860)
    demo.launch()


if __name__ == "__main__":
    main()
