# HealthcareAgent

使用rag对医学数据进行检索

## 使用步骤

1. 安装依赖，安装 `requirements.txt` 中的依赖，可能不全，如果有需要按照的包里面没包含请告诉我

2. 下载模型

运行 `download_hf.py` 文件下载向量化模型和 internlm2 模型

也可以自己下载放到 models 目录下

3. 建立一个 `data` 目录，然后将群里的压缩包解压到data内，最终路径为 `data/FM docs 2024.3/*.pdf`

4. 建立向量数据库

运行`database.py`建立数据库

5. 运行

`run_langchain.py` 是命令行运行模型，功能不全

`run_langchain_gradio.py`是使用 langchain 运行模型，功能不全

`run_custom_gradio.py` 会议中展示的，功能最全

`app.py` 内容和 `run_custom_gradio.py` 相同，不同点是会下载数据集，创建数据库，下载模型并运行，并使用了 lmdeploy 加速



# 说明

模型运行步骤：

1. 对输出判断，是否为无效字符。

2. 检索数据库，使用数据库检索进行初步筛选，然后使用重排序模型过滤。

   如果检索到数据，就将检索数据和问题经过格式化后一起提供给模型，得到输出；如果没有检索到数据，就只给模型提供问题，让模型输出。

2. 将问题和检索数据以及历史记录一起传递给模型得到回答。

细节说明：

1. 在使用 RAG 检索时，给模型的提示词时包含了检索数据和问题的提示词，不过最终呈现给用户时，只将用户的问题和模型的回答呈现给用户。

2. 历史记录同上，也就是说历史记录里面只包含用户的问题和模型的回答，没有保存历史的 RAG 检索数据。

   这样做的原因有2点：一是将检索数据呈现给用户体验不好。二是这些页数也会占用模型的 token 数量，模型的 token 是有上限的，过长会影响模型输出效果。

3. 在每次对话时，如果有检索到信息，会将检索的文档名字返回给用户，没有检索到时也有相应的提示，这些参考信息也是会保存在历史记录中。

   不过需要注意的是，在将过往历史记录传递给模型的时候，是会将历史记录中的参考文献删除的。

   这样做的原因有2点：一是在使用过程中发现，在有历史记录且历史记录的对话没使用检索信息，新的对话也没有使用检索的时候，有时在模型输出的结尾会打印两次 `no reference.`，我排查后发现第一次的打印是模型自己输出的，也就是模型根据历史记录学会了在合适的情况下添加`no reference.`，而这不是我们想要的结果。是这些参考文档也会占用模型的 token 数量，模型的 token 是有上限的，过长会影响模型输出效果。

## TODO

- [x] 返回参考文档
- [x] 当前使用 transformers 库进行推理，速度较慢，可以换成 lmdeploy 库进行加速
- [x] 当查询库中没有对应数据时说明找不到内容
- [x] 加上重排序模型



